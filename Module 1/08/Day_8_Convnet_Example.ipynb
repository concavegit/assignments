{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day 8 Convnet Example",
      "provenance": [],
      "collapsed_sections": [
        "t-35nSvVIbiT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/08/Day_8_Convnet_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVFu3OVp3Xix",
        "colab_type": "text"
      },
      "source": [
        "# Boilerplate Convnet example\n",
        "In this notebook we'll be looking at training a convolutional neural network the [CIFAR 10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR dataset contains relatively low resolution (32x32 pixel) images of 10 distinct categories of objects.\n",
        "\n",
        "* airplane\t\t\t\t\t\t\t\t\t\t\n",
        "* automobile\t\t\t\t\t\t\t\t\t\t\n",
        "* bird\t\t\t\t\t\t\t\t\t\t\n",
        "* cat\t\t\t\t\t\t\t\t\t\t\n",
        "* deer\t\t\t\t\t\t\t\t\t\t\n",
        "* dog\t\t\t\t\t\t\t\t\t\t\n",
        "* frog\t\t\t\t\t\t\t\t\t\t\n",
        "* horse\t\t\t\t\t\t\t\t\t\t\n",
        "* ship\t\t\t\t\t\t\t\t\t\t\n",
        "* truck\n",
        "\n",
        "We recommend that you run, examine, and understand all the code before attempting any of the exercises so that they make sense in a broader context.\n",
        "\n",
        "Thanks to [Algorithmia](https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/) for some of the base code for this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt3N5feFwBVL",
        "colab_type": "text"
      },
      "source": [
        "## Use a GPU!\n",
        "\n",
        "This notebook uses the GPU functionality of Pytorch and Google Collab. We need to make sure we are running our operations on the GPU, verify this in you notebook settings at the top. Setting found under:\n",
        "\n",
        "`Runtime > Change runtime type > Hardware Accelerator -> GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3alCNsXcczI",
        "colab_type": "text"
      },
      "source": [
        "Let's start by installing a package and importing some modules we'll need later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjeB52XEcWmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b40d80ce-37f5-4b5d-a9f5-f7b5aa4505ae"
      },
      "source": [
        "!pip install torchviz\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US13M6R4cxbL",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "This dataset happens to be included with pytorch so we can call some pytorch functions to automatically load and parse the data we need to. \n",
        "\n",
        "Don't worry too much about the specifics of this part. Data loading, cleaning, and parsing is often taylored to every dataset so functions from one dataset loading don't often directly transfer to another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKQk3V09cxrC",
        "colab_type": "code",
        "outputId": "cd45ca88-96be-4c1c-a586-8bb51a29ac9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Data set information\n",
        "image_dims = 3, 32, 32\n",
        "n_training_samples = 20000 # How many training images to use\n",
        "n_test_samples = 5000 # How many test images to use\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Load the training set\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./cifardata', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_sampler = SubsetRandomSampler(\n",
        "    np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root='./cifardata', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbZb_B62w_fg",
        "colab_type": "text"
      },
      "source": [
        "Explore the data a bit.  Here are some suggestions.  (If you haven't yet, enable \n",
        "* Check the shapes of various tensors (note that the training data is stored under `train_set.data`)\n",
        "* Visualize some of the images in the dataset (you can grab data from the train_set using square brackets).  Sample code below. In the past we've shown you how to use subplots to show many images at once (refer to, e.g., the [assignment 5 notebook](https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/05/assignment05.ipynb) for guidance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l-xHj-Gw-7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "30d420ff-a09c-44b4-bb14-fa7cec1f03a4"
      },
      "source": [
        "def disp_image(image, class_idx, predicted=None):\n",
        "    # need to reorder the tensor dimensions to work properly with imshow\n",
        "    plt.imshow(image.transpose(0,2).transpose(0,1))\n",
        "    plt.axis('off')\n",
        "    if predicted:\n",
        "        plt.title(\"Actual: \" + classes[class_idx] + \"     Predicted: \" + classes[predicted])\n",
        "    else:\n",
        "        plt.title(\"Actual: \" + classes[class_idx])\n",
        "    plt.show()\n",
        "\n",
        "print(\"training set input data shape\", train_set.data.shape)\n",
        "print(\"Number of training outputs\", len(train_set.targets))\n",
        "x, y = train_set[1]\n",
        "disp_image(x, y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set input data shape (50000, 32, 32, 3)\n",
            "Number of training outputs 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi1zzYOazFJ4",
        "colab_type": "text"
      },
      "source": [
        "As another quick example, let's show the number of training data points for each particular class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1i02DlZzJzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "8c84c96f-fd0f-4dc6-fd6a-847e77d99a14"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(train_set.targets)\n",
        "plt.xticks(ticks=range(10), labels=classes)\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBockqReDQ5LUi8EhSerl/wNDBUUZr2CH/wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Z46lIIzLP8",
        "colab_type": "text"
      },
      "source": [
        "Add your own explorations as you see fit, or skip ahead for now if you want to spend more time on the convnet stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jc8znegzlB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcfcP8pzdLNo",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "Now that we have the data loaded, let's define our model architecture. As you saw on the previous assignment, convolutional networks have the following basic layer structure.\n",
        "1. Convolutional layer (this helps detect various image features such as edges and corners)\n",
        "2. (optional) non-linear tranformation (e.g., ReLu)\n",
        "3. Max pooling (this reduces the dimensionality of the image and focuses the network on features that are most salient)\n",
        "4. (repeat 1-3 some number of times)\n",
        "5. Fully connected layer\n",
        "6. (repeat (4) some number of times)\n",
        "7. Output layer\n",
        "\n",
        "So a typically convnet would look something like this.  Note: that the softmax in the figure below is just the multiclass generalization of the sigmoid we've been using for binary classification.\n",
        "\n",
        "![](https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
        "\n",
        "For this example we're going to start off by using one convolutional layer to pick up local image features, then a [maxpool operation](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) to reduce our dimensionality a bit, followed by a fully connected layer to perform some logic computations on those features, and finally a fully connected layer too turn those outputs into predictions for the class.  Typically we might have more of each of these sorts of layers, but we are aiming for a simpler network for our first go.\n",
        "\n",
        "## Software Architecture\n",
        "\n",
        "In `pytorch` our neural network will be a class.  Here are a few things to remember about how `pytorch` works with neural networks.\n",
        "\n",
        "* Your neural network class must inherit from `nn.Module`\n",
        "* You should create the layer objects in the `__init__` method.\n",
        "* The forward call is where the action happens (data inputs are transformed into outputs of the network).\n",
        "\n",
        "Consider consulting [the assignment 7 notebook](https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/07/Assignment_07_Companion_Pytorch_Titanic.ipynb) for a refresher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbxu5y57dLn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "    # The init funciton in Pytorch classes is used to keep track of the parameters of the model\n",
        "    # specifically the ones we want to update with gradient descent + backprop\n",
        "    # So we need to make sure we keep track of all of them here\n",
        "    def __init__(self):\n",
        "        super(MyCNN, self).__init__()\n",
        "        # layers defined here\n",
        "\n",
        "        # Make sure you understand what this convolutional layer is doing.\n",
        "        # E.g., considering looking at help(nn.Conv2D).  Draw a picture of what\n",
        "        # this layer does to the data.\n",
        "\n",
        "        # note: image_dims[0] will be 3 as there are 3 color channels (R, G, B)\n",
        "        num_kernels = 16\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Make sure you understand what this MaxPool2D layer is doing.\n",
        "        # E.g., considering looking at help(nn.MaxPool2D).  Draw a picture of\n",
        "        # what this layer does to the data.\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # maxpool_output_size is the total amount of data coming out of that\n",
        "        # layer.  Explain why the line of code below computes this quantity.\n",
        "        self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
        "\n",
        "        # Add on a fully connected layer (like in our MLP)\n",
        "        # fc stands for fully connected\n",
        "        fc1_size = 64\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
        "\n",
        "        # we'll use this activation function internally in the network\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "        # Convert our fully connected layer into outputs that we can compare to the result\n",
        "        fc2_size = len(classes)\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
        "\n",
        "        # Note: that the output will not represent the probability of the\n",
        "        # output being in each class.  The loss function we will use\n",
        "        # `CrossEntropyLoss` will take care of convering these values to\n",
        "        # probabilities and then computing the log loss with respect to the\n",
        "        # true label.  We could break this out into multiple steps, but it turns\n",
        "        # out that the algorithm will be more numerically stable if we do it in\n",
        "        # one go.  We have included a cell to show you the documentation for\n",
        "        # `CrossEntropyLoss` if you'd like to check it out.\n",
        "        \n",
        "    # The forward function in the class defines the operations performed on a given input to the model\n",
        "    # and returns the output of the model\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.activation_func(x)\n",
        "        # this code flattens the output of the convolution, max pool,\n",
        "        # activation sequence of steps into a vector\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
        "    # returns the loss and optimizer used by the model\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7yW3u9F4Rtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e579e8a-272b-481d-c630-111028d5a598"
      },
      "source": [
        "help(nn.CrossEntropyLoss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class CrossEntropyLoss in module torch.nn.modules.loss:\n",
            "\n",
            "class CrossEntropyLoss(_WeightedLoss)\n",
            " |  This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.\n",
            " |  \n",
            " |  It is useful when training a classification problem with `C` classes.\n",
            " |  If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
            " |  assigning weight to each of the classes.\n",
            " |  This is particularly useful when you have an unbalanced training set.\n",
            " |  \n",
            " |  The `input` is expected to contain raw, unnormalized scores for each class.\n",
            " |  \n",
            " |  `input` has to be a Tensor of size either :math:`(minibatch, C)` or\n",
            " |  :math:`(minibatch, C, d_1, d_2, ..., d_K)`\n",
            " |  with :math:`K \\geq 1` for the `K`-dimensional case (described later).\n",
            " |  \n",
            " |  This criterion expects a class index in the range :math:`[0, C-1]` as the\n",
            " |  `target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`\n",
            " |  is specified, this criterion also accepts this class index (this index may not\n",
            " |  necessarily be in the class range).\n",
            " |  \n",
            " |  The loss can be described as:\n",
            " |  \n",
            " |  .. math::\n",
            " |      \\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
            " |                     = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\n",
            " |  \n",
            " |  or in the case of the :attr:`weight` argument being specified:\n",
            " |  \n",
            " |  .. math::\n",
            " |      \\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)\n",
            " |  \n",
            " |  The losses are averaged across observations for each minibatch.\n",
            " |  \n",
            " |  Can also be used for higher dimension inputs, such as 2D images, by providing\n",
            " |  an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`,\n",
            " |  where :math:`K` is the number of dimensions, and a target of appropriate shape\n",
            " |  (see below).\n",
            " |  \n",
            " |  \n",
            " |  Args:\n",
            " |      weight (Tensor, optional): a manual rescaling weight given to each class.\n",
            " |          If given, has to be a Tensor of size `C`\n",
            " |      size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            " |          the losses are averaged over each loss element in the batch. Note that for\n",
            " |          some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
            " |          is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            " |          when reduce is ``False``. Default: ``True``\n",
            " |      ignore_index (int, optional): Specifies a target value that is ignored\n",
            " |          and does not contribute to the input gradient. When :attr:`size_average` is\n",
            " |          ``True``, the loss is averaged over non-ignored targets.\n",
            " |      reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            " |          losses are averaged or summed over observations for each minibatch depending\n",
            " |          on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            " |          batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            " |      reduction (string, optional): Specifies the reduction to apply to the output:\n",
            " |          ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            " |          ``'mean'``: the sum of the output will be divided by the number of\n",
            " |          elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            " |          and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            " |          specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            " |  \n",
            " |  Shape:\n",
            " |      - Input: :math:`(N, C)` where `C = number of classes`, or\n",
            " |        :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
            " |        in the case of `K`-dimensional loss.\n",
            " |      - Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n",
            " |        :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n",
            " |        K-dimensional loss.\n",
            " |      - Output: scalar.\n",
            " |        If :attr:`reduction` is ``'none'``, then the same size as the target:\n",
            " |        :math:`(N)`, or\n",
            " |        :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case\n",
            " |        of K-dimensional loss.\n",
            " |  \n",
            " |  Examples::\n",
            " |  \n",
            " |      >>> loss = nn.CrossEntropyLoss()\n",
            " |      >>> input = torch.randn(3, 5, requires_grad=True)\n",
            " |      >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
            " |      >>> output = loss(input, target)\n",
            " |      >>> output.backward()\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CrossEntropyLoss\n",
            " |      _WeightedLoss\n",
            " |      _Loss\n",
            " |      torch.nn.modules.module.Module\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  forward(self, input, target)\n",
            " |      Defines the computation performed at every call.\n",
            " |      \n",
            " |      Should be overridden by all subclasses.\n",
            " |      \n",
            " |      .. note::\n",
            " |          Although the recipe for forward pass needs to be defined within\n",
            " |          this function, one should call the :class:`Module` instance afterwards\n",
            " |          instead of this since the former takes care of running the\n",
            " |          registered hooks while the latter silently ignores them.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __constants__ = ['weight', 'ignore_index', 'reduction']\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __call__(self, *input, **kwargs)\n",
            " |      Call self as a function.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      __dir__() -> list\n",
            " |      default dir() implementation\n",
            " |  \n",
            " |  __getattr__(self, name)\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_module(self, name, module)\n",
            " |      Adds a child module to the current module.\n",
            " |      \n",
            " |      The module can be accessed as an attribute using the given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the child module. The child module can be\n",
            " |              accessed from this module using the given name\n",
            " |          module (Module): child module to be added to the module.\n",
            " |  \n",
            " |  apply(self, fn)\n",
            " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
            " |      as well as self. Typical use includes initializing the parameters of a model\n",
            " |      (see also :ref:`torch-nn-init`).\n",
            " |      \n",
            " |      Args:\n",
            " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> def init_weights(m):\n",
            " |          >>>     print(m)\n",
            " |          >>>     if type(m) == nn.Linear:\n",
            " |          >>>         m.weight.data.fill_(1.0)\n",
            " |          >>>         print(m.weight)\n",
            " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
            " |          >>> net.apply(init_weights)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 1.,  1.],\n",
            " |                  [ 1.,  1.]])\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 1.,  1.],\n",
            " |                  [ 1.,  1.]])\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |  \n",
            " |  buffers(self, recurse=True)\n",
            " |      Returns an iterator over module buffers.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          torch.Tensor: module buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for buf in model.buffers():\n",
            " |          >>>     print(type(buf.data), buf.size())\n",
            " |          <class 'torch.FloatTensor'> (20L,)\n",
            " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  children(self)\n",
            " |      Returns an iterator over immediate children modules.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a child module\n",
            " |  \n",
            " |  cpu(self)\n",
            " |      Moves all model parameters and buffers to the CPU.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  cuda(self, device=None)\n",
            " |      Moves all model parameters and buffers to the GPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on GPU while being optimized.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  double(self)\n",
            " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  eval(self)\n",
            " |      Sets the module in evaluation mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |  \n",
            " |  extra_repr(self)\n",
            " |      Set the extra representation of the module\n",
            " |      \n",
            " |      To print customized extra information, you should reimplement\n",
            " |      this method in your own modules. Both single-line and multi-line\n",
            " |      strings are acceptable.\n",
            " |  \n",
            " |  float(self)\n",
            " |      Casts all floating point parameters and buffers to float datatype.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  half(self)\n",
            " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  load_state_dict(self, state_dict, strict=True)\n",
            " |      Copies parameters and buffers from :attr:`state_dict` into\n",
            " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
            " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
            " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          state_dict (dict): a dict containing parameters and\n",
            " |              persistent buffers.\n",
            " |          strict (bool, optional): whether to strictly enforce that the keys\n",
            " |              in :attr:`state_dict` match the keys returned by this module's\n",
            " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
            " |      \n",
            " |      Returns:\n",
            " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
            " |              * **missing_keys** is a list of str containing the missing keys\n",
            " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
            " |  \n",
            " |  modules(self)\n",
            " |      Returns an iterator over all modules in the network.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a module in the network\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.modules()):\n",
            " |                  print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
            " |  \n",
            " |  named_buffers(self, prefix='', recurse=True)\n",
            " |      Returns an iterator over module buffers, yielding both the\n",
            " |      name of the buffer as well as the buffer itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all buffer names.\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, buf in self.named_buffers():\n",
            " |          >>>    if name in ['running_var']:\n",
            " |          >>>        print(buf.size())\n",
            " |  \n",
            " |  named_children(self)\n",
            " |      Returns an iterator over immediate children modules, yielding both\n",
            " |      the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Module): Tuple containing a name and child module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, module in model.named_children():\n",
            " |          >>>     if name in ['conv4', 'conv5']:\n",
            " |          >>>         print(module)\n",
            " |  \n",
            " |  named_modules(self, memo=None, prefix='')\n",
            " |      Returns an iterator over all modules in the network, yielding\n",
            " |      both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Module): Tuple of name and module\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.named_modules()):\n",
            " |                  print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> ('', Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          ))\n",
            " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
            " |  \n",
            " |  named_parameters(self, prefix='', recurse=True)\n",
            " |      Returns an iterator over module parameters, yielding both the\n",
            " |      name of the parameter as well as the parameter itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all parameter names.\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Parameter): Tuple containing the name and parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, param in self.named_parameters():\n",
            " |          >>>    if name in ['bias']:\n",
            " |          >>>        print(param.size())\n",
            " |  \n",
            " |  parameters(self, recurse=True)\n",
            " |      Returns an iterator over module parameters.\n",
            " |      \n",
            " |      This is typically passed to an optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Parameter: module parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for param in model.parameters():\n",
            " |          >>>     print(type(param.data), param.size())\n",
            " |          <class 'torch.FloatTensor'> (20L,)\n",
            " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  register_backward_hook(self, hook)\n",
            " |      Registers a backward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients with respect to module\n",
            " |      inputs are computed. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
            " |      \n",
            " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
            " |      module has multiple inputs or outputs. The hook should not modify its\n",
            " |      arguments, but it can optionally return a new gradient with respect to\n",
            " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
            " |      computations.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |      \n",
            " |      .. warning ::\n",
            " |      \n",
            " |          The current implementation will not have the presented behavior\n",
            " |          for complex :class:`Module` that perform many operations.\n",
            " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
            " |          contain the gradients for a subset of the inputs and outputs.\n",
            " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
            " |          directly on a specific input or output to get the required gradients.\n",
            " |  \n",
            " |  register_buffer(self, name, tensor)\n",
            " |      Adds a persistent buffer to the module.\n",
            " |      \n",
            " |      This is typically used to register a buffer that should not to be\n",
            " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
            " |      is not a parameter, but is part of the persistent state.\n",
            " |      \n",
            " |      Buffers can be accessed as attributes using given names.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the buffer. The buffer can be accessed\n",
            " |              from this module using the given name\n",
            " |          tensor (Tensor): buffer to be registered.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
            " |  \n",
            " |  register_forward_hook(self, hook)\n",
            " |      Registers a forward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time after :func:`forward` has computed an output.\n",
            " |      It should have the following signature::\n",
            " |      \n",
            " |          hook(module, input, output) -> None\n",
            " |      \n",
            " |      The hook should not modify the input or output.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_forward_pre_hook(self, hook)\n",
            " |      Registers a forward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time before :func:`forward` is invoked.\n",
            " |      It should have the following signature::\n",
            " |      \n",
            " |          hook(module, input) -> None\n",
            " |      \n",
            " |      The hook should not modify the input.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_parameter(self, name, param)\n",
            " |      Adds a parameter to the module.\n",
            " |      \n",
            " |      The parameter can be accessed as an attribute using given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the parameter. The parameter can be accessed\n",
            " |              from this module using the given name\n",
            " |          param (Parameter): parameter to be added to the module.\n",
            " |  \n",
            " |  share_memory(self)\n",
            " |  \n",
            " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
            " |      Returns a dictionary containing a whole state of the module.\n",
            " |      \n",
            " |      Both parameters and persistent buffers (e.g. running averages) are\n",
            " |      included. Keys are corresponding parameter and buffer names.\n",
            " |      \n",
            " |      Returns:\n",
            " |          dict:\n",
            " |              a dictionary containing a whole state of the module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> module.state_dict().keys()\n",
            " |          ['bias', 'weight']\n",
            " |  \n",
            " |  to(self, *args, **kwargs)\n",
            " |      Moves and/or casts the parameters and buffers.\n",
            " |      \n",
            " |      This can be called as\n",
            " |      \n",
            " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
            " |      \n",
            " |      .. function:: to(dtype, non_blocking=False)\n",
            " |      \n",
            " |      .. function:: to(tensor, non_blocking=False)\n",
            " |      \n",
            " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
            " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
            " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
            " |      (if given). The integral parameters and buffers will be moved\n",
            " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
            " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
            " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
            " |      pinned memory to CUDA devices.\n",
            " |      \n",
            " |      See below for examples.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): the desired device of the parameters\n",
            " |              and buffers in this module\n",
            " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
            " |              the floating point parameters and buffers in this module\n",
            " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
            " |              dtype and device for all parameters and buffers in this module\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]])\n",
            " |          >>> linear.to(torch.double)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
            " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
            " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
            " |          >>> cpu = torch.device(\"cpu\")\n",
            " |          >>> linear.to(cpu)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
            " |  \n",
            " |  train(self, mode=True)\n",
            " |      Sets the module in training mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  type(self, dst_type)\n",
            " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          dst_type (type or string): the desired type\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  zero_grad(self)\n",
            " |      Sets gradients of all model parameters to zero.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  dump_patches = False\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQHPW08-f5fK",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "First let's create our model. Let's also check out a graphical representation of our model (using a library we downloaded earlier) to validate the model looks like we think it should.  This is definitely not the prettiest visualization, and there are lots of things included in here that are related to doing the backward pass (to compute the gradients).  Of particular relevance are the blue nodes, which tell you about the various model parameters and layers.\n",
        "\n",
        "**Running the below cell will override your model if have already trained one**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXeauNXlBIK6",
        "colab_type": "code",
        "outputId": "c500120b-2ccd-4220-ef0a-11680fcc234d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1,image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "\n",
        "visualize_network(net)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7eff60492630>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"267pt\" height=\"493pt\"\n viewBox=\"0.00 0.00 266.50 493.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 489)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-489 262.5,-489 262.5,4 -4,4\"/>\n<!-- 139635297167512 -->\n<g id=\"node1\" class=\"node\">\n<title>139635297167512</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"171,-21 67,-21 67,0 171,0 171,-21\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139635297168072 -->\n<g id=\"node2\" class=\"node\">\n<title>139635297168072</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-91 0,-91 0,-57 54,-57 54,-91\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 139635297168072&#45;&gt;139635297167512 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139635297168072&#45;&gt;139635297167512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.6543,-56.9832C65.1894,-47.641 81.8926,-36.1122 95.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.2865,-29.7398 103.5283,-21.1788 93.3102,-23.9788 97.2865,-29.7398\"/>\n</g>\n<!-- 139635297167456 -->\n<g id=\"node3\" class=\"node\">\n<title>139635297167456</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"166,-84.5 72,-84.5 72,-63.5 166,-63.5 166,-84.5\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139635297167456&#45;&gt;139635297167512 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139635297167456&#45;&gt;139635297167512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119,-63.2281C119,-54.5091 119,-41.9699 119,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.5001,-31.1128 119,-21.1128 115.5001,-31.1129 122.5001,-31.1128\"/>\n</g>\n<!-- 139635297166504 -->\n<g id=\"node4\" class=\"node\">\n<title>139635297166504</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170,-154.5 66,-154.5 66,-133.5 170,-133.5 170,-154.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139635297166504&#45;&gt;139635297167456 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139635297166504&#45;&gt;139635297167456</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118.1519,-133.3685C118.2972,-123.1925 118.5206,-107.5606 118.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.2034,-94.7806 118.8467,-84.7315 115.2041,-94.6805 122.2034,-94.7806\"/>\n</g>\n<!-- 139635297166560 -->\n<g id=\"node5\" class=\"node\">\n<title>139635297166560</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-231 0,-231 0,-197 54,-197 54,-231\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139635297166560&#45;&gt;139635297166504 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139635297166560&#45;&gt;139635297166504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49.4944,-196.6966C63.7034,-185.7666 81.9745,-171.7119 96.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.447,-163.4565 104.2393,-154.5852 94.179,-157.9081 98.447,-163.4565\"/>\n</g>\n<!-- 139635297166840 -->\n<g id=\"node6\" class=\"node\">\n<title>139635297166840</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-224.5 72.5,-224.5 72.5,-203.5 163.5,-203.5 163.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139635297166840&#45;&gt;139635297166504 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139635297166840&#45;&gt;139635297166504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-203.3685C118,-193.1925 118,-177.5606 118,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-164.7315 118,-154.7315 114.5001,-164.7316 121.5001,-164.7315\"/>\n</g>\n<!-- 139635297167176 -->\n<g id=\"node7\" class=\"node\">\n<title>139635297167176</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-294.5 71,-294.5 71,-273.5 165,-273.5 165,-294.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139635297167176&#45;&gt;139635297166840 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139635297167176&#45;&gt;139635297166840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-273.3685C118,-263.1925 118,-247.5606 118,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-234.7315 118,-224.7315 114.5001,-234.7316 121.5001,-234.7315\"/>\n</g>\n<!-- 139635297167008 -->\n<g id=\"node8\" class=\"node\">\n<title>139635297167008</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-358 28,-358 28,-337 208,-337 208,-358\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139635297167008&#45;&gt;139635297167176 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139635297167008&#45;&gt;139635297167176</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-336.7281C118,-328.0091 118,-315.4699 118,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-304.6128 118,-294.6128 114.5001,-304.6129 121.5001,-304.6128\"/>\n</g>\n<!-- 139635297169080 -->\n<g id=\"node9\" class=\"node\">\n<title>139635297169080</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196.5,-415 39.5,-415 39.5,-394 196.5,-394 196.5,-415\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139635297169080&#45;&gt;139635297167008 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139635297169080&#45;&gt;139635297167008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-393.7787C118,-386.6134 118,-376.9517 118,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-368.1732 118,-358.1732 114.5001,-368.1732 121.5001,-368.1732\"/>\n</g>\n<!-- 139635297169248 -->\n<g id=\"node10\" class=\"node\">\n<title>139635297169248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"112.5,-485 31.5,-485 31.5,-451 112.5,-451 112.5,-485\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"72\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 3, 3, 3)</text>\n</g>\n<!-- 139635297169248&#45;&gt;139635297169080 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139635297169248&#45;&gt;139635297169080</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.3272,-450.9832C90.4107,-442.5853 97.7742,-432.4204 104.0621,-423.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.0945,-425.5204 110.1266,-415.3687 101.4256,-421.4138 107.0945,-425.5204\"/>\n</g>\n<!-- 139635297169304 -->\n<g id=\"node11\" class=\"node\">\n<title>139635297169304</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-485 131,-485 131,-451 199,-451 199,-485\"/>\n<text text-anchor=\"middle\" x=\"165\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"165\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 139635297169304&#45;&gt;139635297169080 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139635297169304&#45;&gt;139635297169080</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M152.4049,-450.9832C146.1237,-442.4969 138.5069,-432.2062 132.0384,-423.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.8071,-421.3243 126.0445,-415.3687 129.1806,-425.4888 134.8071,-421.3243\"/>\n</g>\n<!-- 139635297166952 -->\n<g id=\"node12\" class=\"node\">\n<title>139635297166952</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"254.5,-224.5 181.5,-224.5 181.5,-203.5 254.5,-203.5 254.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139635297166952&#45;&gt;139635297166504 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139635297166952&#45;&gt;139635297166504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M202.8122,-203.3685C186.492,-191.9444 160.3486,-173.644 141.398,-160.3786\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.2026,-157.3695 133.0031,-154.5022 139.1883,-163.1042 143.2026,-157.3695\"/>\n</g>\n<!-- 139635297167232 -->\n<g id=\"node13\" class=\"node\">\n<title>139635297167232</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"252.5,-301 183.5,-301 183.5,-267 252.5,-267 252.5,-301\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"218\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 4096)</text>\n</g>\n<!-- 139635297167232&#45;&gt;139635297166952 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139635297167232&#45;&gt;139635297166952</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218,-266.6966C218,-257.0634 218,-245.003 218,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"221.5001,-234.7912 218,-224.7913 214.5001,-234.7913 221.5001,-234.7912\"/>\n</g>\n<!-- 139635297167400 -->\n<g id=\"node14\" class=\"node\">\n<title>139635297167400</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.5,-84.5 185.5,-84.5 185.5,-63.5 258.5,-63.5 258.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139635297167400&#45;&gt;139635297167512 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139635297167400&#45;&gt;139635297167512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.5275,-63.2281C188.1519,-53.1325 163.4682,-37.9149 144.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.5636,-23.3814 136.2145,-21.1128 142.8901,-29.3401 146.5636,-23.3814\"/>\n</g>\n<!-- 139635297166672 -->\n<g id=\"node15\" class=\"node\">\n<title>139635297166672</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"255.5,-161 188.5,-161 188.5,-127 255.5,-127 255.5,-161\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"222\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 64)</text>\n</g>\n<!-- 139635297166672&#45;&gt;139635297167400 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139635297166672&#45;&gt;139635297167400</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M222,-126.6966C222,-117.0634 222,-105.003 222,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.5001,-94.7912 222,-84.7913 218.5001,-94.7913 225.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glMz-8OmoF-u",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Next we'll define the settings we'll use for training.  \n",
        "\n",
        "In the last notebook we mentioned the idea of stochastic gradient descent where we only use a subset of the data to estimate the gradient before taking doing an update to our model parameters.  In the notebook from last time instead we used all of our data to compute the gradient (thus we just used regular gradient descent). Although reliable, this method is often slow for larger models.\n",
        "\n",
        "In this problem we are going to be using a form of **Stochastic Gradient Descent** called **Mini-batch Gradient Descent**.  For mini-batch gradient descent we will use a small batch of data to estimate our gradient and tehn do a step.  We'll iterate through the whole dataset as a series of mini-batches and perform a step after processing each one.  This is a much noisier process of weight optimization, but often converges quicker than the normal gradient descent.\n",
        "\n",
        "In the code below we define a `DataLoader` function that iterates through the training set (or test set) in increments of `batch_size`. We then define a training wrapper function that will make modifying your model parameters (as we'll do later in this notebook) easier (we don't want to have to keep cutting and pasting code everywhere in this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I52pm5-AmwKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 10\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                           sampler=train_sampler, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=128, sampler=test_sampler, num_workers=2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 20\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQorZsxQBIc6",
        "colab_type": "text"
      },
      "source": [
        "### Now let's train the model!\n",
        "\n",
        "Here we go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvcJyKgwdSR8",
        "colab_type": "code",
        "outputId": "6d1720b0-9111-4e01-e523-335a795f1a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 20\t train_loss: 1.12 took: 0.23s\n",
            "Epoch 1, Iteration 40\t train_loss: 1.21 took: 0.12s\n",
            "Epoch 1, Iteration 60\t train_loss: 1.15 took: 0.13s\n",
            "Epoch 1, Iteration 80\t train_loss: 1.05 took: 0.13s\n",
            "Epoch 1, Iteration 100\t train_loss: 1.05 took: 0.12s\n",
            "Epoch 1, Iteration 120\t train_loss: 1.14 took: 0.13s\n",
            "Epoch 1, Iteration 140\t train_loss: 1.13 took: 0.13s\n",
            "Epoch 1, Iteration 160\t train_loss: 1.12 took: 0.11s\n",
            "Epoch 1, Iteration 180\t train_loss: 1.06 took: 0.12s\n",
            "Epoch 1, Iteration 200\t train_loss: 1.09 took: 0.12s\n",
            "Epoch 1, Iteration 220\t train_loss: 1.12 took: 0.12s\n",
            "Epoch 1, Iteration 240\t train_loss: 1.16 took: 0.14s\n",
            "Epoch 1, Iteration 260\t train_loss: 1.07 took: 0.14s\n",
            "Epoch 1, Iteration 280\t train_loss: 1.24 took: 0.13s\n",
            "Epoch 1, Iteration 300\t train_loss: 1.17 took: 0.13s\n",
            "Epoch 1, Iteration 320\t train_loss: 1.15 took: 0.12s\n",
            "Epoch 1, Iteration 340\t train_loss: 1.16 took: 0.12s\n",
            "Epoch 1, Iteration 360\t train_loss: 1.14 took: 0.12s\n",
            "Epoch 1, Iteration 380\t train_loss: 1.20 took: 0.13s\n",
            "Epoch 1, Iteration 400\t train_loss: 1.17 took: 0.12s\n",
            "Epoch 1, Iteration 420\t train_loss: 1.11 took: 0.13s\n",
            "Epoch 1, Iteration 440\t train_loss: 1.22 took: 0.12s\n",
            "Epoch 1, Iteration 460\t train_loss: 1.31 took: 0.14s\n",
            "Epoch 1, Iteration 480\t train_loss: 1.23 took: 0.12s\n",
            "Epoch 1, Iteration 500\t train_loss: 1.17 took: 0.13s\n",
            "Epoch 1, Iteration 520\t train_loss: 1.11 took: 0.13s\n",
            "Epoch 1, Iteration 540\t train_loss: 1.12 took: 0.12s\n",
            "Epoch 1, Iteration 560\t train_loss: 1.16 took: 0.12s\n",
            "Epoch 1, Iteration 580\t train_loss: 1.21 took: 0.13s\n",
            "Epoch 1, Iteration 600\t train_loss: 1.25 took: 0.12s\n",
            "Epoch 1, Iteration 620\t train_loss: 1.15 took: 0.13s\n",
            "Validation loss = 1.84\n",
            "Epoch 2, Iteration 20\t train_loss: 0.98 took: 0.22s\n",
            "Epoch 2, Iteration 40\t train_loss: 0.97 took: 0.12s\n",
            "Epoch 2, Iteration 60\t train_loss: 1.01 took: 0.12s\n",
            "Epoch 2, Iteration 80\t train_loss: 1.08 took: 0.12s\n",
            "Epoch 2, Iteration 100\t train_loss: 1.06 took: 0.11s\n",
            "Epoch 2, Iteration 120\t train_loss: 1.19 took: 0.13s\n",
            "Epoch 2, Iteration 140\t train_loss: 1.15 took: 0.12s\n",
            "Epoch 2, Iteration 160\t train_loss: 1.03 took: 0.12s\n",
            "Epoch 2, Iteration 180\t train_loss: 1.08 took: 0.12s\n",
            "Epoch 2, Iteration 200\t train_loss: 1.12 took: 0.14s\n",
            "Epoch 2, Iteration 220\t train_loss: 1.05 took: 0.12s\n",
            "Epoch 2, Iteration 240\t train_loss: 1.04 took: 0.12s\n",
            "Epoch 2, Iteration 260\t train_loss: 1.13 took: 0.14s\n",
            "Epoch 2, Iteration 280\t train_loss: 1.10 took: 0.13s\n",
            "Epoch 2, Iteration 300\t train_loss: 1.17 took: 0.12s\n",
            "Epoch 2, Iteration 320\t train_loss: 1.14 took: 0.12s\n",
            "Epoch 2, Iteration 340\t train_loss: 1.09 took: 0.13s\n",
            "Epoch 2, Iteration 360\t train_loss: 1.12 took: 0.13s\n",
            "Epoch 2, Iteration 380\t train_loss: 1.18 took: 0.12s\n",
            "Epoch 2, Iteration 400\t train_loss: 1.11 took: 0.14s\n",
            "Epoch 2, Iteration 420\t train_loss: 1.09 took: 0.13s\n",
            "Epoch 2, Iteration 440\t train_loss: 1.14 took: 0.12s\n",
            "Epoch 2, Iteration 460\t train_loss: 1.12 took: 0.12s\n",
            "Epoch 2, Iteration 480\t train_loss: 1.10 took: 0.12s\n",
            "Epoch 2, Iteration 500\t train_loss: 1.07 took: 0.12s\n",
            "Epoch 2, Iteration 520\t train_loss: 1.18 took: 0.14s\n",
            "Epoch 2, Iteration 540\t train_loss: 1.16 took: 0.13s\n",
            "Epoch 2, Iteration 560\t train_loss: 1.15 took: 0.12s\n",
            "Epoch 2, Iteration 580\t train_loss: 1.14 took: 0.13s\n",
            "Epoch 2, Iteration 600\t train_loss: 1.20 took: 0.12s\n",
            "Epoch 2, Iteration 620\t train_loss: 1.09 took: 0.13s\n",
            "Validation loss = 1.98\n",
            "Epoch 3, Iteration 20\t train_loss: 1.07 took: 0.21s\n",
            "Epoch 3, Iteration 40\t train_loss: 1.00 took: 0.12s\n",
            "Epoch 3, Iteration 60\t train_loss: 0.98 took: 0.12s\n",
            "Epoch 3, Iteration 80\t train_loss: 1.00 took: 0.11s\n",
            "Epoch 3, Iteration 100\t train_loss: 1.02 took: 0.14s\n",
            "Epoch 3, Iteration 120\t train_loss: 1.07 took: 0.12s\n",
            "Epoch 3, Iteration 140\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 3, Iteration 160\t train_loss: 1.11 took: 0.13s\n",
            "Epoch 3, Iteration 180\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 3, Iteration 200\t train_loss: 1.03 took: 0.13s\n",
            "Epoch 3, Iteration 220\t train_loss: 1.06 took: 0.12s\n",
            "Epoch 3, Iteration 240\t train_loss: 1.02 took: 0.12s\n",
            "Epoch 3, Iteration 260\t train_loss: 1.08 took: 0.14s\n",
            "Epoch 3, Iteration 280\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 3, Iteration 300\t train_loss: 1.09 took: 0.13s\n",
            "Epoch 3, Iteration 320\t train_loss: 1.12 took: 0.13s\n",
            "Epoch 3, Iteration 340\t train_loss: 1.12 took: 0.12s\n",
            "Epoch 3, Iteration 360\t train_loss: 1.10 took: 0.12s\n",
            "Epoch 3, Iteration 380\t train_loss: 1.08 took: 0.12s\n",
            "Epoch 3, Iteration 400\t train_loss: 1.15 took: 0.12s\n",
            "Epoch 3, Iteration 420\t train_loss: 1.17 took: 0.13s\n",
            "Epoch 3, Iteration 440\t train_loss: 1.14 took: 0.12s\n",
            "Epoch 3, Iteration 460\t train_loss: 1.24 took: 0.13s\n",
            "Epoch 3, Iteration 480\t train_loss: 1.15 took: 0.13s\n",
            "Epoch 3, Iteration 500\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 3, Iteration 520\t train_loss: 1.05 took: 0.12s\n",
            "Epoch 3, Iteration 540\t train_loss: 1.05 took: 0.11s\n",
            "Epoch 3, Iteration 560\t train_loss: 1.09 took: 0.12s\n",
            "Epoch 3, Iteration 580\t train_loss: 1.15 took: 0.13s\n",
            "Epoch 3, Iteration 600\t train_loss: 1.15 took: 0.14s\n",
            "Epoch 3, Iteration 620\t train_loss: 1.07 took: 0.14s\n",
            "Validation loss = 2.00\n",
            "Epoch 4, Iteration 20\t train_loss: 0.91 took: 0.23s\n",
            "Epoch 4, Iteration 40\t train_loss: 0.96 took: 0.12s\n",
            "Epoch 4, Iteration 60\t train_loss: 0.98 took: 0.12s\n",
            "Epoch 4, Iteration 80\t train_loss: 0.99 took: 0.11s\n",
            "Epoch 4, Iteration 100\t train_loss: 0.89 took: 0.13s\n",
            "Epoch 4, Iteration 120\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 4, Iteration 140\t train_loss: 1.03 took: 0.13s\n",
            "Epoch 4, Iteration 160\t train_loss: 0.93 took: 0.13s\n",
            "Epoch 4, Iteration 180\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 4, Iteration 200\t train_loss: 1.12 took: 0.13s\n",
            "Epoch 4, Iteration 220\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 4, Iteration 240\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 4, Iteration 260\t train_loss: 1.10 took: 0.13s\n",
            "Epoch 4, Iteration 280\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 4, Iteration 300\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 4, Iteration 320\t train_loss: 0.95 took: 0.13s\n",
            "Epoch 4, Iteration 340\t train_loss: 1.07 took: 0.12s\n",
            "Epoch 4, Iteration 360\t train_loss: 1.13 took: 0.12s\n",
            "Epoch 4, Iteration 380\t train_loss: 1.20 took: 0.13s\n",
            "Epoch 4, Iteration 400\t train_loss: 1.09 took: 0.12s\n",
            "Epoch 4, Iteration 420\t train_loss: 1.11 took: 0.12s\n",
            "Epoch 4, Iteration 440\t train_loss: 0.98 took: 0.12s\n",
            "Epoch 4, Iteration 460\t train_loss: 1.15 took: 0.13s\n",
            "Epoch 4, Iteration 480\t train_loss: 1.04 took: 0.12s\n",
            "Epoch 4, Iteration 500\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 4, Iteration 520\t train_loss: 1.15 took: 0.13s\n",
            "Epoch 4, Iteration 540\t train_loss: 1.11 took: 0.13s\n",
            "Epoch 4, Iteration 560\t train_loss: 1.08 took: 0.12s\n",
            "Epoch 4, Iteration 580\t train_loss: 1.04 took: 0.13s\n",
            "Epoch 4, Iteration 600\t train_loss: 1.15 took: 0.11s\n",
            "Epoch 4, Iteration 620\t train_loss: 1.10 took: 0.13s\n",
            "Validation loss = 2.09\n",
            "Epoch 5, Iteration 20\t train_loss: 0.94 took: 0.22s\n",
            "Epoch 5, Iteration 40\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 5, Iteration 60\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 5, Iteration 80\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 5, Iteration 100\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 5, Iteration 120\t train_loss: 0.95 took: 0.13s\n",
            "Epoch 5, Iteration 140\t train_loss: 0.91 took: 0.13s\n",
            "Epoch 5, Iteration 160\t train_loss: 1.03 took: 0.13s\n",
            "Epoch 5, Iteration 180\t train_loss: 0.99 took: 0.13s\n",
            "Epoch 5, Iteration 200\t train_loss: 1.00 took: 0.13s\n",
            "Epoch 5, Iteration 220\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 5, Iteration 240\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 5, Iteration 260\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 5, Iteration 280\t train_loss: 1.07 took: 0.13s\n",
            "Epoch 5, Iteration 300\t train_loss: 1.04 took: 0.14s\n",
            "Epoch 5, Iteration 320\t train_loss: 0.95 took: 0.13s\n",
            "Epoch 5, Iteration 340\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 5, Iteration 360\t train_loss: 1.05 took: 0.13s\n",
            "Epoch 5, Iteration 380\t train_loss: 0.98 took: 0.15s\n",
            "Epoch 5, Iteration 400\t train_loss: 1.00 took: 0.13s\n",
            "Epoch 5, Iteration 420\t train_loss: 1.08 took: 0.13s\n",
            "Epoch 5, Iteration 440\t train_loss: 1.04 took: 0.13s\n",
            "Epoch 5, Iteration 460\t train_loss: 1.04 took: 0.13s\n",
            "Epoch 5, Iteration 480\t train_loss: 1.04 took: 0.13s\n",
            "Epoch 5, Iteration 500\t train_loss: 1.01 took: 0.12s\n",
            "Epoch 5, Iteration 520\t train_loss: 1.05 took: 0.13s\n",
            "Epoch 5, Iteration 540\t train_loss: 1.08 took: 0.14s\n",
            "Epoch 5, Iteration 560\t train_loss: 1.05 took: 0.13s\n",
            "Epoch 5, Iteration 580\t train_loss: 1.08 took: 0.13s\n",
            "Epoch 5, Iteration 600\t train_loss: 1.12 took: 0.12s\n",
            "Epoch 5, Iteration 620\t train_loss: 1.11 took: 0.12s\n",
            "Validation loss = 2.12\n",
            "Epoch 6, Iteration 20\t train_loss: 0.99 took: 0.21s\n",
            "Epoch 6, Iteration 40\t train_loss: 0.94 took: 0.11s\n",
            "Epoch 6, Iteration 60\t train_loss: 0.88 took: 0.14s\n",
            "Epoch 6, Iteration 80\t train_loss: 0.88 took: 0.12s\n",
            "Epoch 6, Iteration 100\t train_loss: 0.91 took: 0.12s\n",
            "Epoch 6, Iteration 120\t train_loss: 0.91 took: 0.13s\n",
            "Epoch 6, Iteration 140\t train_loss: 1.03 took: 0.13s\n",
            "Epoch 6, Iteration 160\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 6, Iteration 180\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 6, Iteration 200\t train_loss: 0.97 took: 0.12s\n",
            "Epoch 6, Iteration 220\t train_loss: 1.12 took: 0.13s\n",
            "Epoch 6, Iteration 240\t train_loss: 0.97 took: 0.13s\n",
            "Epoch 6, Iteration 260\t train_loss: 0.98 took: 0.12s\n",
            "Epoch 6, Iteration 280\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 6, Iteration 300\t train_loss: 0.94 took: 0.13s\n",
            "Epoch 6, Iteration 320\t train_loss: 0.94 took: 0.13s\n",
            "Epoch 6, Iteration 340\t train_loss: 1.03 took: 0.14s\n",
            "Epoch 6, Iteration 360\t train_loss: 1.03 took: 0.12s\n",
            "Epoch 6, Iteration 380\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 6, Iteration 400\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 6, Iteration 420\t train_loss: 1.07 took: 0.13s\n",
            "Epoch 6, Iteration 440\t train_loss: 1.12 took: 0.14s\n",
            "Epoch 6, Iteration 460\t train_loss: 1.03 took: 0.12s\n",
            "Epoch 6, Iteration 480\t train_loss: 1.09 took: 0.12s\n",
            "Epoch 6, Iteration 500\t train_loss: 1.04 took: 0.13s\n",
            "Epoch 6, Iteration 520\t train_loss: 1.07 took: 0.12s\n",
            "Epoch 6, Iteration 540\t train_loss: 1.01 took: 0.12s\n",
            "Epoch 6, Iteration 560\t train_loss: 1.00 took: 0.12s\n",
            "Epoch 6, Iteration 580\t train_loss: 1.01 took: 0.13s\n",
            "Epoch 6, Iteration 600\t train_loss: 1.04 took: 0.13s\n",
            "Epoch 6, Iteration 620\t train_loss: 0.95 took: 0.12s\n",
            "Validation loss = 2.29\n",
            "Epoch 7, Iteration 20\t train_loss: 0.93 took: 0.22s\n",
            "Epoch 7, Iteration 40\t train_loss: 0.84 took: 0.12s\n",
            "Epoch 7, Iteration 60\t train_loss: 0.87 took: 0.13s\n",
            "Epoch 7, Iteration 80\t train_loss: 0.90 took: 0.12s\n",
            "Epoch 7, Iteration 100\t train_loss: 0.92 took: 0.14s\n",
            "Epoch 7, Iteration 120\t train_loss: 0.80 took: 0.12s\n",
            "Epoch 7, Iteration 140\t train_loss: 0.88 took: 0.13s\n",
            "Epoch 7, Iteration 160\t train_loss: 1.04 took: 0.12s\n",
            "Epoch 7, Iteration 180\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 7, Iteration 200\t train_loss: 1.03 took: 0.12s\n",
            "Epoch 7, Iteration 220\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 7, Iteration 240\t train_loss: 0.92 took: 0.13s\n",
            "Epoch 7, Iteration 260\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 7, Iteration 280\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 7, Iteration 300\t train_loss: 0.87 took: 0.12s\n",
            "Epoch 7, Iteration 320\t train_loss: 0.93 took: 0.12s\n",
            "Epoch 7, Iteration 340\t train_loss: 1.02 took: 0.13s\n",
            "Epoch 7, Iteration 360\t train_loss: 1.03 took: 0.12s\n",
            "Epoch 7, Iteration 380\t train_loss: 0.94 took: 0.13s\n",
            "Epoch 7, Iteration 400\t train_loss: 0.97 took: 0.13s\n",
            "Epoch 7, Iteration 420\t train_loss: 0.91 took: 0.12s\n",
            "Epoch 7, Iteration 440\t train_loss: 1.01 took: 0.12s\n",
            "Epoch 7, Iteration 460\t train_loss: 1.15 took: 0.13s\n",
            "Epoch 7, Iteration 480\t train_loss: 1.06 took: 0.13s\n",
            "Epoch 7, Iteration 500\t train_loss: 0.94 took: 0.14s\n",
            "Epoch 7, Iteration 520\t train_loss: 1.00 took: 0.12s\n",
            "Epoch 7, Iteration 540\t train_loss: 0.95 took: 0.14s\n",
            "Epoch 7, Iteration 560\t train_loss: 0.94 took: 0.13s\n",
            "Epoch 7, Iteration 580\t train_loss: 1.07 took: 0.13s\n",
            "Epoch 7, Iteration 600\t train_loss: 1.01 took: 0.13s\n",
            "Epoch 7, Iteration 620\t train_loss: 1.07 took: 0.13s\n",
            "Validation loss = 2.37\n",
            "Epoch 8, Iteration 20\t train_loss: 0.96 took: 0.22s\n",
            "Epoch 8, Iteration 40\t train_loss: 0.87 took: 0.12s\n",
            "Epoch 8, Iteration 60\t train_loss: 0.87 took: 0.13s\n",
            "Epoch 8, Iteration 80\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 8, Iteration 100\t train_loss: 0.79 took: 0.13s\n",
            "Epoch 8, Iteration 120\t train_loss: 0.83 took: 0.12s\n",
            "Epoch 8, Iteration 140\t train_loss: 0.88 took: 0.13s\n",
            "Epoch 8, Iteration 160\t train_loss: 0.92 took: 0.12s\n",
            "Epoch 8, Iteration 180\t train_loss: 0.92 took: 0.13s\n",
            "Epoch 8, Iteration 200\t train_loss: 0.86 took: 0.12s\n",
            "Epoch 8, Iteration 220\t train_loss: 1.06 took: 0.12s\n",
            "Epoch 8, Iteration 240\t train_loss: 1.02 took: 0.13s\n",
            "Epoch 8, Iteration 260\t train_loss: 0.87 took: 0.12s\n",
            "Epoch 8, Iteration 280\t train_loss: 0.94 took: 0.12s\n",
            "Epoch 8, Iteration 300\t train_loss: 0.93 took: 0.12s\n",
            "Epoch 8, Iteration 320\t train_loss: 0.93 took: 0.12s\n",
            "Epoch 8, Iteration 340\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 8, Iteration 360\t train_loss: 0.97 took: 0.12s\n",
            "Epoch 8, Iteration 380\t train_loss: 0.95 took: 0.13s\n",
            "Epoch 8, Iteration 400\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 8, Iteration 420\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 8, Iteration 440\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 8, Iteration 460\t train_loss: 1.01 took: 0.12s\n",
            "Epoch 8, Iteration 480\t train_loss: 0.97 took: 0.13s\n",
            "Epoch 8, Iteration 500\t train_loss: 0.96 took: 0.11s\n",
            "Epoch 8, Iteration 520\t train_loss: 1.00 took: 0.12s\n",
            "Epoch 8, Iteration 540\t train_loss: 1.02 took: 0.12s\n",
            "Epoch 8, Iteration 560\t train_loss: 0.92 took: 0.12s\n",
            "Epoch 8, Iteration 580\t train_loss: 0.98 took: 0.14s\n",
            "Epoch 8, Iteration 600\t train_loss: 1.04 took: 0.12s\n",
            "Epoch 8, Iteration 620\t train_loss: 0.97 took: 0.13s\n",
            "Validation loss = 2.37\n",
            "Epoch 9, Iteration 20\t train_loss: 0.83 took: 0.21s\n",
            "Epoch 9, Iteration 40\t train_loss: 0.86 took: 0.12s\n",
            "Epoch 9, Iteration 60\t train_loss: 0.80 took: 0.13s\n",
            "Epoch 9, Iteration 80\t train_loss: 0.90 took: 0.12s\n",
            "Epoch 9, Iteration 100\t train_loss: 0.84 took: 0.12s\n",
            "Epoch 9, Iteration 120\t train_loss: 0.85 took: 0.12s\n",
            "Epoch 9, Iteration 140\t train_loss: 0.90 took: 0.12s\n",
            "Epoch 9, Iteration 160\t train_loss: 0.89 took: 0.13s\n",
            "Epoch 9, Iteration 180\t train_loss: 0.88 took: 0.12s\n",
            "Epoch 9, Iteration 200\t train_loss: 0.86 took: 0.12s\n",
            "Epoch 9, Iteration 220\t train_loss: 0.80 took: 0.12s\n",
            "Epoch 9, Iteration 240\t train_loss: 0.91 took: 0.12s\n",
            "Epoch 9, Iteration 260\t train_loss: 0.84 took: 0.12s\n",
            "Epoch 9, Iteration 280\t train_loss: 0.93 took: 0.14s\n",
            "Epoch 9, Iteration 300\t train_loss: 0.94 took: 0.12s\n",
            "Epoch 9, Iteration 320\t train_loss: 0.93 took: 0.12s\n",
            "Epoch 9, Iteration 340\t train_loss: 0.86 took: 0.13s\n",
            "Epoch 9, Iteration 360\t train_loss: 0.96 took: 0.11s\n",
            "Epoch 9, Iteration 380\t train_loss: 0.82 took: 0.12s\n",
            "Epoch 9, Iteration 400\t train_loss: 0.97 took: 0.11s\n",
            "Epoch 9, Iteration 420\t train_loss: 0.92 took: 0.13s\n",
            "Epoch 9, Iteration 440\t train_loss: 0.98 took: 0.12s\n",
            "Epoch 9, Iteration 460\t train_loss: 0.85 took: 0.12s\n",
            "Epoch 9, Iteration 480\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 9, Iteration 500\t train_loss: 1.02 took: 0.13s\n",
            "Epoch 9, Iteration 520\t train_loss: 1.05 took: 0.12s\n",
            "Epoch 9, Iteration 540\t train_loss: 0.98 took: 0.12s\n",
            "Epoch 9, Iteration 560\t train_loss: 1.03 took: 0.12s\n",
            "Epoch 9, Iteration 580\t train_loss: 0.95 took: 0.13s\n",
            "Epoch 9, Iteration 600\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 9, Iteration 620\t train_loss: 0.98 took: 0.13s\n",
            "Validation loss = 2.51\n",
            "Epoch 10, Iteration 20\t train_loss: 0.86 took: 0.20s\n",
            "Epoch 10, Iteration 40\t train_loss: 0.84 took: 0.13s\n",
            "Epoch 10, Iteration 60\t train_loss: 0.79 took: 0.13s\n",
            "Epoch 10, Iteration 80\t train_loss: 0.93 took: 0.12s\n",
            "Epoch 10, Iteration 100\t train_loss: 0.70 took: 0.12s\n",
            "Epoch 10, Iteration 120\t train_loss: 0.85 took: 0.11s\n",
            "Epoch 10, Iteration 140\t train_loss: 0.85 took: 0.12s\n",
            "Epoch 10, Iteration 160\t train_loss: 0.82 took: 0.12s\n",
            "Epoch 10, Iteration 180\t train_loss: 0.88 took: 0.13s\n",
            "Epoch 10, Iteration 200\t train_loss: 0.76 took: 0.11s\n",
            "Epoch 10, Iteration 220\t train_loss: 0.90 took: 0.13s\n",
            "Epoch 10, Iteration 240\t train_loss: 0.88 took: 0.13s\n",
            "Epoch 10, Iteration 260\t train_loss: 0.98 took: 0.13s\n",
            "Epoch 10, Iteration 280\t train_loss: 0.95 took: 0.12s\n",
            "Epoch 10, Iteration 300\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 10, Iteration 320\t train_loss: 0.87 took: 0.12s\n",
            "Epoch 10, Iteration 340\t train_loss: 0.96 took: 0.13s\n",
            "Epoch 10, Iteration 360\t train_loss: 0.76 took: 0.13s\n",
            "Epoch 10, Iteration 380\t train_loss: 0.89 took: 0.12s\n",
            "Epoch 10, Iteration 400\t train_loss: 0.99 took: 0.13s\n",
            "Epoch 10, Iteration 420\t train_loss: 0.85 took: 0.12s\n",
            "Epoch 10, Iteration 440\t train_loss: 0.85 took: 0.12s\n",
            "Epoch 10, Iteration 460\t train_loss: 1.00 took: 0.12s\n",
            "Epoch 10, Iteration 480\t train_loss: 1.08 took: 0.12s\n",
            "Epoch 10, Iteration 500\t train_loss: 1.13 took: 0.12s\n",
            "Epoch 10, Iteration 520\t train_loss: 0.89 took: 0.12s\n",
            "Epoch 10, Iteration 540\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 10, Iteration 560\t train_loss: 0.99 took: 0.12s\n",
            "Epoch 10, Iteration 580\t train_loss: 0.95 took: 0.13s\n",
            "Epoch 10, Iteration 600\t train_loss: 1.09 took: 0.13s\n",
            "Epoch 10, Iteration 620\t train_loss: 0.96 took: 0.13s\n",
            "Validation loss = 2.48\n",
            "Training finished, took 47.35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz_kZWhpEhuT",
        "colab_type": "text"
      },
      "source": [
        "For this problem the speedup of using a GPU is not as great as it would be for other problems (here the speedup is 2-3x).  Generally the bigger the network is (e.g., bigger images) the greater the speedup.  If you want to play around with this on your own, go the model architecture section and change the device variable to \"cpu\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh1Z2O3QzVNd",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "Lets check the outputs of our network. First Let's plot the loss of the network over time to see if any learning actually occured."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbLDTiF960zY",
        "colab_type": "code",
        "outputId": "b4767002-db58-49f0-8ef0-5f0cb4fe5cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyUnBzMya/g/BRsVb8nAqlwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-WwSRpPA-O",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Interpret what is happening in these curves.  Why is the training curve sampled so much more often (hint: look at the train_model function)?  Why is the training loss noisier (hint: again, look at the train_model function)?  What do the shapes of the curves say about how the model training is going (i.e., is everything going great, are we overfitting?)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-35nSvVIbiT",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umz1sOrpIduk",
        "colab_type": "text"
      },
      "source": [
        "The training loss is evaluated after each mini batch so it is sampled more densely.\n",
        "\n",
        "The training loss is evaluated only on the elements of the mini-batch (32 images), therefore it is much more subject to the random variation in the datta than the validation loss that is computed over the entire validation set.\n",
        "\n",
        "These curves show that overfitting is occurring since the trianing loss is going down while the validation loss stays constant (or maybe goes up a bit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41PUNs48Ifgt",
        "colab_type": "text"
      },
      "source": [
        "### Showing Model Predictions\n",
        "\n",
        "For more subjective santiy-checking, we can plot some images and see what our network predicts vs the ground truth label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDL5SibNzUuV",
        "colab_type": "code",
        "outputId": "118a19f2-cac2-4b45-d0d6-bb49733bdc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    disp_image(image, label, prediction)\n",
        "\n",
        "examine_label(20)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOcWaPLpAWYB",
        "colab_type": "text"
      },
      "source": [
        "## Computing Accuracy\n",
        "\n",
        "The plots of the losses over time give us a sense of how the network is learning.  The losses themselves, however, may not provide a super accurate idea of how the network is actually doing at its task (predicting the correct labels).  We'll do it for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9gR_izeApfA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d750c4d-c2f0-4161-b7ec-125ef47cbde8"
      },
      "source": [
        "n_correct = 0\n",
        "n_total = 0\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    # Get inputs in right form\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "    n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "    n_total += labels.shape[0]\n",
        "print(\"Training accuracy is\", n_correct/n_total)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy is 0.70055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UCwZ3mB5eo",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Adapt this code so that it computes the accuracy on the test set.  Hint: consider factoring this out into a function to make it more flexible.  If you create a function, you'll want to pass in `net` and `loader` as inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR7HRd3vCFEo",
        "colab_type": "text"
      },
      "source": [
        "#### ***Solution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyw-OfHCBjV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de08095f-e040-4193-e89b-0d49dfb1d452"
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.70055\n",
            "Test accuracy is 0.407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz_rgc__DKpE",
        "colab_type": "text"
      },
      "source": [
        "## Model Visualization\n",
        "\n",
        "Next, we should try to better understand what is going on with the model.  The first thing we can do is to visualize the convolutional kernels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk7GlnqWDQ3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEJOv9NpDHEc",
        "colab_type": "text"
      },
      "source": [
        "## Model Iteration\n",
        "\n",
        "### Exercise\n",
        "\n",
        "At this point, try to change your model and see what happens (not necessarily to improve its performance). Here are some possibilities.\n",
        "\n",
        "* Change the size of some layer to either increase or decrease model complexity.\n",
        "* Change the activation function in the network to sigmoid or some other function.\n",
        "* Change the batch size.\n",
        "* Increase or decrease the number of epochs.\n",
        "\n",
        "***Tip: when making these changes, consider modifying your neural network class to allow for whatever you are changing to be customized, rather than just hardcoding new values*** For example, if I wanted to try changing the number of convolutional kernels, I might modify my `__init__` function to take this number as a new input (see solution below for details of this example).\n",
        "\n",
        "Run your new model, compare the performance with the default network, and visualize the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7W3Yj0pERHg",
        "colab_type": "text"
      },
      "source": [
        "#### ***Solution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qiSi6g8EGJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "4eaa658b-3b89-44ed-831e-134411bbecc6"
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "    # The init funciton in Pytorch classes is used to keep track of the parameters of the model\n",
        "    # specifically the ones we want to update with gradient descent + backprop\n",
        "    # So we need to make sure we keep track of all of them here\n",
        "    def __init__(self, num_kernels):\n",
        "        super(MyCNN, self).__init__()\n",
        "        # layers defined here\n",
        "\n",
        "        # Make sure you understand what this convolutional layer is doing.\n",
        "        # E.g., considering looking at help(nn.Conv2D).  Draw a picture of what\n",
        "        # this layer does to the data.\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Make sure you understand what this MaxPool2D layer is doing.\n",
        "        # E.g., considering looking at help(nn.MaxPool2D).  Draw a picture of\n",
        "        # what this layer does to the data.\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # maxpool_output_size is the total amount of data coming out of that\n",
        "        # layer.  Explain why the line of code below computes this quantity.\n",
        "        self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
        "\n",
        "        # Add on a fully connected layer (like in our MLP)\n",
        "        # fc stands for fully connected\n",
        "        fc1_size = 64\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
        "\n",
        "        # we'll use this activation function internally in the network\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "        # Convert our fully connected layer into outputs that we can compare to the result\n",
        "        fc2_size = len(classes)\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
        "\n",
        "        # Note: that the output will not represent the probability of the\n",
        "        # output being in each class.  The loss function we will use\n",
        "        # `CrossEntropyLoss` will take care of convering these values to\n",
        "        # probabilities and then computing the log loss with respect to the\n",
        "        # true label.  We could break this out into multiple steps, but it turns\n",
        "        # out that the algorithm will be more numerically stable if we do it in\n",
        "        # one go.  We have included a cell to show you the documentation for\n",
        "        # `CrossEntropyLoss` if you'd like to check it out.\n",
        "        \n",
        "    # The forward function in the class defines the operations performed on a given input to the model\n",
        "    # and returns the output of the model\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.activation_func(x)\n",
        "        # this code flattens the output of the convolution, max pool,\n",
        "        # activation sequence of steps into a vector\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
        "    # returns the loss and optimizer used by the model\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer\n",
        "\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN(128)\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# visualize the model\n",
        "visualize_network(net)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7eff60146cc0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"267pt\" height=\"493pt\"\n viewBox=\"0.00 0.00 266.50 493.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 489)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-489 262.5,-489 262.5,4 -4,4\"/>\n<!-- 139635293711608 -->\n<g id=\"node1\" class=\"node\">\n<title>139635293711608</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"171,-21 67,-21 67,0 171,0 171,-21\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139635293711664 -->\n<g id=\"node2\" class=\"node\">\n<title>139635293711664</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-91 0,-91 0,-57 54,-57 54,-91\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 139635293711664&#45;&gt;139635293711608 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139635293711664&#45;&gt;139635293711608</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.6543,-56.9832C65.1894,-47.641 81.8926,-36.1122 95.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.2865,-29.7398 103.5283,-21.1788 93.3102,-23.9788 97.2865,-29.7398\"/>\n</g>\n<!-- 139635293711720 -->\n<g id=\"node3\" class=\"node\">\n<title>139635293711720</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"166,-84.5 72,-84.5 72,-63.5 166,-63.5 166,-84.5\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139635293711720&#45;&gt;139635293711608 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139635293711720&#45;&gt;139635293711608</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119,-63.2281C119,-54.5091 119,-41.9699 119,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.5001,-31.1128 119,-21.1128 115.5001,-31.1129 122.5001,-31.1128\"/>\n</g>\n<!-- 139635293709816 -->\n<g id=\"node4\" class=\"node\">\n<title>139635293709816</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170,-154.5 66,-154.5 66,-133.5 170,-133.5 170,-154.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139635293709816&#45;&gt;139635293711720 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139635293709816&#45;&gt;139635293711720</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118.1519,-133.3685C118.2972,-123.1925 118.5206,-107.5606 118.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.2034,-94.7806 118.8467,-84.7315 115.2041,-94.6805 122.2034,-94.7806\"/>\n</g>\n<!-- 139635293710936 -->\n<g id=\"node5\" class=\"node\">\n<title>139635293710936</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-231 0,-231 0,-197 54,-197 54,-231\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139635293710936&#45;&gt;139635293709816 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139635293710936&#45;&gt;139635293709816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49.4944,-196.6966C63.7034,-185.7666 81.9745,-171.7119 96.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.447,-163.4565 104.2393,-154.5852 94.179,-157.9081 98.447,-163.4565\"/>\n</g>\n<!-- 139635293712112 -->\n<g id=\"node6\" class=\"node\">\n<title>139635293712112</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-224.5 72.5,-224.5 72.5,-203.5 163.5,-203.5 163.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139635293712112&#45;&gt;139635293709816 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139635293712112&#45;&gt;139635293709816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-203.3685C118,-193.1925 118,-177.5606 118,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-164.7315 118,-154.7315 114.5001,-164.7316 121.5001,-164.7315\"/>\n</g>\n<!-- 139635293709648 -->\n<g id=\"node7\" class=\"node\">\n<title>139635293709648</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"164,-294.5 70,-294.5 70,-273.5 164,-273.5 164,-294.5\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139635293709648&#45;&gt;139635293712112 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139635293709648&#45;&gt;139635293712112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117.1519,-273.3685C117.2972,-263.1925 117.5206,-247.5606 117.7016,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.2034,-234.7806 117.8467,-224.7315 114.2041,-234.6805 121.2034,-234.7806\"/>\n</g>\n<!-- 139635293709760 -->\n<g id=\"node8\" class=\"node\">\n<title>139635293709760</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"207,-358 27,-358 27,-337 207,-337 207,-358\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139635293709760&#45;&gt;139635293709648 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139635293709760&#45;&gt;139635293709648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117,-336.7281C117,-328.0091 117,-315.4699 117,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.5001,-304.6128 117,-294.6128 113.5001,-304.6129 120.5001,-304.6128\"/>\n</g>\n<!-- 139635293709144 -->\n<g id=\"node9\" class=\"node\">\n<title>139635293709144</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"195.5,-415 38.5,-415 38.5,-394 195.5,-394 195.5,-415\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139635293709144&#45;&gt;139635293709760 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139635293709144&#45;&gt;139635293709760</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117,-393.7787C117,-386.6134 117,-376.9517 117,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.5001,-368.1732 117,-358.1732 113.5001,-368.1732 120.5001,-368.1732\"/>\n</g>\n<!-- 139635293709032 -->\n<g id=\"node10\" class=\"node\">\n<title>139635293709032</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"111.5,-485 30.5,-485 30.5,-451 111.5,-451 111.5,-485\"/>\n<text text-anchor=\"middle\" x=\"71\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"71\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 3, 3, 3)</text>\n</g>\n<!-- 139635293709032&#45;&gt;139635293709144 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139635293709032&#45;&gt;139635293709144</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M83.3272,-450.9832C89.4107,-442.5853 96.7742,-432.4204 103.0621,-423.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0945,-425.5204 109.1266,-415.3687 100.4256,-421.4138 106.0945,-425.5204\"/>\n</g>\n<!-- 139635293708584 -->\n<g id=\"node11\" class=\"node\">\n<title>139635293708584</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"198,-485 130,-485 130,-451 198,-451 198,-485\"/>\n<text text-anchor=\"middle\" x=\"164\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"164\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 139635293708584&#45;&gt;139635293709144 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139635293708584&#45;&gt;139635293709144</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M151.4049,-450.9832C145.1237,-442.4969 137.5069,-432.2062 131.0384,-423.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.8071,-421.3243 125.0445,-415.3687 128.1806,-425.4888 133.8071,-421.3243\"/>\n</g>\n<!-- 139635293712336 -->\n<g id=\"node12\" class=\"node\">\n<title>139635293712336</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"255.5,-224.5 182.5,-224.5 182.5,-203.5 255.5,-203.5 255.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"219\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139635293712336&#45;&gt;139635293709816 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139635293712336&#45;&gt;139635293709816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M203.6603,-203.3685C187.1024,-191.8927 160.533,-173.4783 141.3726,-160.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.3659,-157.3219 133.1532,-154.5022 139.3784,-163.0752 143.3659,-157.3219\"/>\n</g>\n<!-- 139635293708696 -->\n<g id=\"node13\" class=\"node\">\n<title>139635293708696</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"257.5,-301 182.5,-301 182.5,-267 257.5,-267 257.5,-301\"/>\n<text text-anchor=\"middle\" x=\"220\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"220\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 32768)</text>\n</g>\n<!-- 139635293708696&#45;&gt;139635293712336 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139635293708696&#45;&gt;139635293712336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M219.7528,-266.6966C219.6152,-257.0634 219.4429,-245.003 219.2979,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.7967,-234.7402 219.1542,-224.7913 215.7975,-234.8403 222.7967,-234.7402\"/>\n</g>\n<!-- 139635293711440 -->\n<g id=\"node14\" class=\"node\">\n<title>139635293711440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.5,-84.5 185.5,-84.5 185.5,-63.5 258.5,-63.5 258.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139635293711440&#45;&gt;139635293711608 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139635293711440&#45;&gt;139635293711608</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.5275,-63.2281C188.1519,-53.1325 163.4682,-37.9149 144.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.5636,-23.3814 136.2145,-21.1128 142.8901,-29.3401 146.5636,-23.3814\"/>\n</g>\n<!-- 139635293710544 -->\n<g id=\"node15\" class=\"node\">\n<title>139635293710544</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"255.5,-161 188.5,-161 188.5,-127 255.5,-127 255.5,-161\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"222\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 64)</text>\n</g>\n<!-- 139635293710544&#45;&gt;139635293711440 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139635293710544&#45;&gt;139635293711440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M222,-126.6966C222,-117.0634 222,-105.003 222,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.5001,-94.7912 222,-84.7913 218.5001,-94.7913 225.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdvJxTd-Mlvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "635b6181-3224-49fe-ff32-8a0b81e8ae4b"
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 20\t train_loss: 6.40 took: 0.24s\n",
            "Epoch 1, Iteration 40\t train_loss: 2.26 took: 0.12s\n",
            "Epoch 1, Iteration 60\t train_loss: 2.14 took: 0.14s\n",
            "Epoch 1, Iteration 80\t train_loss: 2.04 took: 0.13s\n",
            "Epoch 1, Iteration 100\t train_loss: 1.93 took: 0.12s\n",
            "Epoch 1, Iteration 120\t train_loss: 2.04 took: 0.13s\n",
            "Epoch 1, Iteration 140\t train_loss: 1.90 took: 0.13s\n",
            "Epoch 1, Iteration 160\t train_loss: 1.85 took: 0.12s\n",
            "Epoch 1, Iteration 180\t train_loss: 1.87 took: 0.14s\n",
            "Epoch 1, Iteration 200\t train_loss: 1.86 took: 0.14s\n",
            "Epoch 1, Iteration 220\t train_loss: 1.76 took: 0.13s\n",
            "Epoch 1, Iteration 240\t train_loss: 1.82 took: 0.13s\n",
            "Epoch 1, Iteration 260\t train_loss: 1.77 took: 0.13s\n",
            "Epoch 1, Iteration 280\t train_loss: 1.75 took: 0.12s\n",
            "Epoch 1, Iteration 300\t train_loss: 1.70 took: 0.13s\n",
            "Epoch 1, Iteration 320\t train_loss: 1.72 took: 0.14s\n",
            "Epoch 1, Iteration 340\t train_loss: 1.70 took: 0.12s\n",
            "Epoch 1, Iteration 360\t train_loss: 1.73 took: 0.12s\n",
            "Epoch 1, Iteration 380\t train_loss: 1.72 took: 0.12s\n",
            "Epoch 1, Iteration 400\t train_loss: 1.77 took: 0.13s\n",
            "Epoch 1, Iteration 420\t train_loss: 1.70 took: 0.13s\n",
            "Epoch 1, Iteration 440\t train_loss: 1.68 took: 0.12s\n",
            "Epoch 1, Iteration 460\t train_loss: 1.74 took: 0.13s\n",
            "Epoch 1, Iteration 480\t train_loss: 1.69 took: 0.13s\n",
            "Epoch 1, Iteration 500\t train_loss: 1.70 took: 0.14s\n",
            "Epoch 1, Iteration 520\t train_loss: 1.64 took: 0.13s\n",
            "Epoch 1, Iteration 540\t train_loss: 1.64 took: 0.13s\n",
            "Epoch 1, Iteration 560\t train_loss: 1.72 took: 0.12s\n",
            "Epoch 1, Iteration 580\t train_loss: 1.65 took: 0.14s\n",
            "Epoch 1, Iteration 600\t train_loss: 1.64 took: 0.13s\n",
            "Epoch 1, Iteration 620\t train_loss: 1.65 took: 0.13s\n",
            "Validation loss = 1.66\n",
            "Epoch 2, Iteration 20\t train_loss: 1.57 took: 0.22s\n",
            "Epoch 2, Iteration 40\t train_loss: 1.68 took: 0.13s\n",
            "Epoch 2, Iteration 60\t train_loss: 1.66 took: 0.12s\n",
            "Epoch 2, Iteration 80\t train_loss: 1.60 took: 0.11s\n",
            "Epoch 2, Iteration 100\t train_loss: 1.66 took: 0.13s\n",
            "Epoch 2, Iteration 120\t train_loss: 1.63 took: 0.12s\n",
            "Epoch 2, Iteration 140\t train_loss: 1.69 took: 0.12s\n",
            "Epoch 2, Iteration 160\t train_loss: 1.68 took: 0.12s\n",
            "Epoch 2, Iteration 180\t train_loss: 1.62 took: 0.12s\n",
            "Epoch 2, Iteration 200\t train_loss: 1.62 took: 0.12s\n",
            "Epoch 2, Iteration 220\t train_loss: 1.69 took: 0.13s\n",
            "Epoch 2, Iteration 240\t train_loss: 1.64 took: 0.13s\n",
            "Epoch 2, Iteration 260\t train_loss: 1.61 took: 0.12s\n",
            "Epoch 2, Iteration 280\t train_loss: 1.63 took: 0.12s\n",
            "Epoch 2, Iteration 300\t train_loss: 1.64 took: 0.12s\n",
            "Epoch 2, Iteration 320\t train_loss: 1.58 took: 0.12s\n",
            "Epoch 2, Iteration 340\t train_loss: 1.59 took: 0.12s\n",
            "Epoch 2, Iteration 360\t train_loss: 1.58 took: 0.12s\n",
            "Epoch 2, Iteration 380\t train_loss: 1.61 took: 0.14s\n",
            "Epoch 2, Iteration 400\t train_loss: 1.65 took: 0.11s\n",
            "Epoch 2, Iteration 420\t train_loss: 1.57 took: 0.12s\n",
            "Epoch 2, Iteration 440\t train_loss: 1.59 took: 0.12s\n",
            "Epoch 2, Iteration 460\t train_loss: 1.60 took: 0.12s\n",
            "Epoch 2, Iteration 480\t train_loss: 1.69 took: 0.13s\n",
            "Epoch 2, Iteration 500\t train_loss: 1.69 took: 0.12s\n",
            "Epoch 2, Iteration 520\t train_loss: 1.67 took: 0.12s\n",
            "Epoch 2, Iteration 540\t train_loss: 1.67 took: 0.13s\n",
            "Epoch 2, Iteration 560\t train_loss: 1.61 took: 0.13s\n",
            "Epoch 2, Iteration 580\t train_loss: 1.54 took: 0.14s\n",
            "Epoch 2, Iteration 600\t train_loss: 1.52 took: 0.13s\n",
            "Epoch 2, Iteration 620\t train_loss: 1.53 took: 0.14s\n",
            "Validation loss = 1.69\n",
            "Epoch 3, Iteration 20\t train_loss: 1.58 took: 0.22s\n",
            "Epoch 3, Iteration 40\t train_loss: 1.54 took: 0.11s\n",
            "Epoch 3, Iteration 60\t train_loss: 1.57 took: 0.13s\n",
            "Epoch 3, Iteration 80\t train_loss: 1.61 took: 0.11s\n",
            "Epoch 3, Iteration 100\t train_loss: 1.48 took: 0.12s\n",
            "Epoch 3, Iteration 120\t train_loss: 1.58 took: 0.13s\n",
            "Epoch 3, Iteration 140\t train_loss: 1.53 took: 0.12s\n",
            "Epoch 3, Iteration 160\t train_loss: 1.62 took: 0.12s\n",
            "Epoch 3, Iteration 180\t train_loss: 1.52 took: 0.13s\n",
            "Epoch 3, Iteration 200\t train_loss: 1.60 took: 0.12s\n",
            "Epoch 3, Iteration 220\t train_loss: 1.58 took: 0.12s\n",
            "Epoch 3, Iteration 240\t train_loss: 1.53 took: 0.12s\n",
            "Epoch 3, Iteration 260\t train_loss: 1.54 took: 0.13s\n",
            "Epoch 3, Iteration 280\t train_loss: 1.54 took: 0.12s\n",
            "Epoch 3, Iteration 300\t train_loss: 1.57 took: 0.13s\n",
            "Epoch 3, Iteration 320\t train_loss: 1.59 took: 0.12s\n",
            "Epoch 3, Iteration 340\t train_loss: 1.50 took: 0.12s\n",
            "Epoch 3, Iteration 360\t train_loss: 1.51 took: 0.12s\n",
            "Epoch 3, Iteration 380\t train_loss: 1.58 took: 0.12s\n",
            "Epoch 3, Iteration 400\t train_loss: 1.49 took: 0.12s\n",
            "Epoch 3, Iteration 420\t train_loss: 1.57 took: 0.11s\n",
            "Epoch 3, Iteration 440\t train_loss: 1.49 took: 0.12s\n",
            "Epoch 3, Iteration 460\t train_loss: 1.53 took: 0.14s\n",
            "Epoch 3, Iteration 480\t train_loss: 1.55 took: 0.11s\n",
            "Epoch 3, Iteration 500\t train_loss: 1.52 took: 0.13s\n",
            "Epoch 3, Iteration 520\t train_loss: 1.56 took: 0.13s\n",
            "Epoch 3, Iteration 540\t train_loss: 1.52 took: 0.12s\n",
            "Epoch 3, Iteration 560\t train_loss: 1.69 took: 0.12s\n",
            "Epoch 3, Iteration 580\t train_loss: 1.63 took: 0.12s\n",
            "Epoch 3, Iteration 600\t train_loss: 1.69 took: 0.12s\n",
            "Epoch 3, Iteration 620\t train_loss: 1.49 took: 0.13s\n",
            "Validation loss = 1.56\n",
            "Epoch 4, Iteration 20\t train_loss: 1.45 took: 0.21s\n",
            "Epoch 4, Iteration 40\t train_loss: 1.55 took: 0.13s\n",
            "Epoch 4, Iteration 60\t train_loss: 1.55 took: 0.12s\n",
            "Epoch 4, Iteration 80\t train_loss: 1.53 took: 0.11s\n",
            "Epoch 4, Iteration 100\t train_loss: 1.49 took: 0.11s\n",
            "Epoch 4, Iteration 120\t train_loss: 1.56 took: 0.13s\n",
            "Epoch 4, Iteration 140\t train_loss: 1.50 took: 0.12s\n",
            "Epoch 4, Iteration 160\t train_loss: 1.50 took: 0.12s\n",
            "Epoch 4, Iteration 180\t train_loss: 1.49 took: 0.13s\n",
            "Epoch 4, Iteration 200\t train_loss: 1.42 took: 0.13s\n",
            "Epoch 4, Iteration 220\t train_loss: 1.51 took: 0.12s\n",
            "Epoch 4, Iteration 240\t train_loss: 1.53 took: 0.12s\n",
            "Epoch 4, Iteration 260\t train_loss: 1.49 took: 0.12s\n",
            "Epoch 4, Iteration 280\t train_loss: 1.46 took: 0.12s\n",
            "Epoch 4, Iteration 300\t train_loss: 1.49 took: 0.12s\n",
            "Epoch 4, Iteration 320\t train_loss: 1.54 took: 0.11s\n",
            "Epoch 4, Iteration 340\t train_loss: 1.56 took: 0.13s\n",
            "Epoch 4, Iteration 360\t train_loss: 1.59 took: 0.11s\n",
            "Epoch 4, Iteration 380\t train_loss: 1.48 took: 0.14s\n",
            "Epoch 4, Iteration 400\t train_loss: 1.52 took: 0.13s\n",
            "Epoch 4, Iteration 420\t train_loss: 1.59 took: 0.12s\n",
            "Epoch 4, Iteration 440\t train_loss: 1.55 took: 0.12s\n",
            "Epoch 4, Iteration 460\t train_loss: 1.42 took: 0.12s\n",
            "Epoch 4, Iteration 480\t train_loss: 1.47 took: 0.12s\n",
            "Epoch 4, Iteration 500\t train_loss: 1.51 took: 0.12s\n",
            "Epoch 4, Iteration 520\t train_loss: 1.59 took: 0.11s\n",
            "Epoch 4, Iteration 540\t train_loss: 1.57 took: 0.15s\n",
            "Epoch 4, Iteration 560\t train_loss: 1.58 took: 0.12s\n",
            "Epoch 4, Iteration 580\t train_loss: 1.53 took: 0.12s\n",
            "Epoch 4, Iteration 600\t train_loss: 1.60 took: 0.12s\n",
            "Epoch 4, Iteration 620\t train_loss: 1.49 took: 0.13s\n",
            "Validation loss = 1.64\n",
            "Epoch 5, Iteration 20\t train_loss: 1.44 took: 0.24s\n",
            "Epoch 5, Iteration 40\t train_loss: 1.41 took: 0.12s\n",
            "Epoch 5, Iteration 60\t train_loss: 1.46 took: 0.12s\n",
            "Epoch 5, Iteration 80\t train_loss: 1.41 took: 0.12s\n",
            "Epoch 5, Iteration 100\t train_loss: 1.42 took: 0.12s\n",
            "Epoch 5, Iteration 120\t train_loss: 1.51 took: 0.13s\n",
            "Epoch 5, Iteration 140\t train_loss: 1.49 took: 0.13s\n",
            "Epoch 5, Iteration 160\t train_loss: 1.51 took: 0.12s\n",
            "Epoch 5, Iteration 180\t train_loss: 1.45 took: 0.12s\n",
            "Epoch 5, Iteration 200\t train_loss: 1.44 took: 0.12s\n",
            "Epoch 5, Iteration 220\t train_loss: 1.40 took: 0.13s\n",
            "Epoch 5, Iteration 240\t train_loss: 1.52 took: 0.13s\n",
            "Epoch 5, Iteration 260\t train_loss: 1.51 took: 0.12s\n",
            "Epoch 5, Iteration 280\t train_loss: 1.45 took: 0.13s\n",
            "Epoch 5, Iteration 300\t train_loss: 1.49 took: 0.12s\n",
            "Epoch 5, Iteration 320\t train_loss: 1.56 took: 0.12s\n",
            "Epoch 5, Iteration 340\t train_loss: 1.45 took: 0.13s\n",
            "Epoch 5, Iteration 360\t train_loss: 1.51 took: 0.13s\n",
            "Epoch 5, Iteration 380\t train_loss: 1.50 took: 0.12s\n",
            "Epoch 5, Iteration 400\t train_loss: 1.49 took: 0.12s\n",
            "Epoch 5, Iteration 420\t train_loss: 1.46 took: 0.13s\n",
            "Epoch 5, Iteration 440\t train_loss: 1.43 took: 0.14s\n",
            "Epoch 5, Iteration 460\t train_loss: 1.46 took: 0.13s\n",
            "Epoch 5, Iteration 480\t train_loss: 1.45 took: 0.12s\n",
            "Epoch 5, Iteration 500\t train_loss: 1.43 took: 0.12s\n",
            "Epoch 5, Iteration 520\t train_loss: 1.52 took: 0.11s\n",
            "Epoch 5, Iteration 540\t train_loss: 1.48 took: 0.13s\n",
            "Epoch 5, Iteration 560\t train_loss: 1.50 took: 0.12s\n",
            "Epoch 5, Iteration 580\t train_loss: 1.45 took: 0.12s\n",
            "Epoch 5, Iteration 600\t train_loss: 1.42 took: 0.14s\n",
            "Epoch 5, Iteration 620\t train_loss: 1.50 took: 0.13s\n",
            "Validation loss = 1.51\n",
            "Epoch 6, Iteration 20\t train_loss: 1.35 took: 0.23s\n",
            "Epoch 6, Iteration 40\t train_loss: 1.28 took: 0.12s\n",
            "Epoch 6, Iteration 60\t train_loss: 1.37 took: 0.12s\n",
            "Epoch 6, Iteration 80\t train_loss: 1.40 took: 0.12s\n",
            "Epoch 6, Iteration 100\t train_loss: 1.40 took: 0.12s\n",
            "Epoch 6, Iteration 120\t train_loss: 1.37 took: 0.12s\n",
            "Epoch 6, Iteration 140\t train_loss: 1.39 took: 0.12s\n",
            "Epoch 6, Iteration 160\t train_loss: 1.31 took: 0.12s\n",
            "Epoch 6, Iteration 180\t train_loss: 1.33 took: 0.14s\n",
            "Epoch 6, Iteration 200\t train_loss: 1.46 took: 0.12s\n",
            "Epoch 6, Iteration 220\t train_loss: 1.35 took: 0.13s\n",
            "Epoch 6, Iteration 240\t train_loss: 1.46 took: 0.12s\n",
            "Epoch 6, Iteration 260\t train_loss: 1.46 took: 0.12s\n",
            "Epoch 6, Iteration 280\t train_loss: 1.29 took: 0.12s\n",
            "Epoch 6, Iteration 300\t train_loss: 1.46 took: 0.12s\n",
            "Epoch 6, Iteration 320\t train_loss: 1.40 took: 0.13s\n",
            "Epoch 6, Iteration 340\t train_loss: 1.35 took: 0.13s\n",
            "Epoch 6, Iteration 360\t train_loss: 1.42 took: 0.13s\n",
            "Epoch 6, Iteration 380\t train_loss: 1.44 took: 0.12s\n",
            "Epoch 6, Iteration 400\t train_loss: 1.42 took: 0.12s\n",
            "Epoch 6, Iteration 420\t train_loss: 1.43 took: 0.12s\n",
            "Epoch 6, Iteration 440\t train_loss: 1.43 took: 0.12s\n",
            "Epoch 6, Iteration 460\t train_loss: 1.50 took: 0.13s\n",
            "Epoch 6, Iteration 480\t train_loss: 1.37 took: 0.12s\n",
            "Epoch 6, Iteration 500\t train_loss: 1.38 took: 0.13s\n",
            "Epoch 6, Iteration 520\t train_loss: 1.36 took: 0.13s\n",
            "Epoch 6, Iteration 540\t train_loss: 1.37 took: 0.13s\n",
            "Epoch 6, Iteration 560\t train_loss: 1.42 took: 0.13s\n",
            "Epoch 6, Iteration 580\t train_loss: 1.45 took: 0.13s\n",
            "Epoch 6, Iteration 600\t train_loss: 1.28 took: 0.13s\n",
            "Epoch 6, Iteration 620\t train_loss: 1.32 took: 0.13s\n",
            "Validation loss = 1.53\n",
            "Epoch 7, Iteration 20\t train_loss: 1.36 took: 0.22s\n",
            "Epoch 7, Iteration 40\t train_loss: 1.31 took: 0.12s\n",
            "Epoch 7, Iteration 60\t train_loss: 1.24 took: 0.13s\n",
            "Epoch 7, Iteration 80\t train_loss: 1.22 took: 0.12s\n",
            "Epoch 7, Iteration 100\t train_loss: 1.25 took: 0.13s\n",
            "Epoch 7, Iteration 120\t train_loss: 1.34 took: 0.12s\n",
            "Epoch 7, Iteration 140\t train_loss: 1.24 took: 0.13s\n",
            "Epoch 7, Iteration 160\t train_loss: 1.26 took: 0.12s\n",
            "Epoch 7, Iteration 180\t train_loss: 1.28 took: 0.13s\n",
            "Epoch 7, Iteration 200\t train_loss: 1.30 took: 0.13s\n",
            "Epoch 7, Iteration 220\t train_loss: 1.32 took: 0.13s\n",
            "Epoch 7, Iteration 240\t train_loss: 1.35 took: 0.14s\n",
            "Epoch 7, Iteration 260\t train_loss: 1.43 took: 0.13s\n",
            "Epoch 7, Iteration 280\t train_loss: 1.38 took: 0.11s\n",
            "Epoch 7, Iteration 300\t train_loss: 1.34 took: 0.13s\n",
            "Epoch 7, Iteration 320\t train_loss: 1.34 took: 0.12s\n",
            "Epoch 7, Iteration 340\t train_loss: 1.28 took: 0.12s\n",
            "Epoch 7, Iteration 360\t train_loss: 1.37 took: 0.12s\n",
            "Epoch 7, Iteration 380\t train_loss: 1.38 took: 0.13s\n",
            "Epoch 7, Iteration 400\t train_loss: 1.33 took: 0.12s\n",
            "Epoch 7, Iteration 420\t train_loss: 1.33 took: 0.13s\n",
            "Epoch 7, Iteration 440\t train_loss: 1.29 took: 0.13s\n",
            "Epoch 7, Iteration 460\t train_loss: 1.27 took: 0.12s\n",
            "Epoch 7, Iteration 480\t train_loss: 1.34 took: 0.11s\n",
            "Epoch 7, Iteration 500\t train_loss: 1.34 took: 0.12s\n",
            "Epoch 7, Iteration 520\t train_loss: 1.29 took: 0.12s\n",
            "Epoch 7, Iteration 540\t train_loss: 1.37 took: 0.12s\n",
            "Epoch 7, Iteration 560\t train_loss: 1.37 took: 0.14s\n",
            "Epoch 7, Iteration 580\t train_loss: 1.29 took: 0.14s\n",
            "Epoch 7, Iteration 600\t train_loss: 1.38 took: 0.12s\n",
            "Epoch 7, Iteration 620\t train_loss: 1.39 took: 0.12s\n",
            "Validation loss = 1.54\n",
            "Epoch 8, Iteration 20\t train_loss: 1.33 took: 0.22s\n",
            "Epoch 8, Iteration 40\t train_loss: 1.15 took: 0.11s\n",
            "Epoch 8, Iteration 60\t train_loss: 1.10 took: 0.12s\n",
            "Epoch 8, Iteration 80\t train_loss: 1.17 took: 0.11s\n",
            "Epoch 8, Iteration 100\t train_loss: 1.23 took: 0.13s\n",
            "Epoch 8, Iteration 120\t train_loss: 1.33 took: 0.11s\n",
            "Epoch 8, Iteration 140\t train_loss: 1.38 took: 0.13s\n",
            "Epoch 8, Iteration 160\t train_loss: 1.20 took: 0.13s\n",
            "Epoch 8, Iteration 180\t train_loss: 1.21 took: 0.13s\n",
            "Epoch 8, Iteration 200\t train_loss: 1.25 took: 0.12s\n",
            "Epoch 8, Iteration 220\t train_loss: 1.20 took: 0.13s\n",
            "Epoch 8, Iteration 240\t train_loss: 1.19 took: 0.11s\n",
            "Epoch 8, Iteration 260\t train_loss: 1.33 took: 0.12s\n",
            "Epoch 8, Iteration 280\t train_loss: 1.22 took: 0.12s\n",
            "Epoch 8, Iteration 300\t train_loss: 1.26 took: 0.12s\n",
            "Epoch 8, Iteration 320\t train_loss: 1.39 took: 0.13s\n",
            "Epoch 8, Iteration 340\t train_loss: 1.28 took: 0.12s\n",
            "Epoch 8, Iteration 360\t train_loss: 1.27 took: 0.12s\n",
            "Epoch 8, Iteration 380\t train_loss: 1.27 took: 0.13s\n",
            "Epoch 8, Iteration 400\t train_loss: 1.19 took: 0.12s\n",
            "Epoch 8, Iteration 420\t train_loss: 1.41 took: 0.13s\n",
            "Epoch 8, Iteration 440\t train_loss: 1.33 took: 0.13s\n",
            "Epoch 8, Iteration 460\t train_loss: 1.28 took: 0.12s\n",
            "Epoch 8, Iteration 480\t train_loss: 1.31 took: 0.13s\n",
            "Epoch 8, Iteration 500\t train_loss: 1.31 took: 0.13s\n",
            "Epoch 8, Iteration 520\t train_loss: 1.33 took: 0.12s\n",
            "Epoch 8, Iteration 540\t train_loss: 1.23 took: 0.12s\n",
            "Epoch 8, Iteration 560\t train_loss: 1.36 took: 0.12s\n",
            "Epoch 8, Iteration 580\t train_loss: 1.34 took: 0.12s\n",
            "Epoch 8, Iteration 600\t train_loss: 1.29 took: 0.12s\n",
            "Epoch 8, Iteration 620\t train_loss: 1.33 took: 0.13s\n",
            "Validation loss = 1.62\n",
            "Epoch 9, Iteration 20\t train_loss: 1.19 took: 0.22s\n",
            "Epoch 9, Iteration 40\t train_loss: 1.15 took: 0.12s\n",
            "Epoch 9, Iteration 60\t train_loss: 1.28 took: 0.13s\n",
            "Epoch 9, Iteration 80\t train_loss: 1.16 took: 0.12s\n",
            "Epoch 9, Iteration 100\t train_loss: 1.16 took: 0.12s\n",
            "Epoch 9, Iteration 120\t train_loss: 1.20 took: 0.12s\n",
            "Epoch 9, Iteration 140\t train_loss: 1.17 took: 0.12s\n",
            "Epoch 9, Iteration 160\t train_loss: 1.30 took: 0.13s\n",
            "Epoch 9, Iteration 180\t train_loss: 1.22 took: 0.13s\n",
            "Epoch 9, Iteration 200\t train_loss: 1.25 took: 0.13s\n",
            "Epoch 9, Iteration 220\t train_loss: 1.23 took: 0.13s\n",
            "Epoch 9, Iteration 240\t train_loss: 1.21 took: 0.12s\n",
            "Epoch 9, Iteration 260\t train_loss: 1.29 took: 0.13s\n",
            "Epoch 9, Iteration 280\t train_loss: 1.20 took: 0.13s\n",
            "Epoch 9, Iteration 300\t train_loss: 1.23 took: 0.13s\n",
            "Epoch 9, Iteration 320\t train_loss: 1.31 took: 0.12s\n",
            "Epoch 9, Iteration 340\t train_loss: 1.26 took: 0.13s\n",
            "Epoch 9, Iteration 360\t train_loss: 1.17 took: 0.13s\n",
            "Epoch 9, Iteration 380\t train_loss: 1.16 took: 0.13s\n",
            "Epoch 9, Iteration 400\t train_loss: 1.30 took: 0.14s\n",
            "Epoch 9, Iteration 420\t train_loss: 1.24 took: 0.12s\n",
            "Epoch 9, Iteration 440\t train_loss: 1.14 took: 0.12s\n",
            "Epoch 9, Iteration 460\t train_loss: 1.28 took: 0.12s\n",
            "Epoch 9, Iteration 480\t train_loss: 1.29 took: 0.13s\n",
            "Epoch 9, Iteration 500\t train_loss: 1.38 took: 0.13s\n",
            "Epoch 9, Iteration 520\t train_loss: 1.31 took: 0.12s\n",
            "Epoch 9, Iteration 540\t train_loss: 1.29 took: 0.13s\n",
            "Epoch 9, Iteration 560\t train_loss: 1.37 took: 0.12s\n",
            "Epoch 9, Iteration 580\t train_loss: 1.28 took: 0.12s\n",
            "Epoch 9, Iteration 600\t train_loss: 1.20 took: 0.12s\n",
            "Epoch 9, Iteration 620\t train_loss: 1.28 took: 0.14s\n",
            "Validation loss = 1.49\n",
            "Epoch 10, Iteration 20\t train_loss: 1.14 took: 0.21s\n",
            "Epoch 10, Iteration 40\t train_loss: 1.04 took: 0.12s\n",
            "Epoch 10, Iteration 60\t train_loss: 1.12 took: 0.12s\n",
            "Epoch 10, Iteration 80\t train_loss: 1.25 took: 0.12s\n",
            "Epoch 10, Iteration 100\t train_loss: 1.16 took: 0.12s\n",
            "Epoch 10, Iteration 120\t train_loss: 1.10 took: 0.13s\n",
            "Epoch 10, Iteration 140\t train_loss: 1.18 took: 0.12s\n",
            "Epoch 10, Iteration 160\t train_loss: 1.23 took: 0.12s\n",
            "Epoch 10, Iteration 180\t train_loss: 1.22 took: 0.14s\n",
            "Epoch 10, Iteration 200\t train_loss: 1.17 took: 0.12s\n",
            "Epoch 10, Iteration 220\t train_loss: 1.19 took: 0.12s\n",
            "Epoch 10, Iteration 240\t train_loss: 1.17 took: 0.12s\n",
            "Epoch 10, Iteration 260\t train_loss: 1.21 took: 0.12s\n",
            "Epoch 10, Iteration 280\t train_loss: 1.13 took: 0.13s\n",
            "Epoch 10, Iteration 300\t train_loss: 1.13 took: 0.13s\n",
            "Epoch 10, Iteration 320\t train_loss: 1.25 took: 0.13s\n",
            "Epoch 10, Iteration 340\t train_loss: 1.17 took: 0.13s\n",
            "Epoch 10, Iteration 360\t train_loss: 1.24 took: 0.12s\n",
            "Epoch 10, Iteration 380\t train_loss: 1.21 took: 0.12s\n",
            "Epoch 10, Iteration 400\t train_loss: 1.25 took: 0.13s\n",
            "Epoch 10, Iteration 420\t train_loss: 1.27 took: 0.13s\n",
            "Epoch 10, Iteration 440\t train_loss: 1.18 took: 0.12s\n",
            "Epoch 10, Iteration 460\t train_loss: 1.20 took: 0.14s\n",
            "Epoch 10, Iteration 480\t train_loss: 1.17 took: 0.13s\n",
            "Epoch 10, Iteration 500\t train_loss: 1.18 took: 0.13s\n",
            "Epoch 10, Iteration 520\t train_loss: 1.16 took: 0.14s\n",
            "Epoch 10, Iteration 540\t train_loss: 1.28 took: 0.14s\n",
            "Epoch 10, Iteration 560\t train_loss: 1.24 took: 0.12s\n",
            "Epoch 10, Iteration 580\t train_loss: 1.20 took: 0.13s\n",
            "Epoch 10, Iteration 600\t train_loss: 1.15 took: 0.12s\n",
            "Epoch 10, Iteration 620\t train_loss: 1.22 took: 0.13s\n",
            "Validation loss = 1.54\n",
            "Training finished, took 47.17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9DHRO7sI9jH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "4775ea87-9c3d-460e-a73b-0d30ea4b5bd5"
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB6GBr5RSHYQGvlJKdRAa+Eop1UH8f51TlOTT0t6SAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.60135\n",
            "Test accuracy is 0.4802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9FxWS8_EZTN",
        "colab_type": "text"
      },
      "source": [
        "#### Placeholder for Nick's visualization code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy5lF03FeuuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import ReLU\n",
        "\n",
        "class GuidedBackprop():\n",
        "    \"\"\"\n",
        "       Produces gradients generated with guided back propagation from the given image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.modules = list(self.model.modules())\n",
        "        self.forward_relu_outputs = []\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        self.update_relus()\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[1]\n",
        "        # Register hook to the first layer\n",
        "        first_layer = self.modules[0]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def update_relus(self):\n",
        "        \"\"\"\n",
        "            Updates relu activation functions so that\n",
        "                1- stores output in forward pass\n",
        "                2- imputes zero for gradient values that are less than zero\n",
        "        \"\"\"\n",
        "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
        "            \"\"\"\n",
        "            If there is a negative gradient, change it to zero\n",
        "            \"\"\"\n",
        "            # Get last forward output\n",
        "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
        "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
        "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0).to(device)\n",
        "            del self.forward_relu_outputs[-1]  # Remove last forward output\n",
        "            return (modified_grad_out,)\n",
        "\n",
        "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
        "            \"\"\"\n",
        "            Store results of forward pass\n",
        "            \"\"\"\n",
        "            self.forward_relu_outputs.append(ten_out)\n",
        "\n",
        "        # Loop through layers, hook up ReLUs\n",
        "        for module in self.modules:\n",
        "            if isinstance(module, ReLU):\n",
        "                module.register_backward_hook(relu_backward_hook_function)\n",
        "                module.register_forward_hook(relu_forward_hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward pass\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_().to(device)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.cpu().numpy()\n",
        "        return gradients_as_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXBWjdsae37M",
        "colab_type": "code",
        "outputId": "dcd3854a-d601-48a4-9e84-17fa8280ff16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Guided backprop\n",
        "GBP = GuidedBackprop(net)\n",
        "image, label = test_set[0]\n",
        "guided_grads = GBP.generate_gradients(Variable(image.unsqueeze(0),).requires_grad_().to(device), label)\n",
        "# print(guided_grads.shape)\n",
        "print(guided_grads)\n",
        "# guided_grads = guided_grads\n",
        "grayscale_im = np.sum(np.abs(guided_grads), axis=0)\n",
        "im_max = np.percentile(grayscale_im, 99)\n",
        "im_min = np.min(grayscale_im)\n",
        "grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "print(grayscale_im.shape)\n",
        "plt.imshow(grayscale_im.squeeze(0).transpose(1,2,0))\n",
        "plt.show()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.14308172 -0.21176518 -0.27604222  0.06454816  0.4058339   0.03289086\n",
            "   0.30899957  0.11206275 -0.24319057  0.00151007 -0.35529187  0.22845864\n",
            "  -0.4222089   0.0346281  -0.2177557   0.21737538 -0.89598644  0.3608249\n",
            "  -0.36439213  0.22659734 -0.0328571   0.05016115 -0.14122248 -0.4303256\n",
            "  -0.16370218  0.04142623 -0.03119509  0.05178383  0.2775355  -0.12633838\n",
            "  -0.664316   -0.06687162  0.09551825 -0.0418645   0.27491245 -1.411891\n",
            "  -0.2468485   0.04591249  0.34983322 -0.19846997 -0.15580575 -0.33949572\n",
            "   0.13825406  0.2393001   0.4705144  -0.48712263 -0.25703418 -0.34427503\n",
            "  -0.37143356  0.00903405 -0.10786869  0.23891753 -0.5029381  -0.03570422\n",
            "  -0.26298517 -0.4987273  -0.2596854   0.04710894 -0.00360737 -0.2427321\n",
            "   0.03617467 -0.61993074 -0.13441265 -0.3360672 ]]\n",
            "(1, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-76652cf66f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgrayscale_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrayscale_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrayscale_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrayscale_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: axes don't match array"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5CTZhhufcnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}