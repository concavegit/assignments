{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day 8 Convnet Example",
      "provenance": [],
      "collapsed_sections": [
        "t-35nSvVIbiT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/08/Day_8_Convnet_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVFu3OVp3Xix",
        "colab_type": "text"
      },
      "source": [
        "# Boilerplate Convnet example\n",
        "In this notebook we'll be looking at training a convolutional neural network the [CIFAR 10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR dataset contains relatively low resolution (32x32 pixel) images of 10 distinct categories of objects.\n",
        "\n",
        "* airplane\t\t\t\t\t\t\t\t\t\t\n",
        "* automobile\t\t\t\t\t\t\t\t\t\t\n",
        "* bird\t\t\t\t\t\t\t\t\t\t\n",
        "* cat\t\t\t\t\t\t\t\t\t\t\n",
        "* deer\t\t\t\t\t\t\t\t\t\t\n",
        "* dog\t\t\t\t\t\t\t\t\t\t\n",
        "* frog\t\t\t\t\t\t\t\t\t\t\n",
        "* horse\t\t\t\t\t\t\t\t\t\t\n",
        "* ship\t\t\t\t\t\t\t\t\t\t\n",
        "* truck\n",
        "\n",
        "We recommend that you run, examine, and understand all the code before attempting any of the exercises so that they make sense in a broader context.\n",
        "\n",
        "Thanks to [Algorithmia](https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/) for some of the base code for this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt3N5feFwBVL",
        "colab_type": "text"
      },
      "source": [
        "## Use a GPU!\n",
        "\n",
        "This notebook uses the GPU functionality of Pytorch and Google Collab. We need to make sure we are running our operations on the GPU, verify this in you notebook settings at the top. Setting found under:\n",
        "\n",
        "`Runtime > Change runtime type > Hardware Accelerator -> GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3alCNsXcczI",
        "colab_type": "text"
      },
      "source": [
        "Let's start by installing a package and importing some modules we'll need later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjeB52XEcWmD",
        "colab_type": "code",
        "outputId": "212f2e95-24bf-407e-c85e-8cb28759c28a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install torchviz\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=d6c2b6047200c1f0420083c2e8488c21df2e8a2c702f8e3de04fd2d2208bec76\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US13M6R4cxbL",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "This dataset happens to be included with pytorch so we can call some pytorch functions to automatically load and parse the data we need to. \n",
        "\n",
        "Don't worry too much about the specifics of this part. Data loading, cleaning, and parsing is often taylored to every dataset so functions from one dataset loading don't often directly transfer to another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKQk3V09cxrC",
        "colab_type": "code",
        "outputId": "12d05008-44cf-40a8-d2e8-43bac485d658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Data set information\n",
        "image_dims = 3, 32, 32\n",
        "n_training_samples = 20000 # How many training images to use\n",
        "n_test_samples = 5000 # How many test images to use\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Load the training set\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./cifardata', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_sampler = SubsetRandomSampler(\n",
        "    np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Load the test set\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root='./cifardata', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifardata/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 169197568/170498071 [00:11<00:00, 18052259.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbZb_B62w_fg",
        "colab_type": "text"
      },
      "source": [
        "Explore the data a bit.  Here are some suggestions.  (If you haven't yet, enable \"Automatically trigger code completion suggestions\" by first enabling beta features through the beaker icon and then clicking on the relevant option under \"tools->preferences->editor\")\n",
        "* Check the shapes of various tensors (note that the training data is stored under `train_set.data`)\n",
        "* Visualize some of the images in the dataset (you can grab data from the train_set using square brackets).  Sample code below. In the past we've shown you how to use subplots to show many images at once (refer to, e.g., the [assignment 5 notebook](https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/05/assignment05.ipynb) for guidance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l-xHj-Gw-7r",
        "colab_type": "code",
        "outputId": "c771907d-38b4-45c4-dea2-d748d963ca89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "def disp_image(image, class_idx, predicted=None):\n",
        "    # need to reorder the tensor dimensions to work properly with imshow\n",
        "    plt.imshow(image.transpose(0,2).transpose(0,1))\n",
        "    plt.axis('off')\n",
        "    if predicted:\n",
        "        plt.title(\"Actual: \" + classes[class_idx] + \"     Predicted: \" + classes[predicted])\n",
        "    else:\n",
        "        plt.title(\"Actual: \" + classes[class_idx])\n",
        "    plt.show()\n",
        "\n",
        "print(\"training set input data shape\", train_set.data.shape)\n",
        "print(\"Number of training outputs\", len(train_set.targets))\n",
        "x, y = train_set[1]\n",
        "disp_image(x, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set input data shape (50000, 32, 32, 3)\n",
            "Number of training outputs 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi1zzYOazFJ4",
        "colab_type": "text"
      },
      "source": [
        "As another quick example, let's show the number of training data points for each particular class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1i02DlZzJzl",
        "colab_type": "code",
        "outputId": "e8ef7052-8611-443b-e86e-60f24ce1557d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(train_set.targets)\n",
        "plt.xticks(ticks=range(10), labels=classes)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBockqReDQ5LUi8EhSerl/wNDBUUZr2CH/wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Z46lIIzLP8",
        "colab_type": "text"
      },
      "source": [
        "Add your own explorations as you see fit, or skip ahead for now if you want to spend more time on the convnet stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jc8znegzlB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcfcP8pzdLNo",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "Now that we have the data loaded, let's define our model architecture. As you saw on the previous assignment, convolutional networks have the following basic layer structure.\n",
        "1. Convolutional layer (this helps detect various image features such as edges and corners)\n",
        "2. (optional) non-linear tranformation (e.g., ReLu)\n",
        "3. Max pooling (this reduces the dimensionality of the image and focuses the network on features that are most salient)\n",
        "4. (repeat 1-3 some number of times)\n",
        "5. Fully connected layer\n",
        "6. (repeat (4) some number of times)\n",
        "7. Output layer\n",
        "\n",
        "So a typically convnet would look something like this.  Note: that the softmax in the figure below is just the multiclass generalization of the sigmoid we've been using for binary classification.\n",
        "\n",
        "![](https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
        "\n",
        "For this example we're going to start off by using one convolutional layer to pick up local image features, then a [maxpool operation](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) to reduce our dimensionality a bit, followed by a fully connected layer to perform some logic computations on those features, and finally a fully connected layer to turn those outputs into predictions for the class.  Typically we might have more of each of these sorts of layers, but we are aiming for a simpler network for our first go.\n",
        "\n",
        "## Software Architecture\n",
        "\n",
        "In `pytorch` our neural network will be a class.  Here are a few things to remember about how `pytorch` works with neural networks.\n",
        "\n",
        "* Your neural network class must inherit from `nn.Module`\n",
        "* You should create the layer objects in the `__init__` method.\n",
        "* The forward call is where the action happens (data inputs are transformed into outputs of the network).\n",
        "\n",
        "Consider consulting [the assignment 7 notebook](https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/07/Assignment_07_Companion_Pytorch_Titanic.ipynb) for a refresher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbxu5y57dLn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "    # The init funciton in Pytorch classes is used to keep track of the parameters of the model\n",
        "    # specifically the ones we want to update with gradient descent + backprop\n",
        "    # So we need to make sure we keep track of all of them here\n",
        "    def __init__(self):\n",
        "        super(MyCNN, self).__init__()\n",
        "        # layers defined here\n",
        "\n",
        "        # Make sure you understand what this convolutional layer is doing.\n",
        "        # E.g., considering looking at help(nn.Conv2D).  Draw a picture of what\n",
        "        # this layer does to the data.\n",
        "\n",
        "        # note: image_dims[0] will be 3 as there are 3 color channels (R, G, B)\n",
        "        num_kernels = 16\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Make sure you understand what this MaxPool2D layer is doing.\n",
        "        # E.g., considering looking at help(nn.MaxPool2d).  Draw a picture of\n",
        "        # what this layer does to the data.\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # maxpool_output_size is the total amount of data coming out of that\n",
        "        # layer.  We have an exercise that asks you to explain why the line of\n",
        "        # code below computes this quantity.\n",
        "        self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
        "\n",
        "        # Add on a fully connected layer (like in our MLP)\n",
        "        # fc stands for fully connected\n",
        "        fc1_size = 64\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
        "\n",
        "        # we'll use this activation function internally in the network\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "        # Convert our fully connected layer into outputs that we can compare to the result\n",
        "        fc2_size = len(classes)\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
        "\n",
        "        # Note: that the output will not represent the probability of the\n",
        "        # output being in each class.  The loss function we will use\n",
        "        # `CrossEntropyLoss` will take care of convering these values to\n",
        "        # probabilities and then computing the log loss with respect to the\n",
        "        # true label.  We could break this out into multiple steps, but it turns\n",
        "        # out that the algorithm will be more numerically stable if we do it in\n",
        "        # one go.  We have included a cell to show you the documentation for\n",
        "        # `CrossEntropyLoss` if you'd like to check it out.\n",
        "        \n",
        "    # The forward function in the class defines the operations performed on a given input to the model\n",
        "    # and returns the output of the model\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.activation_func(x)\n",
        "        # this code flattens the output of the convolution, max pool,\n",
        "        # activation sequence of steps into a vector\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
        "    # returns the loss and optimizer used by the model\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWSZHeWKcGuP",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "(a) Draw a picture of this specific network similar to the generic convnet picture shown previously (it doesn't have to be in the exact same format, just try to include similar information).\n",
        "\n",
        "(b) Explain what is happening in the following line of code (e.g., what is the point of it and where does thsi formula come frome)?\n",
        "\n",
        "```python\n",
        "self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKyjN1VsewEg",
        "colab_type": "text"
      },
      "source": [
        "#### Solution\n",
        "\n",
        "(a) We don't have time to render it for you, but it would include one conv + relu layer, one max pool, a fully connected layer, and finally the outputs.\n",
        "\n",
        "(b) We needs to know the max pool output size so we can properly size the first fully connected layer.  The max pool output size is based on the number of convolutional kernels, the stride of the max pool (in this case max pool goes moves in steps of 2 along row and column, downsampling the filtered image by a factor of 2 in each direction).  If we didn't set padding=1 in the convolutional layer we would also have to account for edge effects from the convolution.  Here the convolution doesn't change the size of the filtered image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upA76kARcosL",
        "colab_type": "text"
      },
      "source": [
        "### Reading the Docs\n",
        "\n",
        "Consider reading up on functions in Pytorch that are confusing using the `help` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7yW3u9F4Rtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "help(nn.CrossEntropyLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQHPW08-f5fK",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "First let's create our model. Let's also check out a graphical representation of our model (using a library we downloaded earlier) to validate the model looks like we think it should.  This is definitely not the prettiest visualization, and there are lots of things included in here that are related to doing the backward pass (to compute the gradients).  Of particular relevance are the blue nodes, which tell you about the various model parameters and layers.\n",
        "\n",
        "**Running the below cell will override your model if have already trained one**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXeauNXlBIK6",
        "colab_type": "code",
        "outputId": "94a2f8ff-c5b4-4b07-f268-59bcb0f89345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    fake_input = Variable(torch.zeros((1,image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "\n",
        "visualize_network(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f0f65a00668>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"267pt\" height=\"493pt\"\n viewBox=\"0.00 0.00 266.50 493.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 489)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-489 262.5,-489 262.5,4 -4,4\"/>\n<!-- 139704106225392 -->\n<g id=\"node1\" class=\"node\">\n<title>139704106225392</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"171,-21 67,-21 67,0 171,0 171,-21\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139704106317472 -->\n<g id=\"node2\" class=\"node\">\n<title>139704106317472</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-91 0,-91 0,-57 54,-57 54,-91\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 139704106317472&#45;&gt;139704106225392 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139704106317472&#45;&gt;139704106225392</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.6543,-56.9832C65.1894,-47.641 81.8926,-36.1122 95.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.2865,-29.7398 103.5283,-21.1788 93.3102,-23.9788 97.2865,-29.7398\"/>\n</g>\n<!-- 139704106317248 -->\n<g id=\"node3\" class=\"node\">\n<title>139704106317248</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"166,-84.5 72,-84.5 72,-63.5 166,-63.5 166,-84.5\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139704106317248&#45;&gt;139704106225392 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139704106317248&#45;&gt;139704106225392</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119,-63.2281C119,-54.5091 119,-41.9699 119,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.5001,-31.1128 119,-21.1128 115.5001,-31.1129 122.5001,-31.1128\"/>\n</g>\n<!-- 139704106319432 -->\n<g id=\"node4\" class=\"node\">\n<title>139704106319432</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170,-154.5 66,-154.5 66,-133.5 170,-133.5 170,-154.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139704106319432&#45;&gt;139704106317248 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139704106319432&#45;&gt;139704106317248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118.1519,-133.3685C118.2972,-123.1925 118.5206,-107.5606 118.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.2034,-94.7806 118.8467,-84.7315 115.2041,-94.6805 122.2034,-94.7806\"/>\n</g>\n<!-- 139704106317024 -->\n<g id=\"node5\" class=\"node\">\n<title>139704106317024</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-231 0,-231 0,-197 54,-197 54,-231\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139704106317024&#45;&gt;139704106319432 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139704106317024&#45;&gt;139704106319432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49.4944,-196.6966C63.7034,-185.7666 81.9745,-171.7119 96.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.447,-163.4565 104.2393,-154.5852 94.179,-157.9081 98.447,-163.4565\"/>\n</g>\n<!-- 139704106318816 -->\n<g id=\"node6\" class=\"node\">\n<title>139704106318816</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-224.5 72.5,-224.5 72.5,-203.5 163.5,-203.5 163.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139704106318816&#45;&gt;139704106319432 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139704106318816&#45;&gt;139704106319432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-203.3685C118,-193.1925 118,-177.5606 118,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-164.7315 118,-154.7315 114.5001,-164.7316 121.5001,-164.7315\"/>\n</g>\n<!-- 139704106317192 -->\n<g id=\"node7\" class=\"node\">\n<title>139704106317192</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-294.5 71,-294.5 71,-273.5 165,-273.5 165,-294.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139704106317192&#45;&gt;139704106318816 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139704106317192&#45;&gt;139704106318816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-273.3685C118,-263.1925 118,-247.5606 118,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-234.7315 118,-224.7315 114.5001,-234.7316 121.5001,-234.7315\"/>\n</g>\n<!-- 139704105912640 -->\n<g id=\"node8\" class=\"node\">\n<title>139704105912640</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-358 28,-358 28,-337 208,-337 208,-358\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139704105912640&#45;&gt;139704106317192 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139704105912640&#45;&gt;139704106317192</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-336.7281C118,-328.0091 118,-315.4699 118,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-304.6128 118,-294.6128 114.5001,-304.6129 121.5001,-304.6128\"/>\n</g>\n<!-- 139704105910960 -->\n<g id=\"node9\" class=\"node\">\n<title>139704105910960</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196.5,-415 39.5,-415 39.5,-394 196.5,-394 196.5,-415\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139704105910960&#45;&gt;139704105912640 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139704105910960&#45;&gt;139704105912640</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-393.7787C118,-386.6134 118,-376.9517 118,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-368.1732 118,-358.1732 114.5001,-368.1732 121.5001,-368.1732\"/>\n</g>\n<!-- 139704105912248 -->\n<g id=\"node10\" class=\"node\">\n<title>139704105912248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"112.5,-485 31.5,-485 31.5,-451 112.5,-451 112.5,-485\"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"72\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16, 3, 3, 3)</text>\n</g>\n<!-- 139704105912248&#45;&gt;139704105910960 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139704105912248&#45;&gt;139704105910960</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.3272,-450.9832C90.4107,-442.5853 97.7742,-432.4204 104.0621,-423.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.0945,-425.5204 110.1266,-415.3687 101.4256,-421.4138 107.0945,-425.5204\"/>\n</g>\n<!-- 139704105912416 -->\n<g id=\"node11\" class=\"node\">\n<title>139704105912416</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-485 131,-485 131,-451 199,-451 199,-485\"/>\n<text text-anchor=\"middle\" x=\"165\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"165\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (16)</text>\n</g>\n<!-- 139704105912416&#45;&gt;139704105910960 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139704105912416&#45;&gt;139704105910960</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M152.4049,-450.9832C146.1237,-442.4969 138.5069,-432.2062 132.0384,-423.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.8071,-421.3243 126.0445,-415.3687 129.1806,-425.4888 134.8071,-421.3243\"/>\n</g>\n<!-- 139704106316856 -->\n<g id=\"node12\" class=\"node\">\n<title>139704106316856</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"254.5,-224.5 181.5,-224.5 181.5,-203.5 254.5,-203.5 254.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139704106316856&#45;&gt;139704106319432 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139704106316856&#45;&gt;139704106319432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M202.8122,-203.3685C186.492,-191.9444 160.3486,-173.644 141.398,-160.3786\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.2026,-157.3695 133.0031,-154.5022 139.1883,-163.1042 143.2026,-157.3695\"/>\n</g>\n<!-- 139704105911632 -->\n<g id=\"node13\" class=\"node\">\n<title>139704105911632</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"252.5,-301 183.5,-301 183.5,-267 252.5,-267 252.5,-301\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"218\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 4096)</text>\n</g>\n<!-- 139704105911632&#45;&gt;139704106316856 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139704105911632&#45;&gt;139704106316856</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218,-266.6966C218,-257.0634 218,-245.003 218,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"221.5001,-234.7912 218,-224.7913 214.5001,-234.7913 221.5001,-234.7912\"/>\n</g>\n<!-- 139704106318424 -->\n<g id=\"node14\" class=\"node\">\n<title>139704106318424</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.5,-84.5 185.5,-84.5 185.5,-63.5 258.5,-63.5 258.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139704106318424&#45;&gt;139704106225392 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139704106318424&#45;&gt;139704106225392</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.5275,-63.2281C188.1519,-53.1325 163.4682,-37.9149 144.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.5636,-23.3814 136.2145,-21.1128 142.8901,-29.3401 146.5636,-23.3814\"/>\n</g>\n<!-- 139704106318536 -->\n<g id=\"node15\" class=\"node\">\n<title>139704106318536</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"255.5,-161 188.5,-161 188.5,-127 255.5,-127 255.5,-161\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"222\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 64)</text>\n</g>\n<!-- 139704106318536&#45;&gt;139704106318424 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139704106318536&#45;&gt;139704106318424</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M222,-126.6966C222,-117.0634 222,-105.003 222,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.5001,-94.7912 222,-84.7913 218.5001,-94.7913 225.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glMz-8OmoF-u",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Next we'll define the settings we'll use for training.  \n",
        "\n",
        "In the last notebook we mentioned the idea of stochastic gradient descent where we only use a subset of the data to estimate the gradient before doing an update to our model parameters.  In the notebook from last time we used all of our data to compute the gradient (thus we just used regular gradient descent). Although reliable, this method is often slow for larger models.\n",
        "\n",
        "In this problem we are going to be using a form of **Stochastic Gradient Descent** called **Mini-batch Gradient Descent**.  For mini-batch gradient descent we will use a small batch of data to estimate our gradient and then do an update step (by moving our weights down the negative gradient).  We'll iterate through the whole dataset as a series of mini-batches and perform a step after processing each batch.  This is a much noisier process of weight optimization, but often converges more quickly than the normal gradient descent.\n",
        "\n",
        "In the code below we define a `DataLoader` function that iterates through the training set (or test set) in increments of `batch_size`. We then define a training wrapper function that will make modifying your model parameters (as we'll do later in this notebook) easier (we don't want to have to keep cutting and pasting code everywhere in this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I52pm5-AmwKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 10\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                           sampler=train_sampler, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=128, sampler=test_sampler, num_workers=2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        test_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        test_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 20\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            idx += 1\n",
        "\n",
        "        # At the end of the epoch, do a pass on the test set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "\n",
        "            # Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = net(inputs)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "        test_loss_hist.append(total_test_loss / len(test_loader))\n",
        "        test_hist_x.append(idx)\n",
        "        print(\"Validation loss = {:.2f}\".format(\n",
        "            total_test_loss / len(test_loader)))\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQorZsxQBIc6",
        "colab_type": "text"
      },
      "source": [
        "### Now let's train the model!\n",
        "\n",
        "Here we go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvcJyKgwdSR8",
        "colab_type": "code",
        "outputId": "62464195-2398-4511-8d11-9e7013cbd17b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 20\t train_loss: 2.41 took: 0.30s\n",
            "Epoch 1, Iteration 40\t train_loss: 2.30 took: 0.17s\n",
            "Epoch 1, Iteration 60\t train_loss: 2.23 took: 0.18s\n",
            "Epoch 1, Iteration 80\t train_loss: 2.18 took: 0.16s\n",
            "Epoch 1, Iteration 100\t train_loss: 2.15 took: 0.18s\n",
            "Epoch 1, Iteration 120\t train_loss: 2.10 took: 0.17s\n",
            "Epoch 1, Iteration 140\t train_loss: 2.03 took: 0.18s\n",
            "Epoch 1, Iteration 160\t train_loss: 2.03 took: 0.18s\n",
            "Epoch 1, Iteration 180\t train_loss: 2.01 took: 0.19s\n",
            "Epoch 1, Iteration 200\t train_loss: 1.95 took: 0.16s\n",
            "Epoch 1, Iteration 220\t train_loss: 2.02 took: 0.17s\n",
            "Epoch 1, Iteration 240\t train_loss: 1.95 took: 0.17s\n",
            "Epoch 1, Iteration 260\t train_loss: 1.87 took: 0.17s\n",
            "Epoch 1, Iteration 280\t train_loss: 1.86 took: 0.17s\n",
            "Epoch 1, Iteration 300\t train_loss: 1.88 took: 0.21s\n",
            "Epoch 1, Iteration 320\t train_loss: 1.93 took: 0.17s\n",
            "Epoch 1, Iteration 340\t train_loss: 1.87 took: 0.16s\n",
            "Epoch 1, Iteration 360\t train_loss: 1.81 took: 0.17s\n",
            "Epoch 1, Iteration 380\t train_loss: 1.84 took: 0.17s\n",
            "Epoch 1, Iteration 400\t train_loss: 1.92 took: 0.17s\n",
            "Epoch 1, Iteration 420\t train_loss: 1.77 took: 0.18s\n",
            "Epoch 1, Iteration 440\t train_loss: 1.73 took: 0.17s\n",
            "Epoch 1, Iteration 460\t train_loss: 1.74 took: 0.17s\n",
            "Epoch 1, Iteration 480\t train_loss: 1.77 took: 0.17s\n",
            "Epoch 1, Iteration 500\t train_loss: 1.76 took: 0.17s\n",
            "Epoch 1, Iteration 520\t train_loss: 1.83 took: 0.17s\n",
            "Epoch 1, Iteration 540\t train_loss: 1.81 took: 0.19s\n",
            "Epoch 1, Iteration 560\t train_loss: 1.78 took: 0.16s\n",
            "Epoch 1, Iteration 580\t train_loss: 1.75 took: 0.17s\n",
            "Epoch 1, Iteration 600\t train_loss: 1.66 took: 0.18s\n",
            "Epoch 1, Iteration 620\t train_loss: 1.73 took: 0.17s\n",
            "Validation loss = 1.70\n",
            "Epoch 2, Iteration 20\t train_loss: 1.66 took: 0.24s\n",
            "Epoch 2, Iteration 40\t train_loss: 1.62 took: 0.17s\n",
            "Epoch 2, Iteration 60\t train_loss: 1.68 took: 0.17s\n",
            "Epoch 2, Iteration 80\t train_loss: 1.59 took: 0.17s\n",
            "Epoch 2, Iteration 100\t train_loss: 1.56 took: 0.17s\n",
            "Epoch 2, Iteration 120\t train_loss: 1.68 took: 0.17s\n",
            "Epoch 2, Iteration 140\t train_loss: 1.71 took: 0.17s\n",
            "Epoch 2, Iteration 160\t train_loss: 1.66 took: 0.18s\n",
            "Epoch 2, Iteration 180\t train_loss: 1.66 took: 0.20s\n",
            "Epoch 2, Iteration 200\t train_loss: 1.62 took: 0.18s\n",
            "Epoch 2, Iteration 220\t train_loss: 1.60 took: 0.19s\n",
            "Epoch 2, Iteration 240\t train_loss: 1.53 took: 0.18s\n",
            "Epoch 2, Iteration 260\t train_loss: 1.68 took: 0.18s\n",
            "Epoch 2, Iteration 280\t train_loss: 1.68 took: 0.19s\n",
            "Epoch 2, Iteration 300\t train_loss: 1.68 took: 0.19s\n",
            "Epoch 2, Iteration 320\t train_loss: 1.65 took: 0.18s\n",
            "Epoch 2, Iteration 340\t train_loss: 1.70 took: 0.20s\n",
            "Epoch 2, Iteration 360\t train_loss: 1.72 took: 0.18s\n",
            "Epoch 2, Iteration 380\t train_loss: 1.63 took: 0.20s\n",
            "Epoch 2, Iteration 400\t train_loss: 1.69 took: 0.20s\n",
            "Epoch 2, Iteration 420\t train_loss: 1.65 took: 0.19s\n",
            "Epoch 2, Iteration 440\t train_loss: 1.61 took: 0.20s\n",
            "Epoch 2, Iteration 460\t train_loss: 1.67 took: 0.20s\n",
            "Epoch 2, Iteration 480\t train_loss: 1.71 took: 0.19s\n",
            "Epoch 2, Iteration 500\t train_loss: 1.64 took: 0.20s\n",
            "Epoch 2, Iteration 520\t train_loss: 1.53 took: 0.19s\n",
            "Epoch 2, Iteration 540\t train_loss: 1.65 took: 0.19s\n",
            "Epoch 2, Iteration 560\t train_loss: 1.77 took: 0.19s\n",
            "Epoch 2, Iteration 580\t train_loss: 1.67 took: 0.20s\n",
            "Epoch 2, Iteration 600\t train_loss: 1.57 took: 0.17s\n",
            "Epoch 2, Iteration 620\t train_loss: 1.63 took: 0.18s\n",
            "Validation loss = 1.68\n",
            "Epoch 3, Iteration 20\t train_loss: 1.48 took: 0.25s\n",
            "Epoch 3, Iteration 40\t train_loss: 1.46 took: 0.16s\n",
            "Epoch 3, Iteration 60\t train_loss: 1.45 took: 0.17s\n",
            "Epoch 3, Iteration 80\t train_loss: 1.51 took: 0.17s\n",
            "Epoch 3, Iteration 100\t train_loss: 1.51 took: 0.17s\n",
            "Epoch 3, Iteration 120\t train_loss: 1.48 took: 0.16s\n",
            "Epoch 3, Iteration 140\t train_loss: 1.52 took: 0.17s\n",
            "Epoch 3, Iteration 160\t train_loss: 1.47 took: 0.16s\n",
            "Epoch 3, Iteration 180\t train_loss: 1.70 took: 0.17s\n",
            "Epoch 3, Iteration 200\t train_loss: 1.62 took: 0.16s\n",
            "Epoch 3, Iteration 220\t train_loss: 1.59 took: 0.19s\n",
            "Epoch 3, Iteration 240\t train_loss: 1.49 took: 0.16s\n",
            "Epoch 3, Iteration 260\t train_loss: 1.48 took: 0.17s\n",
            "Epoch 3, Iteration 280\t train_loss: 1.50 took: 0.17s\n",
            "Epoch 3, Iteration 300\t train_loss: 1.51 took: 0.20s\n",
            "Epoch 3, Iteration 320\t train_loss: 1.51 took: 0.17s\n",
            "Epoch 3, Iteration 340\t train_loss: 1.54 took: 0.18s\n",
            "Epoch 3, Iteration 360\t train_loss: 1.50 took: 0.16s\n",
            "Epoch 3, Iteration 380\t train_loss: 1.50 took: 0.18s\n",
            "Epoch 3, Iteration 400\t train_loss: 1.63 took: 0.16s\n",
            "Epoch 3, Iteration 420\t train_loss: 1.57 took: 0.18s\n",
            "Epoch 3, Iteration 440\t train_loss: 1.58 took: 0.17s\n",
            "Epoch 3, Iteration 460\t train_loss: 1.52 took: 0.20s\n",
            "Epoch 3, Iteration 480\t train_loss: 1.56 took: 0.18s\n",
            "Epoch 3, Iteration 500\t train_loss: 1.43 took: 0.18s\n",
            "Epoch 3, Iteration 520\t train_loss: 1.49 took: 0.17s\n",
            "Epoch 3, Iteration 540\t train_loss: 1.64 took: 0.16s\n",
            "Epoch 3, Iteration 560\t train_loss: 1.57 took: 0.18s\n",
            "Epoch 3, Iteration 580\t train_loss: 1.52 took: 0.18s\n",
            "Epoch 3, Iteration 600\t train_loss: 1.54 took: 0.16s\n",
            "Epoch 3, Iteration 620\t train_loss: 1.51 took: 0.17s\n",
            "Validation loss = 1.58\n",
            "Epoch 4, Iteration 20\t train_loss: 1.27 took: 0.24s\n",
            "Epoch 4, Iteration 40\t train_loss: 1.32 took: 0.16s\n",
            "Epoch 4, Iteration 60\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 4, Iteration 80\t train_loss: 1.40 took: 0.18s\n",
            "Epoch 4, Iteration 100\t train_loss: 1.38 took: 0.18s\n",
            "Epoch 4, Iteration 120\t train_loss: 1.48 took: 0.17s\n",
            "Epoch 4, Iteration 140\t train_loss: 1.50 took: 0.17s\n",
            "Epoch 4, Iteration 160\t train_loss: 1.47 took: 0.17s\n",
            "Epoch 4, Iteration 180\t train_loss: 1.45 took: 0.17s\n",
            "Epoch 4, Iteration 200\t train_loss: 1.42 took: 0.17s\n",
            "Epoch 4, Iteration 220\t train_loss: 1.43 took: 0.17s\n",
            "Epoch 4, Iteration 240\t train_loss: 1.36 took: 0.17s\n",
            "Epoch 4, Iteration 260\t train_loss: 1.44 took: 0.16s\n",
            "Epoch 4, Iteration 280\t train_loss: 1.47 took: 0.18s\n",
            "Epoch 4, Iteration 300\t train_loss: 1.48 took: 0.17s\n",
            "Epoch 4, Iteration 320\t train_loss: 1.50 took: 0.18s\n",
            "Epoch 4, Iteration 340\t train_loss: 1.47 took: 0.18s\n",
            "Epoch 4, Iteration 360\t train_loss: 1.40 took: 0.17s\n",
            "Epoch 4, Iteration 380\t train_loss: 1.47 took: 0.18s\n",
            "Epoch 4, Iteration 400\t train_loss: 1.58 took: 0.17s\n",
            "Epoch 4, Iteration 420\t train_loss: 1.55 took: 0.19s\n",
            "Epoch 4, Iteration 440\t train_loss: 1.41 took: 0.18s\n",
            "Epoch 4, Iteration 460\t train_loss: 1.45 took: 0.17s\n",
            "Epoch 4, Iteration 480\t train_loss: 1.45 took: 0.17s\n",
            "Epoch 4, Iteration 500\t train_loss: 1.48 took: 0.17s\n",
            "Epoch 4, Iteration 520\t train_loss: 1.54 took: 0.17s\n",
            "Epoch 4, Iteration 540\t train_loss: 1.49 took: 0.18s\n",
            "Epoch 4, Iteration 560\t train_loss: 1.41 took: 0.17s\n",
            "Epoch 4, Iteration 580\t train_loss: 1.53 took: 0.18s\n",
            "Epoch 4, Iteration 600\t train_loss: 1.48 took: 0.16s\n",
            "Epoch 4, Iteration 620\t train_loss: 1.45 took: 0.17s\n",
            "Validation loss = 1.61\n",
            "Epoch 5, Iteration 20\t train_loss: 1.35 took: 0.29s\n",
            "Epoch 5, Iteration 40\t train_loss: 1.30 took: 0.19s\n",
            "Epoch 5, Iteration 60\t train_loss: 1.31 took: 0.19s\n",
            "Epoch 5, Iteration 80\t train_loss: 1.36 took: 0.18s\n",
            "Epoch 5, Iteration 100\t train_loss: 1.36 took: 0.18s\n",
            "Epoch 5, Iteration 120\t train_loss: 1.45 took: 0.17s\n",
            "Epoch 5, Iteration 140\t train_loss: 1.35 took: 0.18s\n",
            "Epoch 5, Iteration 160\t train_loss: 1.40 took: 0.18s\n",
            "Epoch 5, Iteration 180\t train_loss: 1.23 took: 0.17s\n",
            "Epoch 5, Iteration 200\t train_loss: 1.32 took: 0.17s\n",
            "Epoch 5, Iteration 220\t train_loss: 1.40 took: 0.18s\n",
            "Epoch 5, Iteration 240\t train_loss: 1.40 took: 0.16s\n",
            "Epoch 5, Iteration 260\t train_loss: 1.30 took: 0.17s\n",
            "Epoch 5, Iteration 280\t train_loss: 1.43 took: 0.18s\n",
            "Epoch 5, Iteration 300\t train_loss: 1.44 took: 0.18s\n",
            "Epoch 5, Iteration 320\t train_loss: 1.38 took: 0.18s\n",
            "Epoch 5, Iteration 340\t train_loss: 1.39 took: 0.17s\n",
            "Epoch 5, Iteration 360\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 5, Iteration 380\t train_loss: 1.42 took: 0.18s\n",
            "Epoch 5, Iteration 400\t train_loss: 1.46 took: 0.18s\n",
            "Epoch 5, Iteration 420\t train_loss: 1.46 took: 0.19s\n",
            "Epoch 5, Iteration 440\t train_loss: 1.41 took: 0.17s\n",
            "Epoch 5, Iteration 460\t train_loss: 1.39 took: 0.18s\n",
            "Epoch 5, Iteration 480\t train_loss: 1.37 took: 0.17s\n",
            "Epoch 5, Iteration 500\t train_loss: 1.45 took: 0.20s\n",
            "Epoch 5, Iteration 520\t train_loss: 1.44 took: 0.18s\n",
            "Epoch 5, Iteration 540\t train_loss: 1.45 took: 0.17s\n",
            "Epoch 5, Iteration 560\t train_loss: 1.41 took: 0.19s\n",
            "Epoch 5, Iteration 580\t train_loss: 1.42 took: 0.20s\n",
            "Epoch 5, Iteration 600\t train_loss: 1.49 took: 0.20s\n",
            "Epoch 5, Iteration 620\t train_loss: 1.41 took: 0.21s\n",
            "Validation loss = 1.64\n",
            "Epoch 6, Iteration 20\t train_loss: 1.33 took: 0.24s\n",
            "Epoch 6, Iteration 40\t train_loss: 1.28 took: 0.18s\n",
            "Epoch 6, Iteration 60\t train_loss: 1.19 took: 0.19s\n",
            "Epoch 6, Iteration 80\t train_loss: 1.25 took: 0.18s\n",
            "Epoch 6, Iteration 100\t train_loss: 1.26 took: 0.20s\n",
            "Epoch 6, Iteration 120\t train_loss: 1.32 took: 0.19s\n",
            "Epoch 6, Iteration 140\t train_loss: 1.29 took: 0.19s\n",
            "Epoch 6, Iteration 160\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 6, Iteration 180\t train_loss: 1.33 took: 0.18s\n",
            "Epoch 6, Iteration 200\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 6, Iteration 220\t train_loss: 1.35 took: 0.18s\n",
            "Epoch 6, Iteration 240\t train_loss: 1.34 took: 0.16s\n",
            "Epoch 6, Iteration 260\t train_loss: 1.40 took: 0.17s\n",
            "Epoch 6, Iteration 280\t train_loss: 1.36 took: 0.16s\n",
            "Epoch 6, Iteration 300\t train_loss: 1.43 took: 0.18s\n",
            "Epoch 6, Iteration 320\t train_loss: 1.33 took: 0.17s\n",
            "Epoch 6, Iteration 340\t train_loss: 1.25 took: 0.17s\n",
            "Epoch 6, Iteration 360\t train_loss: 1.31 took: 0.16s\n",
            "Epoch 6, Iteration 380\t train_loss: 1.29 took: 0.18s\n",
            "Epoch 6, Iteration 400\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 6, Iteration 420\t train_loss: 1.34 took: 0.18s\n",
            "Epoch 6, Iteration 440\t train_loss: 1.43 took: 0.20s\n",
            "Epoch 6, Iteration 460\t train_loss: 1.41 took: 0.18s\n",
            "Epoch 6, Iteration 480\t train_loss: 1.43 took: 0.17s\n",
            "Epoch 6, Iteration 500\t train_loss: 1.30 took: 0.18s\n",
            "Epoch 6, Iteration 520\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 6, Iteration 540\t train_loss: 1.46 took: 0.18s\n",
            "Epoch 6, Iteration 560\t train_loss: 1.36 took: 0.17s\n",
            "Epoch 6, Iteration 580\t train_loss: 1.44 took: 0.18s\n",
            "Epoch 6, Iteration 600\t train_loss: 1.37 took: 0.16s\n",
            "Epoch 6, Iteration 620\t train_loss: 1.33 took: 0.20s\n",
            "Validation loss = 1.63\n",
            "Epoch 7, Iteration 20\t train_loss: 1.34 took: 0.25s\n",
            "Epoch 7, Iteration 40\t train_loss: 1.28 took: 0.16s\n",
            "Epoch 7, Iteration 60\t train_loss: 1.24 took: 0.18s\n",
            "Epoch 7, Iteration 80\t train_loss: 1.26 took: 0.17s\n",
            "Epoch 7, Iteration 100\t train_loss: 1.19 took: 0.18s\n",
            "Epoch 7, Iteration 120\t train_loss: 1.24 took: 0.17s\n",
            "Epoch 7, Iteration 140\t train_loss: 1.24 took: 0.17s\n",
            "Epoch 7, Iteration 160\t train_loss: 1.24 took: 0.17s\n",
            "Epoch 7, Iteration 180\t train_loss: 1.18 took: 0.17s\n",
            "Epoch 7, Iteration 200\t train_loss: 1.25 took: 0.18s\n",
            "Epoch 7, Iteration 220\t train_loss: 1.20 took: 0.18s\n",
            "Epoch 7, Iteration 240\t train_loss: 1.34 took: 0.17s\n",
            "Epoch 7, Iteration 260\t train_loss: 1.32 took: 0.18s\n",
            "Epoch 7, Iteration 280\t train_loss: 1.29 took: 0.17s\n",
            "Epoch 7, Iteration 300\t train_loss: 1.30 took: 0.19s\n",
            "Epoch 7, Iteration 320\t train_loss: 1.26 took: 0.18s\n",
            "Epoch 7, Iteration 340\t train_loss: 1.29 took: 0.19s\n",
            "Epoch 7, Iteration 360\t train_loss: 1.31 took: 0.20s\n",
            "Epoch 7, Iteration 380\t train_loss: 1.35 took: 0.19s\n",
            "Epoch 7, Iteration 400\t train_loss: 1.38 took: 0.19s\n",
            "Epoch 7, Iteration 420\t train_loss: 1.36 took: 0.19s\n",
            "Epoch 7, Iteration 440\t train_loss: 1.32 took: 0.17s\n",
            "Epoch 7, Iteration 460\t train_loss: 1.29 took: 0.18s\n",
            "Epoch 7, Iteration 480\t train_loss: 1.26 took: 0.17s\n",
            "Epoch 7, Iteration 500\t train_loss: 1.28 took: 0.19s\n",
            "Epoch 7, Iteration 520\t train_loss: 1.24 took: 0.17s\n",
            "Epoch 7, Iteration 540\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 7, Iteration 560\t train_loss: 1.35 took: 0.17s\n",
            "Epoch 7, Iteration 580\t train_loss: 1.37 took: 0.18s\n",
            "Epoch 7, Iteration 600\t train_loss: 1.41 took: 0.17s\n",
            "Epoch 7, Iteration 620\t train_loss: 1.32 took: 0.18s\n",
            "Validation loss = 1.70\n",
            "Epoch 8, Iteration 20\t train_loss: 1.21 took: 0.27s\n",
            "Epoch 8, Iteration 40\t train_loss: 1.20 took: 0.17s\n",
            "Epoch 8, Iteration 60\t train_loss: 1.22 took: 0.17s\n",
            "Epoch 8, Iteration 80\t train_loss: 1.27 took: 0.17s\n",
            "Epoch 8, Iteration 100\t train_loss: 1.22 took: 0.18s\n",
            "Epoch 8, Iteration 120\t train_loss: 1.30 took: 0.18s\n",
            "Epoch 8, Iteration 140\t train_loss: 1.20 took: 0.20s\n",
            "Epoch 8, Iteration 160\t train_loss: 1.20 took: 0.17s\n",
            "Epoch 8, Iteration 180\t train_loss: 1.25 took: 0.16s\n",
            "Epoch 8, Iteration 200\t train_loss: 1.20 took: 0.16s\n",
            "Epoch 8, Iteration 220\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 8, Iteration 240\t train_loss: 1.16 took: 0.17s\n",
            "Epoch 8, Iteration 260\t train_loss: 1.27 took: 0.20s\n",
            "Epoch 8, Iteration 280\t train_loss: 1.23 took: 0.19s\n",
            "Epoch 8, Iteration 300\t train_loss: 1.19 took: 0.20s\n",
            "Epoch 8, Iteration 320\t train_loss: 1.27 took: 0.19s\n",
            "Epoch 8, Iteration 340\t train_loss: 1.19 took: 0.17s\n",
            "Epoch 8, Iteration 360\t train_loss: 1.23 took: 0.19s\n",
            "Epoch 8, Iteration 380\t train_loss: 1.21 took: 0.18s\n",
            "Epoch 8, Iteration 400\t train_loss: 1.25 took: 0.18s\n",
            "Epoch 8, Iteration 420\t train_loss: 1.32 took: 0.18s\n",
            "Epoch 8, Iteration 440\t train_loss: 1.25 took: 0.17s\n",
            "Epoch 8, Iteration 460\t train_loss: 1.32 took: 0.18s\n",
            "Epoch 8, Iteration 480\t train_loss: 1.29 took: 0.18s\n",
            "Epoch 8, Iteration 500\t train_loss: 1.30 took: 0.18s\n",
            "Epoch 8, Iteration 520\t train_loss: 1.35 took: 0.16s\n",
            "Epoch 8, Iteration 540\t train_loss: 1.28 took: 0.18s\n",
            "Epoch 8, Iteration 560\t train_loss: 1.31 took: 0.17s\n",
            "Epoch 8, Iteration 580\t train_loss: 1.23 took: 0.17s\n",
            "Epoch 8, Iteration 600\t train_loss: 1.23 took: 0.19s\n",
            "Epoch 8, Iteration 620\t train_loss: 1.28 took: 0.21s\n",
            "Validation loss = 1.72\n",
            "Epoch 9, Iteration 20\t train_loss: 1.15 took: 0.24s\n",
            "Epoch 9, Iteration 40\t train_loss: 1.10 took: 0.16s\n",
            "Epoch 9, Iteration 60\t train_loss: 1.19 took: 0.17s\n",
            "Epoch 9, Iteration 80\t train_loss: 1.15 took: 0.16s\n",
            "Epoch 9, Iteration 100\t train_loss: 1.15 took: 0.19s\n",
            "Epoch 9, Iteration 120\t train_loss: 1.17 took: 0.17s\n",
            "Epoch 9, Iteration 140\t train_loss: 1.24 took: 0.17s\n",
            "Epoch 9, Iteration 160\t train_loss: 1.17 took: 0.17s\n",
            "Epoch 9, Iteration 180\t train_loss: 1.15 took: 0.18s\n",
            "Epoch 9, Iteration 200\t train_loss: 1.28 took: 0.17s\n",
            "Epoch 9, Iteration 220\t train_loss: 1.18 took: 0.18s\n",
            "Epoch 9, Iteration 240\t train_loss: 1.12 took: 0.16s\n",
            "Epoch 9, Iteration 260\t train_loss: 1.26 took: 0.18s\n",
            "Epoch 9, Iteration 280\t train_loss: 1.23 took: 0.16s\n",
            "Epoch 9, Iteration 300\t train_loss: 1.15 took: 0.17s\n",
            "Epoch 9, Iteration 320\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 9, Iteration 340\t train_loss: 1.21 took: 0.18s\n",
            "Epoch 9, Iteration 360\t train_loss: 1.26 took: 0.17s\n",
            "Epoch 9, Iteration 380\t train_loss: 1.16 took: 0.17s\n",
            "Epoch 9, Iteration 400\t train_loss: 1.29 took: 0.17s\n",
            "Epoch 9, Iteration 420\t train_loss: 1.31 took: 0.19s\n",
            "Epoch 9, Iteration 440\t train_loss: 1.32 took: 0.18s\n",
            "Epoch 9, Iteration 460\t train_loss: 1.35 took: 0.18s\n",
            "Epoch 9, Iteration 480\t train_loss: 1.31 took: 0.16s\n",
            "Epoch 9, Iteration 500\t train_loss: 1.28 took: 0.18s\n",
            "Epoch 9, Iteration 520\t train_loss: 1.23 took: 0.17s\n",
            "Epoch 9, Iteration 540\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 9, Iteration 560\t train_loss: 1.25 took: 0.19s\n",
            "Epoch 9, Iteration 580\t train_loss: 1.18 took: 0.20s\n",
            "Epoch 9, Iteration 600\t train_loss: 1.26 took: 0.17s\n",
            "Epoch 9, Iteration 620\t train_loss: 1.31 took: 0.17s\n",
            "Validation loss = 1.74\n",
            "Epoch 10, Iteration 20\t train_loss: 1.10 took: 0.24s\n",
            "Epoch 10, Iteration 40\t train_loss: 1.05 took: 0.17s\n",
            "Epoch 10, Iteration 60\t train_loss: 1.15 took: 0.18s\n",
            "Epoch 10, Iteration 80\t train_loss: 1.07 took: 0.16s\n",
            "Epoch 10, Iteration 100\t train_loss: 1.15 took: 0.17s\n",
            "Epoch 10, Iteration 120\t train_loss: 1.14 took: 0.16s\n",
            "Epoch 10, Iteration 140\t train_loss: 1.15 took: 0.18s\n",
            "Epoch 10, Iteration 160\t train_loss: 1.15 took: 0.17s\n",
            "Epoch 10, Iteration 180\t train_loss: 1.16 took: 0.18s\n",
            "Epoch 10, Iteration 200\t train_loss: 1.03 took: 0.16s\n",
            "Epoch 10, Iteration 220\t train_loss: 1.14 took: 0.18s\n",
            "Epoch 10, Iteration 240\t train_loss: 1.20 took: 0.16s\n",
            "Epoch 10, Iteration 260\t train_loss: 1.17 took: 0.18s\n",
            "Epoch 10, Iteration 280\t train_loss: 1.15 took: 0.17s\n",
            "Epoch 10, Iteration 300\t train_loss: 1.22 took: 0.19s\n",
            "Epoch 10, Iteration 320\t train_loss: 1.05 took: 0.17s\n",
            "Epoch 10, Iteration 340\t train_loss: 1.13 took: 0.18s\n",
            "Epoch 10, Iteration 360\t train_loss: 1.14 took: 0.18s\n",
            "Epoch 10, Iteration 380\t train_loss: 1.19 took: 0.18s\n",
            "Epoch 10, Iteration 400\t train_loss: 1.24 took: 0.17s\n",
            "Epoch 10, Iteration 420\t train_loss: 1.20 took: 0.19s\n",
            "Epoch 10, Iteration 440\t train_loss: 1.20 took: 0.17s\n",
            "Epoch 10, Iteration 460\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 10, Iteration 480\t train_loss: 1.22 took: 0.18s\n",
            "Epoch 10, Iteration 500\t train_loss: 1.23 took: 0.20s\n",
            "Epoch 10, Iteration 520\t train_loss: 1.17 took: 0.20s\n",
            "Epoch 10, Iteration 540\t train_loss: 1.20 took: 0.20s\n",
            "Epoch 10, Iteration 560\t train_loss: 1.26 took: 0.18s\n",
            "Epoch 10, Iteration 580\t train_loss: 1.22 took: 0.18s\n",
            "Epoch 10, Iteration 600\t train_loss: 1.28 took: 0.17s\n",
            "Epoch 10, Iteration 620\t train_loss: 1.20 took: 0.17s\n",
            "Validation loss = 1.87\n",
            "Training finished, took 65.15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz_kZWhpEhuT",
        "colab_type": "text"
      },
      "source": [
        "For this problem the speedup of using a GPU is not as great as it would be for other problems (here the speedup is 2-3x).  Generally the bigger the network is (e.g., bigger images) the greater the speedup.  If you want to play around with this on your own, go the model architecture section and change the device variable to \"cpu\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh1Z2O3QzVNd",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "Lets check the outputs of our network. First Let's plot the loss of the network over time to see if any learning actually occured."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbLDTiF960zY",
        "colab_type": "code",
        "outputId": "70e3f6b8-88bd-4926-be00-88ec2cf3831b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzuX8P2g0PlAB5gp4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-WwSRpPA-O",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Interpret what is happening in these curves.  Why is the training curve sampled so much more often (hint: look at the train_model function)?  Why is the training loss noisier (hint: again, look at the train_model function)?  What do the shapes of the curves say about how the model training is going (i.e., is everything going great, are we overfitting?)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-35nSvVIbiT",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umz1sOrpIduk",
        "colab_type": "text"
      },
      "source": [
        "The training loss is evaluated after each mini batch so it is sampled more densely.\n",
        "\n",
        "The training loss is evaluated only on the elements of the mini-batch (32 images), therefore it is much more subject to the random variation in the datta than the validation loss that is computed over the entire validation set.\n",
        "\n",
        "These curves show that overfitting is occurring since the trianing loss is going down while the validation loss stays constant (or maybe goes up a bit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41PUNs48Ifgt",
        "colab_type": "text"
      },
      "source": [
        "### Showing Model Predictions\n",
        "\n",
        "For more subjective santiy-checking, we can plot some images and see what our network predicts vs the ground truth label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDL5SibNzUuV",
        "colab_type": "code",
        "outputId": "77f6816a-d076-4298-fb76-074fad0cd293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "def examine_label(idx):\n",
        "    image, label = test_set[idx]\n",
        "    class_scores = net(Variable(image.unsqueeze(0)).to(device))\n",
        "    prediction = np.argmax(class_scores.cpu().detach().numpy())\n",
        "    disp_image(image, label, prediction)\n",
        "\n",
        "examine_label(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHLFJREFUeJztnXmwZVdVxr915/vmMd3ppIekE0MG\nkk6gDWgkKGhQgiDlFAcSoRC1SiyqtBS1FBQcqhTLKsdSBCUOaNKEoRxATYwxIkmATuh0OnS6Oz3P\n7/Wb77j945ymLu3+FunYfdNhf7+qrnrvrLvP2Wf4zr5vf73WthAChBBf/xRe6A4IIfqDxC5EIkjs\nQiSCxC5EIkjsQiSCxC5EIkjsXwMze7WZ7T+Lzwczu+J89unriTOvr5ltM7NX9+G4Hzaz9z3Hz95l\nZg+d7z6dby54sZvZA2Y2Y2bV5/j5DbngSue7b6mQX89FM1swswNm9gEzK56PY4UQrg0hPPAc+6SX\n6llwQYvdzDYA+BYAAcB3v6Cd6TMX4MvqhhDCEIDXAPghAG8/8wMXYJ8vOF7Ia3RBix3AWwB8FsCH\nAdzZGzCzupn9rpk9a2anzOwhM6sDeDD/yGw+Er3SzN5jZnf3tP2q0d/MfszMtpvZvJntMrN3/D/7\n/Voz+7KZzZrZH5qZ5ccpmNkv530+amZ/ZWajZ/TpbWa2F8C/m1nNzO42sxP5vh4xs1X550fN7INm\ndigfbd93vkbbXkIITwH4TwDX5f3YY2Y/b2aPA1g0s5KZrTGze83smJntNrN3nm6f37cP59/WngSw\nuXf/+f5em/9cNLNfNLNn8nvzmJmtNbPT93hrfo9/IP/87Wb2xfxaPWxm1/fs90Yz+3y+n48CqJ3t\nuZvZ7+T93m1m39mzfY2ZfcLMTprZTjN7e0/sPWZ2T34f5wDcZWbfaGaPmtmcmR0xsw/0fP4Ved9n\nzWzrOf2TJoRwwf4DsBPATwF4GYAWgFU9sT8E8ACASwAUAXwTgCqADci+CZR6PvseAHf3/P5VnwHw\negAbARiAWwEsAbgpj70awP6etn8E4I+cPgcAnwIwBmAdgGMAXpfH3pqf0+UAhgBsAfCRM/r0VwAG\nAdQBvAPAJwEM5Of4MgAj+ec/BuBP889eBOBzAN5xnu5DAHBF/vM1AA4DeFv++x4AXwSwNu9zAcBj\nAH4FQCU/110Abss//1vIXhYTeZsvnXF99wB4bf7zzwF4AsBV+b25AcDkmX3Kf78RwFEAN+fX6s58\nX9W8H88CeBeAMoDvzZ+n9/W0nwVwCzn/u/LPvz3f908COAjA8viD+XNRA7Apv+ff1vPstQC8Kb82\ndQD/DeBH8/gQgFfkP18C4ASA78o/++3579Pn5D6+0IJ2HrBb8os0lf/+FIB35T8XACwj+2p5ZrvT\nonnOYo/s4z4APxMT+3MUxi09v/89gF/If/43AD/VE7sqP8dST58u74m/FcDDAK4/4xirADQA1Hu2\n3QHg/vN0LwKAOQAzAJ4B8D4AhTy2B8Bbez57M4C9Z7R/N4AP5T/vQv7yy3//cXCx7wDwRqdPvWL/\nYwC/fsZndiB7eb8KPeLMYw+jR+xf4/zvArCz5/eB/Pirkb2wOgCGe+K/CeDDPc/eg2fs70EA7z39\nbPds/3nkL/+ebf8C4M5zcR8v5L+x7gTw6RDC8fz3v8m3/R6AKWRv0WfOxYHyr2S/CuAbkL1IBpCN\nKM+Xwz0/LyF7ewPAGmQjzGmeRSb0VT3b9vX8/BFkD9PfmdkYgLsB/BKA9chGqEP5XwjI+93b9lxz\nUwhhJ4n1Hnc9gDVmNtuzrYhsNAeya9D7+d7rcSZr8dzv8XoAd5rZT/dsq+THCwAOhFw9z+G4Mb5y\nT0MIS/l1HwIwCeBkCGH+jH2/vOf3M+/L2wD8GoCnzGw3gPeGED6Vn8P3mdkbej5bBnD/WfY1ygUp\n9vxv7+8HUDSz0xe5CmDMzG5AJsQVZF+9t57RPJbGt4hMwKdZ3XOsKoB7kc0PfDyE0DKz+5B9bTzX\nHER2Q0+zDkAbwBEAl+bbvtL/EEIL2Qjw3nyy8h+RjVb/iGxknwohtM9DP8+W3mu+D8DuEMKV5LOH\nkIl4W/77Ome/+5Dd4y89hz7sA/D+EML7zwyY2a0ALjEz6xH8OpybweIggAkzG+4R/DoAB3o+81XP\nZAjhywDuMLMCgDcDuMfMJvNz+EgI4f9Mfp4LLtQJujch+2p0DbK/gTYBuBrZ6PCWEEIXwF8A+EA+\nOVLMJ+KqyP5e6iL7W/E0XwTwKjNbl0+IvbsnVkH2IjkGoJ2P8t9xns7rbwG8y8wuM7MhAL8B4KNM\nsGb2rWb20nzibQ7ZV/5uCOEQgE8D+F0zG8kn/jbmD/ULzecAzOeTdvX83lxnZqcn4v4ewLvNbNzM\nLgXw03xX+HMAv25mV1rG9bkogOwF2XuP/wzAT5jZzflnB83s9WY2jOxv5DaAd5pZ2czeDOAbz8XJ\nhhD2IfuT4Dctm1C9HtnIfTdrY2Y/YmbT+XN8+htQN2/zBjO7Lb9uNcv+H8KlbF9nw4Uq9juR/Y23\nN4Rw+PQ/AH8A4Ictm0X/WWQj/CMATgL4bWR/Ry4BeD+A/8pnNF8RQvgMgI8CeBzZ5NGnTh8ofxu/\nE9lDOIPMVvoE65iZ/YmZ/cnzPK+/QPbV/EEAu5F9O/Ee9tUA7kEm9O0A/iNvD2TfRCoAnsz7fQ+A\ni59nv84ZIYQOgNuRvaB3AziOTLSj+Ufei+xr7m5kL6yPRHZzmg8guy+fRnYNPohsggvI/hb+y/we\nf38I4VFkE2h/gOx67ET2tzZCCE1kI+hdyJ6VH0A2OfoV8ln9b3l+Z407kM25HEQ2cfqrIYR/dT7/\nOgDbzGwBwO8D+MEQwnL+4ngjgF9ENvjsQzZJeU50aiHEvvUKIb7euFBHdiHEOUZiFyIRJHYhEkFi\nFyIR+uqz33jTjXQ2cHxykoUwOT0V3V4b5f+9uVDu0NjE6DiNjQ1dRGO333Z7dPtLrnkJbeNNf1qX\nR+cX5mlsYWmJH6/bjbdZXHT2x2OLy/xY884+Z2Zno9unxydomw2rL6GxTov/d4JCmacEdMj1aDSb\ntE2lUqGxUpEfq1zicnr00Udo7EMf+mB0+8I8fwbazvV4/PFt0f8jopFdiESQ2IVIBIldiESQ2IVI\nBIldiESQ2IVIhL5ab7t28ozCwi4eK5bi76ShqaHodgCYWjNKY1e/5BoaWzW+hsb2HzwQ3X7wxFHa\nZs6x0DqtFo3NzJ2isUXHemPW0MryCm2zvLxMY13HPLSCM1YU4hnCg1Vul65evYrGKqUyjRUrPMb6\n6OUvFxx7reCcs2e9bd36RRp79tl4av2SY4kWn0cFMo3sQiSCxC5EIkjsQiSCxC5EIkjsQiSCxC5E\nIvTVehtbM0hjFXAroU0ylNZfOk3bbLx2Le/H6ACNdVa4VbbtiXh16erYMG2zsMJtMgvcADJiXQE8\nsw0AJsbiGX2v3HwzbTNc49ej6mSA1QZ4u6d3fjm6/dChQ7TNyAi3SyuOrRWca1UkVplXji042Yie\nZ+fZcu02z1JjffH25+dTxtHILkQiSOxCJILELkQiSOxCJILELkQi9HU2fuMmXmOs5iyXvXwqnqgx\nuYrP3jYrfBb82HKDxoYG+Az/m173vdHtxUE+K/0P922hsePHjtPYopNAszjHYzddvym6/drb30jb\njAzwhKLgJcIU+VjxsXvvjW4/cvwYP9Z3v4nGuh3uQARnitxI7Hkv5OdNgjsz/J0Or4nIYisrPHmJ\nuQweGtmFSASJXYhEkNiFSASJXYhEkNiFSASJXYhE6Kv1Vqvzw9WLPOGi2Y3Xautytw5W5nXJFud5\n7bdlZ1mdifF4kslhx0J7ett2Gms5yRHtFW4Pdpyli1ZNxpfKqpb59W11nKWVHItnbia+xBMAbH/y\nyej24RGeNLTsLDXl2Zuhwy2vdiN+bgXHQ6s4Ne26BZ6wZc7YacZjBVJPrlTg/ajX+f2kxznrFkKI\nFyUSuxCJILELkQgSuxCJILELkQgSuxCJ0FfrzVrctqjV6zTWHI6/k6an+FJNN29+JY09tY0vNYVl\nx8YhttxApUrbvHzTjTRmJWeZIcdOKjvLE23evDm6vVjmt7rj2I1erTYv8+r273p9dHvJsUTZklFZ\njB9rx1NP0dhTX47f6+uu/gba5uqrrqIxL+nNS0RrNPgSW50Qz+irOfX/Kk6MoZFdiESQ2IVIBIld\niESQ2IVIBIldiESQ2IVIhP5mvZXiWWMAMDF6MY2Nk4ytVZOX0jblzkU0tnaan/bJI3tpjNkn6zds\noG3e8sM/QmNdr+qhU2CxXOT9r9XiqYBdp+Chh1cosehYgLfeemt0+4JTRPGxL2ylsUMHD9PYvFOc\nc6ERjy01TtE2e/ftobGpiXhWIeBbZYcPH6Cxai1uR4YWf0AKnk3J2px1CyHEixKJXYhEkNiFSASJ\nXYhEkNiFSASJXYhE6Kv1dmqGF3q8ZBXPehsajFt21uXWz/ISzzIKXZ7l1e3wQo/NZjxWLvHLWCo7\n66g5KVRdpxilVwSSxryDmWfx8GN5GXFG9rlvH7c2P37ffTS2f/8hGpsYn6CxK69cG90+UuUFLKdH\n+P4OPLOLxhpt/nwXnGvMClwGx9oMXW7N0j6cdQshxIsSiV2IRJDYhUgEiV2IRJDYhUiEvs7Gz8wc\npLFjx3gdtxML8SSIppNUcdnll9DYIEkWAYCVJp/Fbzbjs63eRLc3a+rN0Pr75MFA6pl5M+de7beC\ns9zR4uIijT383w9Ht2/Z8g+0zTM7+Ux3uczdmrnjJ2msgrir8U1OjcIrr7yGxo4c4K6AN3Z6SznV\n6oPxNs5sfGuFP6cMjexCJILELkQiSOxCJILELkQiSOxCJILELkQi9NV6u/aaeFICAIxOcDtsvh23\nGazGk0WsNkdjy21es6zZ5u+/BrHlHAfNtd5Wlrl12HZq0FWr3KYskVgA76QRuw4Ann56O4198hOf\norH7778/un3mJK8lVyxxe2rFSWzqOuX1Tp0ai25vB36f/2bLFhqbnz1OY8O1uIUGAFu3bqOxkaF4\nH4tFfs/mneeDoZFdiESQ2IVIBIldiESQ2IVIBIldiESQ2IVIhL5ab2OlYRqrlrltcXwlbpWVeRkx\nNJ06c2hw26Ld4dlhXcTtn8bKEm3z0INxCwoAnnziSzTWcpZdmhwfobH169ZHt49O8OWwDu7ndeE+\n88//QmNPP/0Mja2sxOv1FZ2MvWaTW5GFErdmX775JhoD4pmKJ44dpS0++fFP0thtr/8OGjt46BiN\nPbP7WRobGY0/+wMD3GJdXlTWmxCCILELkQgSuxCJILELkQgSuxCJILELkQh9td6mR1bT2Op1vEDk\n9gd2RLeH6gJt01zkNkjFKaI44CzX1OrGLbYHSIYXANzz0b+msZkTvFBic5Fn5g1VeDZUmSzXtLjE\nl7WaWeAxL7lqaIhbqfVSvB/NDr/2MB5be/llNPaDd9xBYw899EB0+6OPPULbHDvGn53Dh3jByR3b\nn6Yxrzhnqx23HKtVrhdvWS7a5qxbCCFelEjsQiSCxC5EIkjsQiSCxC5EIkjsQiRCX623RuCWUdPJ\nNgMpDtha5JltnTI/VqMcz4QCAHMsjWf37Ylu/9Jnd9M2M8d5gULPjmk4WU2dIvfDRmrxTKnFU6do\nm/k5fqxanacWDlYnaIwVxTx5it8zr4Tia1/zGhq7fONGGrtnyz3R7YcP8fsyMsIzMB97hFt2Mydn\naazZbDqx+JmvLHNLtNng+2NoZBciESR2IRJBYhciESR2IRJBYhciESR2IRKhr9bb3qMHaaxZ5TZD\noRS35Tq8PiGc2oUoOEUPSyRbCwB27Y5bbDuefJK2mXWst2aH21AtYscAQKPDbZdatR7dHircTmq3\nefZg1yncWa042YP1SnT73Dy/vmsvvpjGbvnmW3g/arwY5aZNL4tub7+UF/TcsuVjNLZ7Fy+yWanw\nfgRnPb3R0dF4G2csbjS4Xhga2YVIBIldiESQ2IVIBIldiESQ2IVIhL7Oxg/EJ2gBALNH9tPYBGk4\nOcbr1lmJz7Z2wGcyy2W+5M7eZ+NL+Owj2wGgBGc2e4gnmbSWeZKMBX5ujWb83BaXuT1B8owAAM0W\nn/mfneOJH5VqfKa+ThJkAODKjVfQ2OQkT7ppNXli06te9ero9sef2ErbHD7EXaNO20nkcZ65SpU/\n/DXiJnTbfH9tJ8bQyC5EIkjsQiSCxC5EIkjsQiSCxC5EIkjsQiRCX623qWluP7RaPDmlHeI2TqXK\nrSur8P0trPB6bEsNbuMcOXA0un15hddwG6rxZJFykccWnMSJrpPIMzsfXzZqyVn+qdXhNk7XnOu4\nFF8OCwCm2uPx/XX5eQ2ThBDAX4aqtcLvWYvUftu/n1u9S8v8flYq/Bnugl+rconf66NHD0e3Dzj1\n/8rlMo0xNLILkQgSuxCJILELkQgSuxCJILELkQgSuxCJ0FfrrVl3lqyp8fdOEczu4JZLx7GnCjZM\nY4NVXqut0ZwjO+S2ijnv0w7vPqoDIzRWcmq/Lc3Hs+XmG9wmazS49VaqcIun4ixPdPJUvK5d27kv\nnSK/VifmuF36Pw99lsY+/4XPR7fv3beXtgmB99GM93F4gD874+NxKxIAniQ1DINjUxaLst6EEASJ\nXYhEkNiFSASJXYhEkNiFSASJXYhE6Kv1hiq3NLzlcdqkyF+nwN9VocNP7eQxbkPt2cmLRx48Es96\nqw3w7KTQ5vbU9KrVNPa67/k+Glu/YT2NHT50ILp92xNP0DYnT87Q2L4DPDts3574clgAMEOy7EpO\n4cXPff4LNLbrUDwzDAB2bH+axo4dORLd3nSWTzIzGus4S3bVS/zcSoHvE8SObJKMPQAwc3xbgkZ2\nIRJBYhciESR2IRJBYhciESR2IRJBYhciEfpqvZWdV0vByRwLpFhfx+q0zfISX9sMRW671Id4BhhN\neHJsldogt2OOzsWtPAD4z//hmVyHHatsw7q4Lfdtt72BtqmW+bX/t3//DI0dPXqMxlbIGnFeRtny\nXm7zPb59B42VS/wxZgUnO06RTSs4Npljyx07cZzG5uZJxiSA1RfHLdiuc62aZE0/D43sQiSCxC5E\nIkjsQiSCxC5EIkjsQiRCX2fju86rpe0kGLC6XxZ4okClzBNr1q6borGR4SEaO7g7vrRSc86Z+ee7\nQ6scrxcHAP/64L009k+f4edWq8QPODV2EW2zapon5MwvxM8ZALpdfs86ZBa80XWcC+OuwFCNOy9L\nyzyx6dSpeO06bwa/4CRYFeu8j4ValcbWXX45jV00Hb83y87yWouL/L4wNLILkQgSuxCJILELkQgS\nuxCJILELkQgSuxCJ0GfrjdsWHe4moUiSD4rgjbpdnujQcWKtFV7bi8W6Tv28iWm+7M8VN3I75gZn\nuaYTJ3gizMkTcUvmyCGetPLEDl5LLoDfs+nhVTRWqtai26t1Xq+v5divS0vcpuw4FmCR2GidJr/P\nDVLzEPCXf7pu0yYaW79uLY0NkWWjykV+7dtt1aATQhAkdiESQWIXIhEkdiESQWIXIhEkdiESoa/W\nW70ct2MAoGvcDmMZSiUnS6pZ5NZEwbhVNgfej3Yrbsk4jiLWblhDY1PTPCVuosAtqtVruJ03c2oh\nuv2SDdwmW5jhNt/Jk7x22vxB5xqjHN0+NsrP6+hhXpNvfnaWxkbHxmhsfGwiun1xjmeNFZ2llUaG\nR2hseppnU5ZL8esB8OWmCo71VnYy8xga2YVIBIldiESQ2IVIBIldiESQ2IVIBIldiEToq/VW8Yr8\nFZ0YyTQKjk1mjr1WML6sTossWwQAIcQtmXqdW4pDI7xQYijw7KrQ5X2Es3QRu4qjA9zyGq7x/k9M\ncatpz8phGuu243bS6il+nyecLLoDR/gyWidmeEbcAMkoYzYqALTa/Pq+9IaX0tj69fGlt77W8Srl\nuC3XdbLv2k6MoZFdiESQ2IVIBIldiESQ2IVIBIldiESQ2IVIhL5ab8GpKhmcQn4kKQid7vMrOAnw\n9cYaTsHJSiVukUyM86yrcpVf4mbHKXq40qAxC/xa1arx9cYKxDYEgHbXyV4r8/4PDnM7bGgwbtld\ntI5bgF2ngOjwhriFBgB79x2nsZkj8Yy+5eOOXees93ft9dfT2JiTfbey7KwHGIjN6ljVXefZZ2hk\nFyIRJHYhEkFiFyIRJHYhEkFiFyIR+jobX6zy2duVpWUao3PFbBYTQHBmrNsdJ8kkOIka4/EaYyOj\no7TNgnNebeMztIUCdwxKzpJMIEshmfdeD547wV2BepXPkC8txdvNNngCR7nuODJObO0VPFlndDSe\niDQ/y2vrveTKq2ns0nXraKzTdpKoHCeqQ5Jaio4T0vUSpQga2YVIBIldiESQ2IVIBIldiESQ2IVI\nBIldiEToq/XWbHr13Xi7QJa68d5UpSK3+bpdbjVVyryP5dJMdHuzxZNuZmZ5wkWNrwiEco2fXddZ\n+qdYJLHO83uvdx0rsgNuozVZIpLxRBhviaSyk/hRKPL7Wb4o3v8bXn4NbfPKzd9KY9U6rym4OO8k\nFDlLOXWJLVdwljfz6hfSPpx1CyHEixKJXYhEkNiFSASJXYhEkNiFSASJXYhE6Kv11lnmtlax4Ngu\npbiNFhy/rsVdECzOcdvCqxU2OBa3QgaH+PJJo5O8ntnACLfsQpH30VsqCxa/jl4WYMG5WEMFfm6z\nJ3lG3ORkPBNweMyx3srcQis645KBPweFUjx22ebraJuNV1xFYw1nGSc49lrLeVbbxFYsFrkmgpOp\nyNDILkQiSOxCJILELkQiSOxCJILELkQiSOxCJEJfrTdzll3ybDTWrgNuXbWNWyRW4+2mLuVFFNdc\nFo8VHcvFiPUDAOUa72PoOilxxF4DgDYpROgV2TQnQ7BIbE8A2Hj1ehorWNzOa3R5AU5vqalalVuA\nZceKLBF3cHpyFW3TXHbstYJzrHJ86S0AKJa47cyWMesUPE2c/TitkV2IRJDYhUgEiV2IRJDYhUgE\niV2IRJDYhUiEvlpvJcdaMWf9snYnboU0A7czusbttRKvGYjyAO9HIcQtEs82RIHvr+Nla519UlPW\njhyvZI6N49hJbeOFHp1V+IAQj3mjS7vD71nLycxja6UBQLkwHN0+NT5G26zMc3uw0XTWbKMRwBy7\ntEJ04T5XzrPP0MguRCJI7EIkgsQuRCJI7EIkgsQuRCJI7EIkwgWU9eZZPPF3kucKOU4Tus6xPPun\nSrLDBmq8iCJK3HpbafDilhZ4PwqOL9chdk2n5RT7dIpRVnj33Uw6VhDRK3zpPQLtBu9/peys69eJ\np73t2b2NthmsTtBYtc6zIocqcZsPAOoVbr11uvFrsuxk37H77KGRXYhEkNiFSASJXYhEkNiFSASJ\nXYhE6O/yT85Md8F4V7rdeLtu20sU4KFiic+MekkhbIa/7My4d8GnmMvOrLo3aw1zZuPb8dnnbof3\nI3SdWnhOu6qzPFHB4jXjlrvcgSjXnPvi5Ro51wPEAVpcPEJbtFZmaay8wmf+B+sjNFY0nn1VrY9H\nt4+NOvsLPJGHoZFdiESQ2IVIBIldiESQ2IVIBIldiESQ2IVIhL5ab13nP+9Xyp7tErdPigVuC5nj\nvTU7PKmi1SXrBQEoluPvRq9NlyzHBACdFrcizbyafLweW7sZjxUdS7Ho1Efr4OxrnQEAWdEIBacf\nTu4PKs4SVQUnwapL6tOVvXqITvJPs83r04V5bmGWinxpqKWV49Ht7TbvSLnIE3KAN0e3amQXIhEk\ndiESQWIXIhEkdiESQWIXIhEkdiESwdwlZoQQXzdoZBciESR2IRJBYhciESR2IRJBYhciESR2IRJB\nYhciESR2IRJBYhciESR2IRJBYhciESR2IRJBYhciESR2IRJBYhciESR2IRJBYhciESR2IRJBYhci\nESR2IRJBYhciESR2IRJBYhciEf4XCNj+vWoN8ZIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOcWaPLpAWYB",
        "colab_type": "text"
      },
      "source": [
        "## Computing Accuracy\n",
        "\n",
        "The plots of the losses over time give us a sense of how the network is learning.  The losses themselves, however, may not provide a super accurate idea of how the network is actually doing at its task (predicting the correct labels).  We'll do it for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9gR_izeApfA",
        "colab_type": "code",
        "outputId": "335cc3ab-8b14-40d9-89d2-ea5b71563c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_correct = 0\n",
        "n_total = 0\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    # Get inputs in right form\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "    n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "    n_total += labels.shape[0]\n",
        "print(\"Training accuracy is\", n_correct/n_total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy is 0.60415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UCwZ3mB5eo",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Adapt this code so that it computes the accuracy on the test set.  Hint: consider factoring this out into a function to make it more flexible.  If you create a function, you'll want to pass in `net` and `loader` as inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR7HRd3vCFEo",
        "colab_type": "text"
      },
      "source": [
        "#### ***Solution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyw-OfHCBjV9",
        "colab_type": "code",
        "outputId": "b1da73e3-ee6e-47fd-c258-16b40186389c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_accuracy(net, loader):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
        "        n_total += labels.shape[0]\n",
        "    return n_correct/n_total\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.60415\n",
            "Test accuracy is 0.4392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz_rgc__DKpE",
        "colab_type": "text"
      },
      "source": [
        "## Model Visualization\n",
        "\n",
        "There are some really cool visualizations you can create based on your neural network (e.g., [here](https://mc.ai/feature-visualisation-in-pytorch%E2%80%8A-%E2%80%8Asaliency-maps/)).  You saw some of them in the assignment when you looked at the convnet.js examples.  We won't do anything nearly that fancy.  For now, let's just look at convolutional kernels learned by our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIZYhXuU54sB",
        "colab_type": "code",
        "outputId": "027f525e-d248-4654-a71e-eaa6c4db4f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.subplots(4, 4)\n",
        "for i in range(net.conv1.weight.shape[0]):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    kernel = net.conv1.weight[i].cpu().detach().numpy()\n",
        "    im = kernel.mean(axis=0)\n",
        "    plt.pcolor(im, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3GhHSCS71_E",
        "colab_type": "text"
      },
      "source": [
        "There's not really all that much to see here.  These look like some edge filters.  Maybe if we use a 5 by 5 kernel they will be more interesting?\n",
        "\n",
        "Nick Steelman made some fancy visualizations for this network along the lines of the gradient activations we saw in the convnet.js example.  For this super small network, they're not really all that interpretable, but here they are in case you are interested.\n",
        "\n",
        "Please don't get hung up on trying to interpret these.  We will be giving more guidance on visualizing network features for the project.\n",
        "\n",
        "Explanation of images from left to right:\n",
        "\n",
        "1. Gradient of the network towards ground truth label\n",
        "2. Gradient of the network towards ground truth label overlayed with image\n",
        "3. Original image\n",
        "4. Gradient of the network towards predicted label overlayed with image\n",
        "5. Gradient of the network towards predicted label\n",
        "6. Precentage difference between the true and predicted gradients\n",
        "7. What pixels the network should pay more attention to fix this error\n",
        "8. What pixels the network should pay less attention to fix this error\n",
        "\n",
        "![](https://raw.githubusercontent.com/mlfa19/assignments/master/Module%201/08/conv_net_viz.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEJOv9NpDHEc",
        "colab_type": "text"
      },
      "source": [
        "## Model Iteration\n",
        "\n",
        "### Exercise\n",
        "\n",
        "At this point, try to change your model and see what happens (not necessarily to improve its performance). Here are some possibilities.\n",
        "\n",
        "* Change the size of some layer to either increase or decrease model complexity.\n",
        "* Change the activation function in the network to sigmoid or some other function.\n",
        "* Change the batch size.\n",
        "* Increase or decrease the number of epochs.\n",
        "\n",
        "***Tip: when making these changes, consider modifying your neural network class to allow for whatever you are changing to be customized, rather than just hardcoding new values*** For example, if I wanted to try changing the number of convolutional kernels, I might modify my `__init__` function to take this number as a new input (see solution below for details of this example).\n",
        "\n",
        "Run your new model, compare the performance with the default network, and visualize the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7W3Yj0pERHg",
        "colab_type": "text"
      },
      "source": [
        "#### ***Solution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qiSi6g8EGJN",
        "colab_type": "code",
        "outputId": "9ecc7970-d023-4dd6-8bc9-120e55bea158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "# ***Solution***\n",
        "class MyCNN(nn.Module):\n",
        "    # The init funciton in Pytorch classes is used to keep track of the parameters of the model\n",
        "    # specifically the ones we want to update with gradient descent + backprop\n",
        "    # So we need to make sure we keep track of all of them here\n",
        "    def __init__(self, num_kernels):\n",
        "        super(MyCNN, self).__init__()\n",
        "        # layers defined here\n",
        "\n",
        "        # Make sure you understand what this convolutional layer is doing.\n",
        "        # E.g., considering looking at help(nn.Conv2D).  Draw a picture of what\n",
        "        # this layer does to the data.\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], num_kernels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Make sure you understand what this MaxPool2D layer is doing.\n",
        "        # E.g., considering looking at help(nn.MaxPool2D).  Draw a picture of\n",
        "        # what this layer does to the data.\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # maxpool_output_size is the total amount of data coming out of that\n",
        "        # layer.  Explain why the line of code below computes this quantity.\n",
        "        self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
        "\n",
        "        # Add on a fully connected layer (like in our MLP)\n",
        "        # fc stands for fully connected\n",
        "        fc1_size = 64\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
        "\n",
        "        # we'll use this activation function internally in the network\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "        # Convert our fully connected layer into outputs that we can compare to the result\n",
        "        fc2_size = len(classes)\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
        "\n",
        "        # Note: that the output will not represent the probability of the\n",
        "        # output being in each class.  The loss function we will use\n",
        "        # `CrossEntropyLoss` will take care of convering these values to\n",
        "        # probabilities and then computing the log loss with respect to the\n",
        "        # true label.  We could break this out into multiple steps, but it turns\n",
        "        # out that the algorithm will be more numerically stable if we do it in\n",
        "        # one go.  We have included a cell to show you the documentation for\n",
        "        # `CrossEntropyLoss` if you'd like to check it out.\n",
        "        \n",
        "    # The forward function in the class defines the operations performed on a given input to the model\n",
        "    # and returns the output of the model\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.activation_func(x)\n",
        "        # this code flattens the output of the convolution, max pool,\n",
        "        # activation sequence of steps into a vector\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
        "    # returns the loss and optimizer used by the model\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer\n",
        "\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = MyCNN(128)\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "# visualize the model\n",
        "visualize_network(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f0fdd9f3940>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"267pt\" height=\"493pt\"\n viewBox=\"0.00 0.00 266.50 493.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 489)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-489 262.5,-489 262.5,4 -4,4\"/>\n<!-- 139704040614432 -->\n<g id=\"node1\" class=\"node\">\n<title>139704040614432</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"171,-21 67,-21 67,0 171,0 171,-21\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139704040613816 -->\n<g id=\"node2\" class=\"node\">\n<title>139704040613816</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-91 0,-91 0,-57 54,-57 54,-91\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 139704040613816&#45;&gt;139704040614432 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139704040613816&#45;&gt;139704040614432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.6543,-56.9832C65.1894,-47.641 81.8926,-36.1122 95.278,-26.8734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.2865,-29.7398 103.5283,-21.1788 93.3102,-23.9788 97.2865,-29.7398\"/>\n</g>\n<!-- 139704040613704 -->\n<g id=\"node3\" class=\"node\">\n<title>139704040613704</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"166,-84.5 72,-84.5 72,-63.5 166,-63.5 166,-84.5\"/>\n<text text-anchor=\"middle\" x=\"119\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139704040613704&#45;&gt;139704040614432 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139704040613704&#45;&gt;139704040614432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119,-63.2281C119,-54.5091 119,-41.9699 119,-31.3068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.5001,-31.1128 119,-21.1128 115.5001,-31.1129 122.5001,-31.1128\"/>\n</g>\n<!-- 139704040612752 -->\n<g id=\"node4\" class=\"node\">\n<title>139704040612752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170,-154.5 66,-154.5 66,-133.5 170,-133.5 170,-154.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-140.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 139704040612752&#45;&gt;139704040613704 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139704040612752&#45;&gt;139704040613704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118.1519,-133.3685C118.2972,-123.1925 118.5206,-107.5606 118.7016,-94.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.2034,-94.7806 118.8467,-84.7315 115.2041,-94.6805 122.2034,-94.7806\"/>\n</g>\n<!-- 139704040615104 -->\n<g id=\"node5\" class=\"node\">\n<title>139704040615104</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-231 0,-231 0,-197 54,-197 54,-231\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-217.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64)</text>\n</g>\n<!-- 139704040615104&#45;&gt;139704040612752 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139704040615104&#45;&gt;139704040612752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49.4944,-196.6966C63.7034,-185.7666 81.9745,-171.7119 96.0735,-160.8666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.447,-163.4565 104.2393,-154.5852 94.179,-157.9081 98.447,-163.4565\"/>\n</g>\n<!-- 139704040612976 -->\n<g id=\"node6\" class=\"node\">\n<title>139704040612976</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-224.5 72.5,-224.5 72.5,-203.5 163.5,-203.5 163.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"118\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139704040612976&#45;&gt;139704040612752 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139704040612976&#45;&gt;139704040612752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M118,-203.3685C118,-193.1925 118,-177.5606 118,-164.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-164.7315 118,-154.7315 114.5001,-164.7316 121.5001,-164.7315\"/>\n</g>\n<!-- 139704040613088 -->\n<g id=\"node7\" class=\"node\">\n<title>139704040613088</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"164,-294.5 70,-294.5 70,-273.5 164,-273.5 164,-294.5\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-280.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139704040613088&#45;&gt;139704040612976 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139704040613088&#45;&gt;139704040612976</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117.1519,-273.3685C117.2972,-263.1925 117.5206,-247.5606 117.7016,-234.8912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.2034,-234.7806 117.8467,-224.7315 114.2041,-234.6805 121.2034,-234.7806\"/>\n</g>\n<!-- 139704040613032 -->\n<g id=\"node8\" class=\"node\">\n<title>139704040613032</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"207,-358 27,-358 27,-337 207,-337 207,-358\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n</g>\n<!-- 139704040613032&#45;&gt;139704040613088 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139704040613032&#45;&gt;139704040613088</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117,-336.7281C117,-328.0091 117,-315.4699 117,-304.8068\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.5001,-304.6128 117,-294.6128 113.5001,-304.6129 120.5001,-304.6128\"/>\n</g>\n<!-- 139704040612696 -->\n<g id=\"node9\" class=\"node\">\n<title>139704040612696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"195.5,-415 38.5,-415 38.5,-394 195.5,-394 195.5,-415\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n</g>\n<!-- 139704040612696&#45;&gt;139704040613032 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139704040612696&#45;&gt;139704040613032</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117,-393.7787C117,-386.6134 117,-376.9517 117,-368.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.5001,-368.1732 117,-358.1732 113.5001,-368.1732 120.5001,-368.1732\"/>\n</g>\n<!-- 139704040614544 -->\n<g id=\"node10\" class=\"node\">\n<title>139704040614544</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"111.5,-485 30.5,-485 30.5,-451 111.5,-451 111.5,-485\"/>\n<text text-anchor=\"middle\" x=\"71\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n<text text-anchor=\"middle\" x=\"71\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128, 3, 3, 3)</text>\n</g>\n<!-- 139704040614544&#45;&gt;139704040612696 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139704040614544&#45;&gt;139704040612696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M83.3272,-450.9832C89.4107,-442.5853 96.7742,-432.4204 103.0621,-423.7404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0945,-425.5204 109.1266,-415.3687 100.4256,-421.4138 106.0945,-425.5204\"/>\n</g>\n<!-- 139704040613200 -->\n<g id=\"node11\" class=\"node\">\n<title>139704040613200</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"198,-485 130,-485 130,-451 198,-451 198,-485\"/>\n<text text-anchor=\"middle\" x=\"164\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n<text text-anchor=\"middle\" x=\"164\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 139704040613200&#45;&gt;139704040612696 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139704040613200&#45;&gt;139704040612696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M151.4049,-450.9832C145.1237,-442.4969 137.5069,-432.2062 131.0384,-423.4668\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.8071,-421.3243 125.0445,-415.3687 128.1806,-425.4888 133.8071,-421.3243\"/>\n</g>\n<!-- 139704040615328 -->\n<g id=\"node12\" class=\"node\">\n<title>139704040615328</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"255.5,-224.5 182.5,-224.5 182.5,-203.5 255.5,-203.5 255.5,-224.5\"/>\n<text text-anchor=\"middle\" x=\"219\" y=\"-210.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139704040615328&#45;&gt;139704040612752 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139704040615328&#45;&gt;139704040612752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M203.6603,-203.3685C187.1024,-191.8927 160.533,-173.4783 141.3726,-160.1988\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.3659,-157.3219 133.1532,-154.5022 139.3784,-163.0752 143.3659,-157.3219\"/>\n</g>\n<!-- 139704040612472 -->\n<g id=\"node13\" class=\"node\">\n<title>139704040612472</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"257.5,-301 182.5,-301 182.5,-267 257.5,-267 257.5,-301\"/>\n<text text-anchor=\"middle\" x=\"220\" y=\"-287.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n<text text-anchor=\"middle\" x=\"220\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (64, 32768)</text>\n</g>\n<!-- 139704040612472&#45;&gt;139704040615328 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139704040612472&#45;&gt;139704040615328</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M219.7528,-266.6966C219.6152,-257.0634 219.4429,-245.003 219.2979,-234.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.7967,-234.7402 219.1542,-224.7913 215.7975,-234.8403 222.7967,-234.7402\"/>\n</g>\n<!-- 139704040614936 -->\n<g id=\"node14\" class=\"node\">\n<title>139704040614936</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.5,-84.5 185.5,-84.5 185.5,-63.5 258.5,-63.5 258.5,-84.5\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-70.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 139704040614936&#45;&gt;139704040614432 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139704040614936&#45;&gt;139704040614432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.5275,-63.2281C188.1519,-53.1325 163.4682,-37.9149 144.8209,-26.4187\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.5636,-23.3814 136.2145,-21.1128 142.8901,-29.3401 146.5636,-23.3814\"/>\n</g>\n<!-- 139704040614656 -->\n<g id=\"node15\" class=\"node\">\n<title>139704040614656</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"255.5,-161 188.5,-161 188.5,-127 255.5,-127 255.5,-161\"/>\n<text text-anchor=\"middle\" x=\"222\" y=\"-147.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n<text text-anchor=\"middle\" x=\"222\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 64)</text>\n</g>\n<!-- 139704040614656&#45;&gt;139704040614936 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139704040614656&#45;&gt;139704040614936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M222,-126.6966C222,-117.0634 222,-105.003 222,-94.8518\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.5001,-94.7912 222,-84.7913 218.5001,-94.7913 225.5001,-94.7912\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdvJxTd-Mlvc",
        "colab_type": "code",
        "outputId": "6daf023f-9518-4e67-de39-5a19e9b1ac66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ***Solution***\n",
        "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Iteration 20\t train_loss: 5.39 took: 0.31s\n",
            "Epoch 1, Iteration 40\t train_loss: 2.24 took: 0.21s\n",
            "Epoch 1, Iteration 60\t train_loss: 2.21 took: 0.20s\n",
            "Epoch 1, Iteration 80\t train_loss: 2.06 took: 0.19s\n",
            "Epoch 1, Iteration 100\t train_loss: 2.03 took: 0.19s\n",
            "Epoch 1, Iteration 120\t train_loss: 1.94 took: 0.19s\n",
            "Epoch 1, Iteration 140\t train_loss: 1.95 took: 0.19s\n",
            "Epoch 1, Iteration 160\t train_loss: 1.89 took: 0.20s\n",
            "Epoch 1, Iteration 180\t train_loss: 1.93 took: 0.19s\n",
            "Epoch 1, Iteration 200\t train_loss: 1.87 took: 0.21s\n",
            "Epoch 1, Iteration 220\t train_loss: 1.79 took: 0.19s\n",
            "Epoch 1, Iteration 240\t train_loss: 1.86 took: 0.20s\n",
            "Epoch 1, Iteration 260\t train_loss: 1.81 took: 0.18s\n",
            "Epoch 1, Iteration 280\t train_loss: 1.78 took: 0.19s\n",
            "Epoch 1, Iteration 300\t train_loss: 1.76 took: 0.19s\n",
            "Epoch 1, Iteration 320\t train_loss: 1.78 took: 0.20s\n",
            "Epoch 1, Iteration 340\t train_loss: 1.75 took: 0.20s\n",
            "Epoch 1, Iteration 360\t train_loss: 1.78 took: 0.19s\n",
            "Epoch 1, Iteration 380\t train_loss: 1.68 took: 0.19s\n",
            "Epoch 1, Iteration 400\t train_loss: 1.73 took: 0.18s\n",
            "Epoch 1, Iteration 420\t train_loss: 1.77 took: 0.20s\n",
            "Epoch 1, Iteration 440\t train_loss: 1.76 took: 0.19s\n",
            "Epoch 1, Iteration 460\t train_loss: 1.74 took: 0.20s\n",
            "Epoch 1, Iteration 480\t train_loss: 1.70 took: 0.18s\n",
            "Epoch 1, Iteration 500\t train_loss: 1.67 took: 0.19s\n",
            "Epoch 1, Iteration 520\t train_loss: 1.69 took: 0.19s\n",
            "Epoch 1, Iteration 540\t train_loss: 1.64 took: 0.21s\n",
            "Epoch 1, Iteration 560\t train_loss: 1.75 took: 0.21s\n",
            "Epoch 1, Iteration 580\t train_loss: 1.65 took: 0.21s\n",
            "Epoch 1, Iteration 600\t train_loss: 1.64 took: 0.21s\n",
            "Epoch 1, Iteration 620\t train_loss: 1.70 took: 0.20s\n",
            "Validation loss = 1.59\n",
            "Epoch 2, Iteration 20\t train_loss: 1.59 took: 0.25s\n",
            "Epoch 2, Iteration 40\t train_loss: 1.64 took: 0.18s\n",
            "Epoch 2, Iteration 60\t train_loss: 1.59 took: 0.18s\n",
            "Epoch 2, Iteration 80\t train_loss: 1.51 took: 0.18s\n",
            "Epoch 2, Iteration 100\t train_loss: 1.50 took: 0.20s\n",
            "Epoch 2, Iteration 120\t train_loss: 1.55 took: 0.18s\n",
            "Epoch 2, Iteration 140\t train_loss: 1.58 took: 0.20s\n",
            "Epoch 2, Iteration 160\t train_loss: 1.50 took: 0.19s\n",
            "Epoch 2, Iteration 180\t train_loss: 1.60 took: 0.19s\n",
            "Epoch 2, Iteration 200\t train_loss: 1.58 took: 0.19s\n",
            "Epoch 2, Iteration 220\t train_loss: 1.57 took: 0.20s\n",
            "Epoch 2, Iteration 240\t train_loss: 1.52 took: 0.19s\n",
            "Epoch 2, Iteration 260\t train_loss: 1.56 took: 0.21s\n",
            "Epoch 2, Iteration 280\t train_loss: 1.46 took: 0.19s\n",
            "Epoch 2, Iteration 300\t train_loss: 1.56 took: 0.21s\n",
            "Epoch 2, Iteration 320\t train_loss: 1.60 took: 0.21s\n",
            "Epoch 2, Iteration 340\t train_loss: 1.54 took: 0.19s\n",
            "Epoch 2, Iteration 360\t train_loss: 1.63 took: 0.19s\n",
            "Epoch 2, Iteration 380\t train_loss: 1.58 took: 0.19s\n",
            "Epoch 2, Iteration 400\t train_loss: 1.56 took: 0.19s\n",
            "Epoch 2, Iteration 420\t train_loss: 1.56 took: 0.19s\n",
            "Epoch 2, Iteration 440\t train_loss: 1.56 took: 0.19s\n",
            "Epoch 2, Iteration 460\t train_loss: 1.61 took: 0.19s\n",
            "Epoch 2, Iteration 480\t train_loss: 1.48 took: 0.19s\n",
            "Epoch 2, Iteration 500\t train_loss: 1.58 took: 0.19s\n",
            "Epoch 2, Iteration 520\t train_loss: 1.58 took: 0.20s\n",
            "Epoch 2, Iteration 540\t train_loss: 1.57 took: 0.20s\n",
            "Epoch 2, Iteration 560\t train_loss: 1.50 took: 0.19s\n",
            "Epoch 2, Iteration 580\t train_loss: 1.59 took: 0.19s\n",
            "Epoch 2, Iteration 600\t train_loss: 1.61 took: 0.19s\n",
            "Epoch 2, Iteration 620\t train_loss: 1.56 took: 0.20s\n",
            "Validation loss = 1.57\n",
            "Epoch 3, Iteration 20\t train_loss: 1.43 took: 0.27s\n",
            "Epoch 3, Iteration 40\t train_loss: 1.49 took: 0.18s\n",
            "Epoch 3, Iteration 60\t train_loss: 1.41 took: 0.19s\n",
            "Epoch 3, Iteration 80\t train_loss: 1.42 took: 0.18s\n",
            "Epoch 3, Iteration 100\t train_loss: 1.44 took: 0.19s\n",
            "Epoch 3, Iteration 120\t train_loss: 1.45 took: 0.19s\n",
            "Epoch 3, Iteration 140\t train_loss: 1.41 took: 0.19s\n",
            "Epoch 3, Iteration 160\t train_loss: 1.36 took: 0.19s\n",
            "Epoch 3, Iteration 180\t train_loss: 1.39 took: 0.19s\n",
            "Epoch 3, Iteration 200\t train_loss: 1.58 took: 0.19s\n",
            "Epoch 3, Iteration 220\t train_loss: 1.38 took: 0.20s\n",
            "Epoch 3, Iteration 240\t train_loss: 1.44 took: 0.19s\n",
            "Epoch 3, Iteration 260\t train_loss: 1.44 took: 0.19s\n",
            "Epoch 3, Iteration 280\t train_loss: 1.49 took: 0.18s\n",
            "Epoch 3, Iteration 300\t train_loss: 1.50 took: 0.21s\n",
            "Epoch 3, Iteration 320\t train_loss: 1.37 took: 0.25s\n",
            "Epoch 3, Iteration 340\t train_loss: 1.47 took: 0.21s\n",
            "Epoch 3, Iteration 360\t train_loss: 1.41 took: 0.22s\n",
            "Epoch 3, Iteration 380\t train_loss: 1.42 took: 0.21s\n",
            "Epoch 3, Iteration 400\t train_loss: 1.38 took: 0.19s\n",
            "Epoch 3, Iteration 420\t train_loss: 1.39 took: 0.21s\n",
            "Epoch 3, Iteration 440\t train_loss: 1.47 took: 0.23s\n",
            "Epoch 3, Iteration 460\t train_loss: 1.44 took: 0.21s\n",
            "Epoch 3, Iteration 480\t train_loss: 1.48 took: 0.21s\n",
            "Epoch 3, Iteration 500\t train_loss: 1.49 took: 0.22s\n",
            "Epoch 3, Iteration 520\t train_loss: 1.47 took: 0.21s\n",
            "Epoch 3, Iteration 540\t train_loss: 1.57 took: 0.19s\n",
            "Epoch 3, Iteration 560\t train_loss: 1.46 took: 0.20s\n",
            "Epoch 3, Iteration 580\t train_loss: 1.37 took: 0.19s\n",
            "Epoch 3, Iteration 600\t train_loss: 1.47 took: 0.19s\n",
            "Epoch 3, Iteration 620\t train_loss: 1.42 took: 0.20s\n",
            "Validation loss = 1.49\n",
            "Epoch 4, Iteration 20\t train_loss: 1.37 took: 0.29s\n",
            "Epoch 4, Iteration 40\t train_loss: 1.34 took: 0.21s\n",
            "Epoch 4, Iteration 60\t train_loss: 1.29 took: 0.18s\n",
            "Epoch 4, Iteration 80\t train_loss: 1.27 took: 0.19s\n",
            "Epoch 4, Iteration 100\t train_loss: 1.33 took: 0.20s\n",
            "Epoch 4, Iteration 120\t train_loss: 1.27 took: 0.19s\n",
            "Epoch 4, Iteration 140\t train_loss: 1.26 took: 0.19s\n",
            "Epoch 4, Iteration 160\t train_loss: 1.42 took: 0.21s\n",
            "Epoch 4, Iteration 180\t train_loss: 1.40 took: 0.21s\n",
            "Epoch 4, Iteration 200\t train_loss: 1.42 took: 0.23s\n",
            "Epoch 4, Iteration 220\t train_loss: 1.28 took: 0.22s\n",
            "Epoch 4, Iteration 240\t train_loss: 1.40 took: 0.21s\n",
            "Epoch 4, Iteration 260\t train_loss: 1.38 took: 0.19s\n",
            "Epoch 4, Iteration 280\t train_loss: 1.30 took: 0.20s\n",
            "Epoch 4, Iteration 300\t train_loss: 1.44 took: 0.20s\n",
            "Epoch 4, Iteration 320\t train_loss: 1.42 took: 0.18s\n",
            "Epoch 4, Iteration 340\t train_loss: 1.48 took: 0.20s\n",
            "Epoch 4, Iteration 360\t train_loss: 1.39 took: 0.19s\n",
            "Epoch 4, Iteration 380\t train_loss: 1.39 took: 0.19s\n",
            "Epoch 4, Iteration 400\t train_loss: 1.36 took: 0.19s\n",
            "Epoch 4, Iteration 420\t train_loss: 1.40 took: 0.19s\n",
            "Epoch 4, Iteration 440\t train_loss: 1.45 took: 0.20s\n",
            "Epoch 4, Iteration 460\t train_loss: 1.38 took: 0.20s\n",
            "Epoch 4, Iteration 480\t train_loss: 1.31 took: 0.22s\n",
            "Epoch 4, Iteration 500\t train_loss: 1.44 took: 0.20s\n",
            "Epoch 4, Iteration 520\t train_loss: 1.38 took: 0.20s\n",
            "Epoch 4, Iteration 540\t train_loss: 1.38 took: 0.20s\n",
            "Epoch 4, Iteration 560\t train_loss: 1.39 took: 0.18s\n",
            "Epoch 4, Iteration 580\t train_loss: 1.38 took: 0.21s\n",
            "Epoch 4, Iteration 600\t train_loss: 1.36 took: 0.22s\n",
            "Epoch 4, Iteration 620\t train_loss: 1.35 took: 0.19s\n",
            "Validation loss = 1.57\n",
            "Epoch 5, Iteration 20\t train_loss: 1.38 took: 0.26s\n",
            "Epoch 5, Iteration 40\t train_loss: 1.27 took: 0.18s\n",
            "Epoch 5, Iteration 60\t train_loss: 1.27 took: 0.19s\n",
            "Epoch 5, Iteration 80\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 5, Iteration 100\t train_loss: 1.18 took: 0.20s\n",
            "Epoch 5, Iteration 120\t train_loss: 1.27 took: 0.19s\n",
            "Epoch 5, Iteration 140\t train_loss: 1.22 took: 0.19s\n",
            "Epoch 5, Iteration 160\t train_loss: 1.31 took: 0.18s\n",
            "Epoch 5, Iteration 180\t train_loss: 1.34 took: 0.19s\n",
            "Epoch 5, Iteration 200\t train_loss: 1.20 took: 0.19s\n",
            "Epoch 5, Iteration 220\t train_loss: 1.25 took: 0.19s\n",
            "Epoch 5, Iteration 240\t train_loss: 1.23 took: 0.18s\n",
            "Epoch 5, Iteration 260\t train_loss: 1.28 took: 0.20s\n",
            "Epoch 5, Iteration 280\t train_loss: 1.32 took: 0.19s\n",
            "Epoch 5, Iteration 300\t train_loss: 1.28 took: 0.20s\n",
            "Epoch 5, Iteration 320\t train_loss: 1.23 took: 0.19s\n",
            "Epoch 5, Iteration 340\t train_loss: 1.29 took: 0.19s\n",
            "Epoch 5, Iteration 360\t train_loss: 1.24 took: 0.18s\n",
            "Epoch 5, Iteration 380\t train_loss: 1.28 took: 0.20s\n",
            "Epoch 5, Iteration 400\t train_loss: 1.31 took: 0.20s\n",
            "Epoch 5, Iteration 420\t train_loss: 1.30 took: 0.21s\n",
            "Epoch 5, Iteration 440\t train_loss: 1.18 took: 0.19s\n",
            "Epoch 5, Iteration 460\t train_loss: 1.29 took: 0.21s\n",
            "Epoch 5, Iteration 480\t train_loss: 1.28 took: 0.21s\n",
            "Epoch 5, Iteration 500\t train_loss: 1.21 took: 0.19s\n",
            "Epoch 5, Iteration 520\t train_loss: 1.24 took: 0.21s\n",
            "Epoch 5, Iteration 540\t train_loss: 1.30 took: 0.19s\n",
            "Epoch 5, Iteration 560\t train_loss: 1.34 took: 0.20s\n",
            "Epoch 5, Iteration 580\t train_loss: 1.26 took: 0.19s\n",
            "Epoch 5, Iteration 600\t train_loss: 1.27 took: 0.22s\n",
            "Epoch 5, Iteration 620\t train_loss: 1.35 took: 0.21s\n",
            "Validation loss = 1.48\n",
            "Epoch 6, Iteration 20\t train_loss: 1.10 took: 0.27s\n",
            "Epoch 6, Iteration 40\t train_loss: 1.15 took: 0.20s\n",
            "Epoch 6, Iteration 60\t train_loss: 1.14 took: 0.22s\n",
            "Epoch 6, Iteration 80\t train_loss: 1.12 took: 0.21s\n",
            "Epoch 6, Iteration 100\t train_loss: 1.17 took: 0.22s\n",
            "Epoch 6, Iteration 120\t train_loss: 1.14 took: 0.19s\n",
            "Epoch 6, Iteration 140\t train_loss: 1.18 took: 0.19s\n",
            "Epoch 6, Iteration 160\t train_loss: 1.20 took: 0.19s\n",
            "Epoch 6, Iteration 180\t train_loss: 1.21 took: 0.19s\n",
            "Epoch 6, Iteration 200\t train_loss: 1.14 took: 0.20s\n",
            "Epoch 6, Iteration 220\t train_loss: 1.08 took: 0.19s\n",
            "Epoch 6, Iteration 240\t train_loss: 1.10 took: 0.19s\n",
            "Epoch 6, Iteration 260\t train_loss: 1.30 took: 0.19s\n",
            "Epoch 6, Iteration 280\t train_loss: 1.26 took: 0.22s\n",
            "Epoch 6, Iteration 300\t train_loss: 1.29 took: 0.20s\n",
            "Epoch 6, Iteration 320\t train_loss: 1.26 took: 0.18s\n",
            "Epoch 6, Iteration 340\t train_loss: 1.29 took: 0.20s\n",
            "Epoch 6, Iteration 360\t train_loss: 1.24 took: 0.19s\n",
            "Epoch 6, Iteration 380\t train_loss: 1.17 took: 0.21s\n",
            "Epoch 6, Iteration 400\t train_loss: 1.27 took: 0.20s\n",
            "Epoch 6, Iteration 420\t train_loss: 1.31 took: 0.20s\n",
            "Epoch 6, Iteration 440\t train_loss: 1.14 took: 0.21s\n",
            "Epoch 6, Iteration 460\t train_loss: 1.28 took: 0.19s\n",
            "Epoch 6, Iteration 480\t train_loss: 1.26 took: 0.21s\n",
            "Epoch 6, Iteration 500\t train_loss: 1.23 took: 0.20s\n",
            "Epoch 6, Iteration 520\t train_loss: 1.17 took: 0.19s\n",
            "Epoch 6, Iteration 540\t train_loss: 1.29 took: 0.19s\n",
            "Epoch 6, Iteration 560\t train_loss: 1.31 took: 0.21s\n",
            "Epoch 6, Iteration 580\t train_loss: 1.30 took: 0.21s\n",
            "Epoch 6, Iteration 600\t train_loss: 1.23 took: 0.20s\n",
            "Epoch 6, Iteration 620\t train_loss: 1.19 took: 0.18s\n",
            "Validation loss = 1.46\n",
            "Epoch 7, Iteration 20\t train_loss: 1.01 took: 0.27s\n",
            "Epoch 7, Iteration 40\t train_loss: 1.08 took: 0.18s\n",
            "Epoch 7, Iteration 60\t train_loss: 1.12 took: 0.19s\n",
            "Epoch 7, Iteration 80\t train_loss: 1.05 took: 0.19s\n",
            "Epoch 7, Iteration 100\t train_loss: 1.11 took: 0.20s\n",
            "Epoch 7, Iteration 120\t train_loss: 1.19 took: 0.18s\n",
            "Epoch 7, Iteration 140\t train_loss: 1.02 took: 0.19s\n",
            "Epoch 7, Iteration 160\t train_loss: 1.15 took: 0.18s\n",
            "Epoch 7, Iteration 180\t train_loss: 1.15 took: 0.19s\n",
            "Epoch 7, Iteration 200\t train_loss: 1.07 took: 0.19s\n",
            "Epoch 7, Iteration 220\t train_loss: 1.15 took: 0.20s\n",
            "Epoch 7, Iteration 240\t train_loss: 1.08 took: 0.20s\n",
            "Epoch 7, Iteration 260\t train_loss: 1.06 took: 0.19s\n",
            "Epoch 7, Iteration 280\t train_loss: 1.03 took: 0.19s\n",
            "Epoch 7, Iteration 300\t train_loss: 1.07 took: 0.20s\n",
            "Epoch 7, Iteration 320\t train_loss: 1.09 took: 0.19s\n",
            "Epoch 7, Iteration 340\t train_loss: 1.11 took: 0.21s\n",
            "Epoch 7, Iteration 360\t train_loss: 1.07 took: 0.20s\n",
            "Epoch 7, Iteration 380\t train_loss: 1.12 took: 0.21s\n",
            "Epoch 7, Iteration 400\t train_loss: 1.21 took: 0.20s\n",
            "Epoch 7, Iteration 420\t train_loss: 1.17 took: 0.22s\n",
            "Epoch 7, Iteration 440\t train_loss: 1.11 took: 0.25s\n",
            "Epoch 7, Iteration 460\t train_loss: 1.03 took: 0.23s\n",
            "Epoch 7, Iteration 480\t train_loss: 1.10 took: 0.22s\n",
            "Epoch 7, Iteration 500\t train_loss: 1.23 took: 0.23s\n",
            "Epoch 7, Iteration 520\t train_loss: 1.15 took: 0.21s\n",
            "Epoch 7, Iteration 540\t train_loss: 1.06 took: 0.21s\n",
            "Epoch 7, Iteration 560\t train_loss: 1.13 took: 0.24s\n",
            "Epoch 7, Iteration 580\t train_loss: 1.10 took: 0.19s\n",
            "Epoch 7, Iteration 600\t train_loss: 1.22 took: 0.22s\n",
            "Epoch 7, Iteration 620\t train_loss: 1.27 took: 0.20s\n",
            "Validation loss = 1.62\n",
            "Epoch 8, Iteration 20\t train_loss: 0.95 took: 0.29s\n",
            "Epoch 8, Iteration 40\t train_loss: 0.98 took: 0.19s\n",
            "Epoch 8, Iteration 60\t train_loss: 0.97 took: 0.20s\n",
            "Epoch 8, Iteration 80\t train_loss: 0.94 took: 0.19s\n",
            "Epoch 8, Iteration 100\t train_loss: 0.99 took: 0.19s\n",
            "Epoch 8, Iteration 120\t train_loss: 1.00 took: 0.19s\n",
            "Epoch 8, Iteration 140\t train_loss: 1.06 took: 0.19s\n",
            "Epoch 8, Iteration 160\t train_loss: 1.03 took: 0.20s\n",
            "Epoch 8, Iteration 180\t train_loss: 1.06 took: 0.20s\n",
            "Epoch 8, Iteration 200\t train_loss: 1.01 took: 0.19s\n",
            "Epoch 8, Iteration 220\t train_loss: 1.03 took: 0.19s\n",
            "Epoch 8, Iteration 240\t train_loss: 0.97 took: 0.19s\n",
            "Epoch 8, Iteration 260\t train_loss: 1.08 took: 0.19s\n",
            "Epoch 8, Iteration 280\t train_loss: 1.13 took: 0.19s\n",
            "Epoch 8, Iteration 300\t train_loss: 1.01 took: 0.19s\n",
            "Epoch 8, Iteration 320\t train_loss: 1.08 took: 0.19s\n",
            "Epoch 8, Iteration 340\t train_loss: 1.17 took: 0.20s\n",
            "Epoch 8, Iteration 360\t train_loss: 1.08 took: 0.19s\n",
            "Epoch 8, Iteration 380\t train_loss: 1.10 took: 0.21s\n",
            "Epoch 8, Iteration 400\t train_loss: 1.09 took: 0.19s\n",
            "Epoch 8, Iteration 420\t train_loss: 1.14 took: 0.20s\n",
            "Epoch 8, Iteration 440\t train_loss: 1.11 took: 0.20s\n",
            "Epoch 8, Iteration 460\t train_loss: 0.99 took: 0.19s\n",
            "Epoch 8, Iteration 480\t train_loss: 1.10 took: 0.23s\n",
            "Epoch 8, Iteration 500\t train_loss: 1.11 took: 0.20s\n",
            "Epoch 8, Iteration 520\t train_loss: 1.10 took: 0.18s\n",
            "Epoch 8, Iteration 540\t train_loss: 1.06 took: 0.20s\n",
            "Epoch 8, Iteration 560\t train_loss: 1.19 took: 0.19s\n",
            "Epoch 8, Iteration 580\t train_loss: 1.05 took: 0.21s\n",
            "Epoch 8, Iteration 600\t train_loss: 1.18 took: 0.20s\n",
            "Epoch 8, Iteration 620\t train_loss: 1.13 took: 0.19s\n",
            "Validation loss = 1.61\n",
            "Epoch 9, Iteration 20\t train_loss: 0.91 took: 0.26s\n",
            "Epoch 9, Iteration 40\t train_loss: 0.92 took: 0.18s\n",
            "Epoch 9, Iteration 60\t train_loss: 0.98 took: 0.20s\n",
            "Epoch 9, Iteration 80\t train_loss: 0.89 took: 0.18s\n",
            "Epoch 9, Iteration 100\t train_loss: 0.92 took: 0.19s\n",
            "Epoch 9, Iteration 120\t train_loss: 0.95 took: 0.18s\n",
            "Epoch 9, Iteration 140\t train_loss: 0.90 took: 0.22s\n",
            "Epoch 9, Iteration 160\t train_loss: 1.00 took: 0.22s\n",
            "Epoch 9, Iteration 180\t train_loss: 0.97 took: 0.19s\n",
            "Epoch 9, Iteration 200\t train_loss: 1.04 took: 0.20s\n",
            "Epoch 9, Iteration 220\t train_loss: 0.92 took: 0.18s\n",
            "Epoch 9, Iteration 240\t train_loss: 0.98 took: 0.19s\n",
            "Epoch 9, Iteration 260\t train_loss: 0.99 took: 0.18s\n",
            "Epoch 9, Iteration 280\t train_loss: 0.99 took: 0.20s\n",
            "Epoch 9, Iteration 300\t train_loss: 1.07 took: 0.18s\n",
            "Epoch 9, Iteration 320\t train_loss: 0.99 took: 0.20s\n",
            "Epoch 9, Iteration 340\t train_loss: 1.11 took: 0.20s\n",
            "Epoch 9, Iteration 360\t train_loss: 1.02 took: 0.19s\n",
            "Epoch 9, Iteration 380\t train_loss: 1.06 took: 0.20s\n",
            "Epoch 9, Iteration 400\t train_loss: 1.02 took: 0.21s\n",
            "Epoch 9, Iteration 420\t train_loss: 1.06 took: 0.20s\n",
            "Epoch 9, Iteration 440\t train_loss: 1.06 took: 0.19s\n",
            "Epoch 9, Iteration 460\t train_loss: 1.06 took: 0.19s\n",
            "Epoch 9, Iteration 480\t train_loss: 1.03 took: 0.21s\n",
            "Epoch 9, Iteration 500\t train_loss: 1.04 took: 0.21s\n",
            "Epoch 9, Iteration 520\t train_loss: 1.04 took: 0.19s\n",
            "Epoch 9, Iteration 540\t train_loss: 1.05 took: 0.21s\n",
            "Epoch 9, Iteration 560\t train_loss: 1.00 took: 0.21s\n",
            "Epoch 9, Iteration 580\t train_loss: 0.98 took: 0.19s\n",
            "Epoch 9, Iteration 600\t train_loss: 1.00 took: 0.19s\n",
            "Epoch 9, Iteration 620\t train_loss: 1.04 took: 0.19s\n",
            "Validation loss = 1.70\n",
            "Epoch 10, Iteration 20\t train_loss: 0.99 took: 0.27s\n",
            "Epoch 10, Iteration 40\t train_loss: 0.98 took: 0.18s\n",
            "Epoch 10, Iteration 60\t train_loss: 0.88 took: 0.18s\n",
            "Epoch 10, Iteration 80\t train_loss: 0.86 took: 0.19s\n",
            "Epoch 10, Iteration 100\t train_loss: 0.86 took: 0.19s\n",
            "Epoch 10, Iteration 120\t train_loss: 0.98 took: 0.20s\n",
            "Epoch 10, Iteration 140\t train_loss: 0.91 took: 0.22s\n",
            "Epoch 10, Iteration 160\t train_loss: 0.91 took: 0.21s\n",
            "Epoch 10, Iteration 180\t train_loss: 0.94 took: 0.22s\n",
            "Epoch 10, Iteration 200\t train_loss: 0.86 took: 0.19s\n",
            "Epoch 10, Iteration 220\t train_loss: 0.92 took: 0.20s\n",
            "Epoch 10, Iteration 240\t train_loss: 0.89 took: 0.18s\n",
            "Epoch 10, Iteration 260\t train_loss: 0.96 took: 0.20s\n",
            "Epoch 10, Iteration 280\t train_loss: 1.10 took: 0.20s\n",
            "Epoch 10, Iteration 300\t train_loss: 1.06 took: 0.19s\n",
            "Epoch 10, Iteration 320\t train_loss: 0.99 took: 0.19s\n",
            "Epoch 10, Iteration 340\t train_loss: 0.88 took: 0.19s\n",
            "Epoch 10, Iteration 360\t train_loss: 0.93 took: 0.19s\n",
            "Epoch 10, Iteration 380\t train_loss: 0.88 took: 0.20s\n",
            "Epoch 10, Iteration 400\t train_loss: 0.83 took: 0.20s\n",
            "Epoch 10, Iteration 420\t train_loss: 0.93 took: 0.22s\n",
            "Epoch 10, Iteration 440\t train_loss: 1.01 took: 0.23s\n",
            "Epoch 10, Iteration 460\t train_loss: 0.98 took: 0.23s\n",
            "Epoch 10, Iteration 480\t train_loss: 0.96 took: 0.23s\n",
            "Epoch 10, Iteration 500\t train_loss: 0.99 took: 0.21s\n",
            "Epoch 10, Iteration 520\t train_loss: 0.98 took: 0.22s\n",
            "Epoch 10, Iteration 540\t train_loss: 0.92 took: 0.20s\n",
            "Epoch 10, Iteration 560\t train_loss: 0.98 took: 0.21s\n",
            "Epoch 10, Iteration 580\t train_loss: 1.01 took: 0.21s\n",
            "Epoch 10, Iteration 600\t train_loss: 0.99 took: 0.21s\n",
            "Epoch 10, Iteration 620\t train_loss: 0.92 took: 0.22s\n",
            "Validation loss = 1.73\n",
            "Training finished, took 72.69s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9DHRO7sI9jH",
        "colab_type": "code",
        "outputId": "82904dfa-2d9b-4c0a-ec26-0864b52deba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(test_hist_x,test_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
        "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is 0.71065\n",
            "Test accuracy is 0.4956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogVa8vQaxT2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}