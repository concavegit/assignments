{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working with Large Datasets COCO Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/m1_project/Examples/Working_with_Large_Datasets_COCO_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVFu3OVp3Xix",
        "colab_type": "text"
      },
      "source": [
        "# COCO Convnet example project\n",
        "This notebook uses the GPU functionality of Pytorch and Google Collab. We need to make sure we are running our operations on the GPU, verify this in your notebook settings at the top. Setting found under:\n",
        "\n",
        "`Runtime > Change runtime type > Hardware Accelerator -> GPU`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3alCNsXcczI",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjeB52XEcWmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchviz\n",
        "!pip install pycoco\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # we always love numpy\n",
        "import time\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QSmvpB8hYG8",
        "colab_type": "text"
      },
      "source": [
        "Now let's get the dataset we want to work with:  [COCO!](http://cocodataset.org/#detection-2017)\n",
        "\n",
        "This is a biiiig dataset, so it's gonna be a lot of data. We have uploaded the dataset to Google Drive and will be downloading it via shared link. Google Colab is capable of downloading from Google Drive links at a very fast rate (~200 MB/sec).  We recommend this as a method of uploading large datasets.  We have a Google Drive account with lots of storage, so if you are not able to add a database to your account (due to limitations), we can add it to ours and give you a link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2uCI6vKUlQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1M3doqupItS419I6z-D3rCHUaPo93HbUE&export=download',\n",
        "               'train2017.zip',\n",
        "               quiet=False)\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=19-0acEBHMn7LOoT0CUw_ExoLkQiWnfsw&export=download',\n",
        "               'val2017.zip',\n",
        "               quiet=False)\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1D_xHd_WlZTrvA2tA1D_Io8ezTSoIYEpq&export=download',\n",
        "               'annotations_trainval2017.zip',\n",
        "               quiet=False)\n",
        "\n",
        "print(\"Dataset downloaded\")\n",
        "\n",
        "!unzip -qq -o annotations_trainval2017.zip\n",
        "print(\"Annotations extracted\")\n",
        "\n",
        "!unzip -qq -o train2017.zip\n",
        "print(\"training data extracted\")\n",
        "\n",
        "!unzip -qq -o val2017.zip\n",
        "print(\"validation data extracted\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US13M6R4cxbL",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "This dataset happens to be included with pytorch so we can call some pytorch functions to automatically load and parse the data we need to. \n",
        "\n",
        "This dataset has 80 classes that we're interested in. More importantly, more than one class can appear in a single image. So, we cannot rely on our typical loss function that assumes one answer per image.\n",
        "\n",
        "Also, the images in this dataset are large, on the order of 480 x 640 pixels. We learned (through trial and error) that google colab cannot really handle images this large, so we're going to shrink them to 120 x 160 for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5ZJcyhVG07J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data set information\n",
        "image_dims = 3, 120, 160\n",
        "n_training_samples = 100000 # How many training images to use\n",
        "n_test_samples = 5000 # How many test images to use\n",
        "\n",
        "# Read the class labels from a file\n",
        "with open('./annotations/instances_val2017.json','r') as COCO:\n",
        "    js = json.loads(COCO.read())\n",
        "    class_list = js['categories']\n",
        "\n",
        "# We convert the total class IDs (91) to the IDs we're interested in (80)\n",
        "classes_dict = {}\n",
        "classes = []\n",
        "class_index = 0\n",
        "for d in class_list:\n",
        "  classes_dict[d['id']] = class_index\n",
        "  classes.append(d['name'])\n",
        "  class_index += 1\n",
        "class_len = len(classes)\n",
        "\n",
        "# Define a function to parse a sample label into a form that we want (aka a tensor)\n",
        "def get_classes(target):\n",
        "  class_tensor = torch.zeros((class_len))\n",
        "  for c in target:\n",
        "    class_id = c['category_id']\n",
        "    if class_id not in classes_dict:\n",
        "      continue\n",
        "    idx = classes_dict[class_id]\n",
        "    class_tensor[idx] = 1\n",
        "  return class_tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKQk3V09cxrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to transform our image into a form we can handle\n",
        "# First we crop the image at the center to make sure theyre all the same size\n",
        "# Then we squish it in to our desired size to make it more manageable\n",
        "# transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
        "transform = transforms.Compose(\n",
        "    [transforms.CenterCrop((480,640)), transforms.Resize((120,160)), transforms.ToTensor()])\n",
        "\n",
        "print(\"loading training set\")\n",
        "\n",
        "# Load the training and test set\n",
        "train_set = torchvision.datasets.CocoDetection(\n",
        "    root='./train2017', annFile=\"./annotations/instances_train2017.json\", transform=transform, target_transform=get_classes)\n",
        "train_sampler = SubsetRandomSampler(\n",
        "    np.arange(n_training_samples, dtype=np.int64))\n",
        "print(\"loading test set\")\n",
        "\n",
        "\n",
        "test_set = torchvision.datasets.CocoDetection(\n",
        "    root='./val2017', annFile=\"./annotations/instances_val2017.json\", transform=transform, target_transform=get_classes)\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynLxCQiAPIzZ",
        "colab_type": "text"
      },
      "source": [
        "# Explore the data\n",
        "Let's take a look at some images and their labels to make sure it makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXBo45urPHfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def disp_image(image, class_idxs, predicted=None):\n",
        "    # need to reorder the tensor dimensions to work properly with imshow\n",
        "    plt.imshow(image.transpose(0,2).transpose(0,1))\n",
        "    plt.axis('off')\n",
        "    classes_title = [classes[class_idx[0]] for class_idx in class_idxs]\n",
        "    classes_title = ', '.join(classes_title)\n",
        "\n",
        "    if predicted:\n",
        "        p_classes_title = [classes[class_idx[0]] for class_idx in predicted]\n",
        "        p_classes_title = ', '.join(classes_title)\n",
        "        plt.title(\"Actual: \" + classes_title + \"     Predicted: \" + p_classes_title)\n",
        "    else:\n",
        "        plt.title(\"Actual: \" + classes_title)\n",
        "    plt.show()\n",
        "    \n",
        "x, y = train_set[3456]\n",
        "y = y.numpy()\n",
        "y = np.argwhere(y == np.amax(y))\n",
        "print(y)\n",
        "disp_image(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJvsFLDDdR8l",
        "colab_type": "text"
      },
      "source": [
        "## Define Model Class\n",
        "Since this is a more complicated dataset than the ones we've seen in the past, we'll need a bigger model. Let's add 4 convolutional layers and expand the size of our fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk53mHNXf4Ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(myCNN, self).__init__()\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=5, stride=5, padding=0)\n",
        "        self.fc1_size = 256\n",
        "        self.fc2_size = class_len\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], 32, kernel_size=3,\n",
        "                  stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.maxpool_output_size = int(256 * (image_dims[1] / 40) * (image_dims[2] / 40))\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, self.fc1_size)\n",
        "        self.fc2 = nn.Linear(self.fc1_size, self.fc2_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Convolutional Layers\n",
        "        x = self.activation_func(self.pool2(self.conv1(x)))\n",
        "        x = self.activation_func(self.pool2(self.conv2(x)))\n",
        "        x = self.activation_func(self.pool2(self.conv3(x)))\n",
        "        x = self.activation_func(self.pool5(self.conv4(x)))\n",
        "        # Fully Connected Layers\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function, we'll use BCE or Binary CrossEntropy that does not assume one class fer example\n",
        "      # https://pytorch.org/docs/stable/nn.html\n",
        "      loss = nn.BCEWithLogitsLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQHPW08-f5fK",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "First let's create our model. Let's also check out a graphical representation of our model (using a library we downloaded earlier) to validate the model looks like we think it should.\n",
        "\n",
        "**Running the below cell will override your model if have already trained one**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXeauNXlBIK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_pretrained_model = True\n",
        "\n",
        "# Define what device we want to use\n",
        "device = 'cuda' # 'cpu' if we want to not use the gpu\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = myCNN()\n",
        "\n",
        "if use_pretrained_model:\n",
        "    gdown.download('https://drive.google.com/uc?authuser=0&id=1AUM8t698p6JIFaH-Awg_8lATuwArRzN0&export=download',\n",
        "                   'coco_pretrained.pth',\n",
        "                   quiet=False)\n",
        "    check_point = torch.load('coco_pretrained.pth')\n",
        "    net.load_state_dict(check_point['state_dict'])\n",
        "\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "\n",
        "# Visualize the architecture of the model\n",
        "# We need to give the net a fake input for this library to visualize the architecture\n",
        "fake_input = Variable(torch.zeros((1,image_dims[0], image_dims[1], image_dims[2]))).to(device)\n",
        "outputs = net(fake_input)\n",
        "# Plot the DAG (Directed Acyclic Graph) of the model\n",
        "make_dot(outputs, dict(net.named_parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I52pm5-AmwKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not use_pretrained_model:\n",
        "    # Define training parameters\n",
        "    batch_size = 32\n",
        "    learning_rate = 3e-3\n",
        "    n_epochs = 8\n",
        "    # Get our data into the mini batch size that we defined\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                            sampler=train_sampler, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_set, batch_size=2, sampler=test_sampler, num_workers=2)\n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 20\n",
        "    test_every = 200\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    test_hist_x = []\n",
        "    test_loss_hist = []\n",
        "else:\n",
        "    print(\"skipping training definitions since we are using pretrained model\")\n",
        "\n",
        "# Get the brute accuracy of our model\n",
        "# This doesn't really do a good job of characterizing the performance as it is the\n",
        "# raw accuracy (which includes predicting 0 versus 1 for each class)\n",
        "def get_acc(output,targets):\n",
        "    # Get the guess of each class\n",
        "    output = torch.round(torch.sigmoid(output))\n",
        "    # Compare guesses\n",
        "    diff = targets - output\n",
        "    avg = torch.mean(torch.abs(diff))\n",
        "    return 1 - avg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQorZsxQBIc6",
        "colab_type": "text"
      },
      "source": [
        "### Now let's train the model!\n",
        "This may take a while (2+ hours), so don't let your computer go to sleep or it may time out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvcJyKgwdSR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_loss(run_idx):\n",
        "    # do a pass on the test set\n",
        "    total_test_loss = 0\n",
        "    total_acc_loss = 0\n",
        "    idx = 0\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Wrap tensors in Variables\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        test_outputs = net(inputs)\n",
        "        test_loss_size = loss(test_outputs, labels)\n",
        "        total_test_loss += test_loss_size.data.item()\n",
        "        total_acc_loss += get_acc(test_outputs, labels)\n",
        "        if idx >= 100:\n",
        "          break\n",
        "        idx += 1\n",
        "    test_loss_hist.append(total_test_loss / (idx+1))\n",
        "    test_hist_x.append(run_idx)\n",
        "    print(\"Validation loss = {:.4f}\".format(\n",
        "        total_test_loss / (idx+1)))\n",
        "    print(\"Validation Accuracy = {:.4f}\".format(\n",
        "        total_acc_loss / (idx+1)))\n",
        "\n",
        "if not use_pretrained_model:\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "            # Get inputs in right form\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "            \n",
        "            # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Compute the loss and find the loss with respect to each parameter of the model\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            \n",
        "            # Change each parameter with respect to the recently computed loss.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print every 20th batch of an epoch\n",
        "            if (i % print_every) == print_every-1:\n",
        "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.4f} took: {:.4f}s\".format(\n",
        "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "                # Reset running loss and time\n",
        "                train_loss_hist.append(running_loss / print_every)\n",
        "                train_hist_x.append(idx)\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            # Check test set every nth batch\n",
        "            if (i % test_every) == test_every -1:\n",
        "                test_loss(idx)\n",
        "                \n",
        "                idx += 1\n",
        "\n",
        "\n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "else:\n",
        "    print(\"skipping training since we are using pretrained model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEwIESYIl0VU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not use_pretrained_model:\n",
        "    # loading and saving via this link:\n",
        "    # https://medium.com/udacity-pytorch-challengers/saving-loading-your-model-in-pytorch-741b80daf3c\n",
        "    checkpoint = {'model': myCNN(),\n",
        "            'state_dict': net.state_dict(),\n",
        "            'optimizer' : optimizer.state_dict()\n",
        "            }\n",
        "    f = open('coco_pretrained.pth', 'wb')\n",
        "    torch.save(checkpoint, f)\n",
        "    f.close()\n",
        "    from google.colab import files\n",
        "    files.download('coco_pretrained.pth')\n",
        "else:\n",
        "    print(\"skipping downloading model since we are using pretrained model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh1Z2O3QzVNd",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "Let's plot the loss of the network over time to see if any learning actually occured."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbLDTiF960zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not use_pretrained_model:\n",
        "    plt.plot(train_hist_x,train_loss_hist)\n",
        "    plt.plot(test_hist_x,test_loss_hist)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"skipping showing error plots since we are using pretrained model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHnhHsKrEp-V",
        "colab_type": "text"
      },
      "source": [
        "## Visualization\n",
        "Now that we have a trianed model, let's see if we can get some insights out of it.  \n",
        "\n",
        "In this case we examined gradients of different classes with respect to the **inputs** of the model. One way to think about this, if we set the gradient of the output of class `plane` to 1, what pixels played the biggest part in the prediction of that class? AKA what pixels does the Model pay the most attention to when predicting `plane`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDL5SibNzUuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global input_gradients\n",
        "# Used to grab the gradient\n",
        "def set_gradient_hook(grad):\n",
        "    global input_gradients\n",
        "    input_gradients = grad\n",
        "\n",
        "# Show an image on a given subplot\n",
        "def disp_image(image, subplot, label=None, cmap='viridis', correct=True):\n",
        "    plt.subplot(subplot[0], subplot[1], subplot[2])\n",
        "    if(cmap != 'viridis'):  \n",
        "        plt.imshow(image, cmap=cmap,vmin=0, vmax=1)\n",
        "    else:\n",
        "        plt.imshow(image, cmap=cmap)\n",
        "    plt.axis('off')\n",
        "    truth_phrase = ' GROUND TRUTH' if correct else ' FALSE PREDICTION'\n",
        "    if(label is not None):\n",
        "      plt.title(\"A: \" + label + truth_phrase)\n",
        "\n",
        "# Function that computes the relevant gradient\n",
        "def get_input_gradient(image, label):\n",
        "    input_img = Variable(image.unsqueeze(0), requires_grad=True).to(device)\n",
        "    input_img.register_hook(set_gradient_hook)\n",
        "    model_output = net(input_img)\n",
        "    one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_().to(device)\n",
        "    one_hot_output[0][label] = 1\n",
        "    model_output.backward(gradient=one_hot_output)\n",
        "    grad = np.abs(input_gradients.cpu().numpy()).squeeze()\n",
        "    # Normalize Heatmap\n",
        "    grayscale_im = np.sum(grad, axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    return grayscale_im\n",
        "\n",
        "# Plot the image, its gradients and the product of the 2 for visualization\n",
        "def plot_image_row(image, grayscale, label, subplot, correct=True):\n",
        "    # 1\n",
        "    disp_image(grayscale, subplot)\n",
        "    subplot[2] += 1\n",
        "    # 2\n",
        "    masked_img = np.copy(image.transpose(0,2).transpose(0,1).numpy())\n",
        "    # Sqrt makes the images easier to see\n",
        "    masked_img[:,:,0] = masked_img[:,:,0] * grayscale\n",
        "    masked_img[:,:,1] = masked_img[:,:,1] * grayscale\n",
        "    masked_img[:,:,2] = masked_img[:,:,2] * grayscale\n",
        "    disp_image(masked_img, subplot)\n",
        "    subplot[2] += 1\n",
        "    # 3\n",
        "    if label == -1:\n",
        "        class_label = \"sum of all correct classes\"\n",
        "    else:\n",
        "        class_label = classes[label]\n",
        "    disp_image(image.transpose(0,2).transpose(0,1), subplot, class_label, correct=correct)\n",
        "    subplot[2] += 1\n",
        "    return subplot\n",
        "\n",
        "# Plot the gradients and image for every correct class for an image, also plot any incorrect guesses if applicable\n",
        "def analyse_image(idx):\n",
        "    image, label = test_set[idx]\n",
        "    logits = net(Variable(image.unsqueeze(0)).to(device)).squeeze()\n",
        "    correct_classes = [idx for idx, val in enumerate(label) if val == 1]\n",
        "    incorrect_classes = [idx for idx, val in enumerate(logits) if (val >= 0 and idx not in correct_classes)]\n",
        "    num_plots = len(correct_classes) + len(incorrect_classes) + bool(correct_classes) + bool(incorrect_classes)\n",
        "    plt.figure(figsize=(16,4 * (num_plots)))\n",
        "    subplot = [num_plots, 3, 1]\n",
        "    # Correct\n",
        "    correct_grayscales = []\n",
        "    for idx in correct_classes:\n",
        "        grayscale_correct = get_input_gradient(image, idx)\n",
        "        correct_grayscales.append(grayscale_correct)\n",
        "        subplot = plot_image_row(image, grayscale_correct, idx, subplot)\n",
        "    # Incorrect\n",
        "    incorrect_grayscales = []\n",
        "    for idx in incorrect_classes:\n",
        "        grayscale_incorrect = get_input_gradient(image, idx)\n",
        "        incorrect_grayscales.append(grayscale_incorrect)\n",
        "        subplot = plot_image_row(image, grayscale_incorrect, idx, subplot, False)\n",
        "    if(len(correct_classes)):\n",
        "        correct_grayscales = np.array(correct_grayscales)\n",
        "        correct_grayscales = correct_grayscales.mean(axis=0)\n",
        "        subplot = plot_image_row(image, correct_grayscales, -1, subplot)\n",
        "    if(len(incorrect_classes)):\n",
        "        incorrect_grayscales = np.array(incorrect_grayscales)\n",
        "        incorrect_grayscales = incorrect_grayscales.mean(axis=0)\n",
        "        subplot = plot_image_row(image, incorrect_grayscales, -1, subplot, False)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "analyse_image(8)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-VF77a5UDm",
        "colab_type": "text"
      },
      "source": [
        "## Extensions\n",
        "We chose to look into the input visualizations of this model, but there are many other sample directions we could have gone. We could:\n",
        "\n",
        "- Change the structure of the network to improve the fit to data\n",
        "- Add some regularization on the model to prevent overfitting (such as penalizing the square of the weights as we did in ridge regression or using a special technique for neural networks called [Dropout](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5))\n",
        "- Try instead to use simple logistic regression on the data and see how your results look in comparison\n",
        "- Test the trained model on another dataset to see how much knowledge transfers over\n",
        "- Do some form of visualization on the weights of the model\n",
        "- Try this visualization on a pre-trained model, such as Inception\n"
      ]
    }
  ]
}