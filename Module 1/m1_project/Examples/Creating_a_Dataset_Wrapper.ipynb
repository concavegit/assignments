{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating a Dataset Wrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/m1_project/Examples/Creating_a_Dataset_Wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1HhEVYrOyA",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Dataset Wrapper\n",
        "\n",
        "If you are having trouble getting your data into Colab for use with pytorch, this notebook might be for you. This will cover the case where it is not feasible to create a giant tensor with all of your images.  For instance, if you are trying to learn on a dataset with tens of thousands of images, the images will take up too much memory when decompressed.\n",
        "\n",
        "Instead, we will write a dataset class that will allow pytorch to selectively decompress images when it needs them.  For our example, we're going to use the Caltech 256 dataset, which we've uploaded to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8XI8q81rqnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=1GMtKmoH2Yu9yY6jx4Mm5fZ1sLkGV9i_c&export=download',\n",
        "               'caltech256.zip',\n",
        "               quiet=False)\n",
        "!unzip -qq caltech256.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyMURCpbs2dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision.datasets import VisionDataset\n",
        "import os\n",
        "from glob import glob\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image(img_tensor):\n",
        "    # need to reorder the tensor dimensions to work properly with imshow\n",
        "    plt.imshow(img_tensor.transpose(0,2).transpose(0,1))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "class Caltech256(VisionDataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "        super(Caltech256, self).__init__('.',\n",
        "                                         transform=transform,\n",
        "                                         target_transform=target_transform)\n",
        "        self.categories = []\n",
        "        self.index = []\n",
        "        self.y = []\n",
        "        for c in sorted(glob(os.path.join(self.root, \"256_ObjectCategories\",'???.*'))):\n",
        "            _, category_dir = os.path.split(c)\n",
        "            class_idx = int(category_dir[0:3]) - 1\n",
        "            if class_idx >= 256:\n",
        "                # skip the clutter category\n",
        "                continue\n",
        "            n = len(glob(os.path.join(self.root, \"256_ObjectCategories\", category_dir, '*.jpg')))\n",
        "            self.categories.append(category_dir)\n",
        "            self.index.extend(range(1, n + 1))\n",
        "            self.y.extend(n * [class_idx])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        # a gotcha is when some of the images are black and white, we can use\n",
        "        # the convert('RGB') command to make sure everything is a three channel\n",
        "        # RGB image.\n",
        "        img = Image.open(os.path.join(self.root,\n",
        "                                      \"256_ObjectCategories\",\n",
        "                                      self.categories[self.y[index]],\n",
        "                                      \"{:03d}_{:04d}.jpg\".format(self.y[index] + 1, self.index[index]))).convert('RGB')\n",
        "\n",
        "        target = self.y[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img.float(), target\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "\n",
        "# center crop 200, 200 pixel patch and then resize to 100 by 100 for\n",
        "# computational efficiency\n",
        "cal_tech = Caltech256(transform=transforms.Compose([transforms.CenterCrop((200,200)),\n",
        "                                                    transforms.Resize((100,100)),\n",
        "                                                    transforms.ToTensor()]))\n",
        "\n",
        "im, target = cal_tech[2000]\n",
        "show_image(im)\n",
        "print(im.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-s1-ds-zOnL",
        "colab_type": "text"
      },
      "source": [
        "The rest is adapted from the other notebook on working with the COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AF4auKH2MpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 3e-3\n",
        "n_epochs = 8\n",
        "image_dims = 3, 100, 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfBe_EY2vY0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "class myCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(myCNN, self).__init__()\n",
        "        class_len = 256\n",
        "        self.activation_func = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=5, stride=5, padding=0)\n",
        "        self.fc1_size = 512\n",
        "        self.fc2_size = class_len\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(image_dims[0], 32, kernel_size=3,\n",
        "                  stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3,\n",
        "          stride=1, padding=1)\n",
        "        self.maxpool_output_size = int(128 * (image_dims[1] / 20) * (image_dims[2] / 20))\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(self.maxpool_output_size, self.fc1_size)\n",
        "        self.fc2 = nn.Linear(self.fc1_size, self.fc2_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Convolutional Layers\n",
        "        x = self.activation_func(self.pool2(self.conv1(x)))\n",
        "        x = self.activation_func(self.pool2(self.conv2(x)))\n",
        "        x = self.activation_func(self.pool5(self.conv3(x)))\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = x.view(-1, self.maxpool_output_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def get_loss(self, learning_rate):\n",
        "      # Loss function, we'll use BCE or Binary CrossEntropy that does not assume one class fer example\n",
        "      # https://pytorch.org/docs/stable/nn.html\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "      return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4r3hpNazRR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = myCNN()\n",
        "loss, optimizer = net.get_loss(learning_rate)\n",
        "\n",
        "# Define some parameters to keep track of metrics\n",
        "print_every = 20\n",
        "test_every = 2000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHDKNYRf520B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def test_loss(run_idx):\n",
        "    # do a pass on the test set\n",
        "    total_test_loss = 0\n",
        "    idx = 0\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Wrap tensors in Variables\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        test_outputs = net(inputs)\n",
        "        test_loss_size = loss(test_outputs, labels)\n",
        "        total_test_loss += test_loss_size.data.item()\n",
        "        idx += 1\n",
        "    test_loss_hist.append(total_test_loss / (idx+1))\n",
        "    test_hist_x.append(run_idx)\n",
        "    print(\"Validation loss = {:.4f}\".format(\n",
        "        total_test_loss / (idx+1)))\n",
        "idx = 0\n",
        "train_hist_x = []\n",
        "train_loss_hist = []\n",
        "test_hist_x = []\n",
        "test_loss_hist = []\n",
        "\n",
        "n_train = 20000\n",
        "indices = torch.randperm(len(cal_tech))\n",
        "\n",
        "train_idx, test_idx = indices[:n_train], indices[n_train:]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "# Get our data into the mini batch size that we defined\n",
        "train_loader = torch.utils.data.DataLoader(cal_tech, batch_size=batch_size,\n",
        "                                        sampler=train_sampler)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(cal_tech, batch_size=batch_size,\n",
        "                                        sampler=test_sampler)\n",
        "\n",
        "\n",
        "device = 'cuda'\n",
        "net.to(device)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # Get inputs in right form\n",
        "        inputs, labels = data\n",
        "        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n",
        "        \n",
        "        # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        # Compute the loss and find the loss with respect to each parameter of the model\n",
        "        loss_size = loss(outputs, labels)\n",
        "        loss_size.backward()\n",
        "        \n",
        "        # Change each parameter with respect to the recently computed loss.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update statistics\n",
        "        running_loss += loss_size.data.item()\n",
        "        \n",
        "        # Print every 20th batch of an epoch\n",
        "        if (i % print_every) == print_every-1:\n",
        "            print(\"Epoch {}, Iteration {}\\t train_loss: {:.4f} took: {:.4f}s\".format(\n",
        "                epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "            # Reset running loss and time\n",
        "            train_loss_hist.append(running_loss / print_every)\n",
        "            train_hist_x.append(idx)\n",
        "            running_loss = 0.0\n",
        "            start_time = time.time()\n",
        "        # Check test set every nth batch\n",
        "        if (i % test_every) == test_every -1:\n",
        "            test_loss(idx)\n",
        "            idx += 1\n",
        "\n",
        "print(\"Training finished, took {:.2f}s\".format(\n",
        "    time.time() - training_start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz5a5aP-E52u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}