{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Estimating Face Pose from Images.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlfa19/assignments/blob/master/Module%201/03/Estimating_Face_Pose_from_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxJ1nir4k-1P",
        "colab_type": "text"
      },
      "source": [
        "# Estimating Head Pose from Images\n",
        "\n",
        "Let's try to estimate people's head position from photos. The [original dataset](http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html) link shows how the images where collected. Check out the photos of their setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cOdc0iulZgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, let's download the data (or upload if using colab). \n",
        "# You may need to click yes to approve running this code if using colab... Can Paul Ruvolo be trusted?\n",
        "\n",
        "!wget \"https://www.dropbox.com/s/9u9znk0utfr7yjf/HeadPoseImageDatabase.tar.gz?dl=0\" -O head_pose.tar.gz\n",
        "!tar -xvzf head_pose.tar.gz > /dev/null\n",
        "\n",
        "#old: !wget \"https://drive.google.com/uc?authuser=0&id=1304LwlF0o_L0N3njQyB1Gy77FGwUGIUg&export=download\" -O head_pose.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nD8zu1InTlR",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "These data comprise multiple people, each at various head positions (both pitch and yaw).\n",
        "\n",
        "This code loads the images, crops to the face (face boxes provided in the dataset), and downsamples to 20x20 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JlYRkyIkumB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "def load_data():\n",
        "    \"\"\" Load the head pose image dataset from here:\n",
        "        http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html\n",
        "        \n",
        "        returns a tuple containing\n",
        "            person_id: a list of subject directories where the image came from\n",
        "            images: the face boxes cropped to (20, 20) grayscale pixels\n",
        "            head pitch: the pitch of the subject's head\n",
        "            yaw: the yaw of the subject's head \"\"\"\n",
        "    person_ids = []\n",
        "    images = []\n",
        "    pitches = []\n",
        "    yaws = []\n",
        "    for person_path in glob.glob('Person*'):\n",
        "        for image_path in glob.glob(os.path.join(person_path, '*.jpg')):\n",
        "            m = re.search('([-\\+][0-9]*)([-\\+][0-9]*).jpg$', image_path)\n",
        "            pitch = float(m.group(1))\n",
        "            yaw = float(m.group(2))\n",
        "\n",
        "            # don't use images with extreme pitches\n",
        "            if np.abs(pitch) > 30:\n",
        "                continue\n",
        "            im = cv2.imread(image_path)\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "            face_box_path = os.path.join(image_path[:-4] + '.txt')\n",
        "            with open(face_box_path) as f:\n",
        "                lines = f.readlines()\n",
        "                # grab the center pixel coordinate of the face (x_c, y_c) and\n",
        "                # size (w, h) of the face bounding box\n",
        "                x_c, y_c, w, h = (int(l) for l in lines[3:])\n",
        "                # use a square cropping by taking the maximum of the two sizes\n",
        "                big_length = max(w, h)\n",
        "\n",
        "                # grab the face by indexing into the numpy array\n",
        "                face_pixels = im[y_c-big_length//2:y_c+big_length//2,\\\n",
        "                                 x_c-big_length//2:x_c+big_length//2]\n",
        "                try:\n",
        "                    # resize the image to a (20, 20) patch to make it easier\n",
        "                    # for linear regression (less dimensions)\n",
        "                    face_pixels = cv2.resize(face_pixels, (20, 20))\n",
        "                    images.append(face_pixels)\n",
        "                    pitches.append(pitch)\n",
        "                    yaws.append(yaw)\n",
        "                    print(person_path, yaw, pitch)\n",
        "                    person_ids.append(person_path)\n",
        "                except Exception as ex:\n",
        "                    continue\n",
        "    return person_ids, np.array(images), np.array(pitches), np.array(yaws)\n",
        "\n",
        "person_ids, images, pitches, yaws = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17a7KeQup4dS",
        "colab_type": "text"
      },
      "source": [
        "## Visualize the data\n",
        "An important first step is to visualize your data to make sure everything is arranged as expected. The code below plots multiple head positions for the first three people. You can alter this code to look at other examples. \n",
        "\n",
        "**Discuss: What do you notice about the images?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGd01pt6nm0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_person(person_id):\n",
        "    fig_scale = 2\n",
        "    fig, ax = plt.subplots(5, 13, figsize=(13*fig_scale, 5*fig_scale))\n",
        "    plt.suptitle(person_id)\n",
        "    subplot_idx = 0\n",
        "    for pitch in np.linspace(-30, 30, 5):\n",
        "        for yaw in np.linspace(-90, 90, 13):\n",
        "            subplot_idx += 1\n",
        "            ax = plt.subplot(5, 13, subplot_idx)\n",
        "            img_idx = np.argwhere(np.logical_and([p == person_id for p in person_ids], np.logical_and(pitches == pitch, yaws == yaw)))\n",
        "            if img_idx.size:\n",
        "                plt.set_cmap('gray')\n",
        "                ax.imshow(images[img_idx[0]].squeeze(), interpolation='none')\n",
        "                ax.set_axis_off()\n",
        "    plt.show()\n",
        "\n",
        "visualize_person('Person01')\n",
        "visualize_person('Person02')\n",
        "visualize_person('Person03')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9iZYUs2K-_",
        "colab_type": "text"
      },
      "source": [
        "## Build a model\n",
        "Read and run the code below.\n",
        "\n",
        "**Discuss: What equation does \"w = ...\" represent?**\n",
        "\n",
        "**Discuss: Do you see any visual pattern in the weights (first plot)?**\n",
        "\n",
        "**Discuss: What is point of the line that starts with \"X = np.hstack(...\"?**\n",
        "\n",
        "**Discuss: What should the y-axes label be on the last plot?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDmX4FEZkumE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the images so each is represented by a vector of pixel values\n",
        "X = images.reshape((images.shape[0], images.shape[1]*images.shape[2]))\n",
        "num_pixels = len(X[0]) # num pixels\n",
        "X = np.hstack((X, np.ones((X.shape[0],1))))\n",
        "\n",
        "w = np.linalg.inv(X.T.dot(X) + 100000000*np.eye(num_pixels+1)).dot(X.T.dot(yaws))\n",
        "\n",
        "\n",
        "# Plot the weights\n",
        "plt.imshow(w[:-1].reshape((20, 20)))\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot the actual and predicted yaws\n",
        "plt.scatter(yaws, X.dot(w))\n",
        "plt.xlabel('actual yaw (degrees)')\n",
        "plt.ylabel('predicted yaw (degrees)')\n",
        "plt.show()\n",
        "\n",
        "# Make another plot!\n",
        "plt.scatter(np.arange(len(yaws)),X.dot(w)-yaws,c=pitches,cmap=\"jet\")\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('What should this label say?')\n",
        "cbar = plt.colorbar()\n",
        "cbar.ax.set_ylabel('Pitches')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEWnDAGlvsyu",
        "colab_type": "text"
      },
      "source": [
        "In the code above, we combined our data across all pitches (looking up vs straight ahead vs down). \n",
        "\n",
        "Now, let's only consider that data in which the pitch = 0 (looking straight ahead). \n",
        "\n",
        "**Discuss: Why might we opt to do this?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNRy9-BE1Qr3",
        "colab_type": "text"
      },
      "source": [
        "## Analyze when looking straight ahead (pitch = 0)\n",
        "\n",
        "Investigate the relationship between the variable lam (for lambda) in the code below. \n",
        "\n",
        "**Discuss: What happens when your value for lam is much lower or higher than starting value? Condisder this in relation to the 3 plots. What happens to the fit of the model (e.g., when is it overfit or underfit)?**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOhuZpxykumJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pitch_value = 0\n",
        "lam = 10000000 #10000000 was starting point\n",
        "X_restricted = X[pitches == pitch_value, :]\n",
        "w = np.linalg.inv(X_restricted.T.dot(X_restricted) + lam*np.eye(X.shape[1])).dot(X_restricted.T.dot(yaws[pitches == pitch_value]))\n",
        "\n",
        "# Plot weights\n",
        "plt.imshow(w[:-1].reshape((20, 20)))\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Plot Predicted and Actual yaw\n",
        "plt.scatter(yaws[pitches == pitch_value], X_restricted.dot(w))\n",
        "plt.xlabel('actual yaw (degrees)')\n",
        "plt.ylabel('predicted yaw (degrees)')\n",
        "plt.show()\n",
        "\n",
        "# Plot residuals\n",
        "plt.scatter(np.arange(len(yaws[pitches == pitch_value])),X_restricted.dot(w)-yaws[pitches == pitch_value])\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BQBYT9thsul",
        "colab_type": "text"
      },
      "source": [
        "## Consider the model and data\n",
        "\n",
        "Now that you've had a chance to work with these data, refer back to the [original dataset](http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html) and data collection protocol. Check out the photos of their setup. You can also see a few sample videos of people.\n",
        "\n",
        "**Discuss: What are some of the limitations of this data set and/or the linear regression model?**\n",
        "\n",
        "\n",
        "You may also want to check out the [Gender Shades Project](http://gendershades.org/overview.html). \n"
      ]
    }
  ]
}